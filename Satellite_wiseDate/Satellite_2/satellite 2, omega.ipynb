{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi step model (simple encoder-decoder)\n",
    "\n",
    "In this notebook, we demonstrate how to:\n",
    "- prepare time series data for training a RNN forecasting model\n",
    "- get data in the required shape for the keras API\n",
    "- implement a RNN model in keras to predict the next 3 steps ahead (time *t+1* to *t+3*) in the time series. This model uses a simple encoder decoder approach in which the final hidden state of the encoder is replicated across each time step of the decoder. \n",
    "- enable early stopping to reduce the likelihood of model overfitting\n",
    "- evaluate the model on a test dataset\n",
    "\n",
    "The data in this example is taken from the GEFCom2014 forecasting competition<sup>1</sup>. It consists of 3 years of hourly electricity load and temperature values between 2012 and 2014. The task is to forecast future values of electricity load.\n",
    "\n",
    "<sup>1</sup>Tao Hong, Pierre Pinson, Shu Fan, Hamidreza Zareipour, Alberto Troccoli and Rob J. Hyndman, \"Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond\", International Journal of Forecasting, vol.32, no.3, pp 896-913, July-September, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from collections import UserDict\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "from common.utils import load_data, mape, TimeSeriesTensor, create_evaluation_df\n",
    "\n",
    "pd.options.display.float_format = '{:,.20f}'.format\n",
    "np.set_printoptions(precision=20)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>OMEGA</th>\n",
       "      <th>i0</th>\n",
       "      <th>omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-01 14:00:00</th>\n",
       "      <td>0.00709403527435000045</td>\n",
       "      <td>1.00937285970000001356</td>\n",
       "      <td>0.96871190895600001181</td>\n",
       "      <td>0.63272009923999994463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 16:00:00</th>\n",
       "      <td>0.00709456193727000042</td>\n",
       "      <td>1.00931255967999988776</td>\n",
       "      <td>0.96871347574200006303</td>\n",
       "      <td>0.63273892260700004275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 18:00:00</th>\n",
       "      <td>0.00709454016760000059</td>\n",
       "      <td>1.00925244690999993402</td>\n",
       "      <td>0.96871571254299992937</td>\n",
       "      <td>0.63268337999699997276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 20:00:00</th>\n",
       "      <td>0.00709462445228999962</td>\n",
       "      <td>1.00919191575000000149</td>\n",
       "      <td>0.96871721203500005259</td>\n",
       "      <td>0.63265364472399998608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 22:00:00</th>\n",
       "      <td>0.00709612155333000025</td>\n",
       "      <td>1.00913133191999992988</td>\n",
       "      <td>0.96871909334700001537</td>\n",
       "      <td>0.63270434800099994987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         e                  OMEGA  \\\n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-01 14:00:00 0.00709403527435000045 1.00937285970000001356   \n",
       "2017-11-01 16:00:00 0.00709456193727000042 1.00931255967999988776   \n",
       "2017-11-01 18:00:00 0.00709454016760000059 1.00925244690999993402   \n",
       "2017-11-01 20:00:00 0.00709462445228999962 1.00919191575000000149   \n",
       "2017-11-01 22:00:00 0.00709612155333000025 1.00913133191999992988   \n",
       "\n",
       "                                        i0                  omega  \n",
       "Epoch_Time_of_Clock                                                \n",
       "2017-11-01 14:00:00 0.96871190895600001181 0.63272009923999994463  \n",
       "2017-11-01 16:00:00 0.96871347574200006303 0.63273892260700004275  \n",
       "2017-11-01 18:00:00 0.96871571254299992937 0.63268337999699997276  \n",
       "2017-11-01 20:00:00 0.96871721203500005259 0.63265364472399998608  \n",
       "2017-11-01 22:00:00 0.96871909334700001537 0.63270434800099994987  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Cleaned1.csv\" , parse_dates = True)\n",
    "a = pd.to_datetime(df['Epoch_Time_of_Clock'])\n",
    "print(type(a[0]))\n",
    "df = df.drop(['Unnamed: 0', 'Unnamed: 0.1' ,'sqrt_A'  ,'PRN','SV_Clock_Bias', 'SV_Clock_Drift', 'SV_Clock_Drift_Rate', 'IODE', 'Crs',\n",
    "       'Del_n', 'Cuc','Cus','Toe', 'Cic', \n",
    "       'Cis', 'Crc', 'M0', 'OMEGA_dot', 'I_dot', 'Codes', 'GPS_week',\n",
    "       'L2_P_Data_flag', 'SV_accuracy', 'SV_health', 'Tgd', 'IODC', 'T_Tx',\n",
    "       'Fit_Interval' ,'Epoch_Time_of_Clock' ],axis =1 )\n",
    "df.head()\n",
    "#df = df.set_index(['Epoch_Time_of_Clock'])\n",
    "df = df.set_index(a)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter Variable Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_var = 1\n",
    "var_name = 'omega'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['e', 'OMEGA', 'omega'], dtype='object')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.iloc[5 : , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>OMEGA</th>\n",
       "      <th>omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 12:00:00</th>\n",
       "      <td>0.00709870201535999987</td>\n",
       "      <td>1.00870849592999989319</td>\n",
       "      <td>0.63246247790400000888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 14:00:00</th>\n",
       "      <td>0.00709954684135000007</td>\n",
       "      <td>1.00864845192999985635</td>\n",
       "      <td>0.63229294617900000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 16:00:00</th>\n",
       "      <td>0.00709992146584999956</td>\n",
       "      <td>1.00858859223999997035</td>\n",
       "      <td>0.63230983703100007709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 18:00:00</th>\n",
       "      <td>0.00710008526221000014</td>\n",
       "      <td>1.00852901782999992975</td>\n",
       "      <td>0.63230376299500001824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 20:00:00</th>\n",
       "      <td>0.00710032973439000091</td>\n",
       "      <td>1.00846886117999989807</td>\n",
       "      <td>0.63223729823800001171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 22:00:00</th>\n",
       "      <td>0.00710155733395000013</td>\n",
       "      <td>1.00840879082999990146</td>\n",
       "      <td>0.63230010131100000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 12:00:00</th>\n",
       "      <td>0.00710492231883000022</td>\n",
       "      <td>1.00799067275000009403</td>\n",
       "      <td>0.63232765829899995058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         e                  OMEGA  \\\n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-02 12:00:00 0.00709870201535999987 1.00870849592999989319   \n",
       "2017-11-02 14:00:00 0.00709954684135000007 1.00864845192999985635   \n",
       "2017-11-02 16:00:00 0.00709992146584999956 1.00858859223999997035   \n",
       "2017-11-02 18:00:00 0.00710008526221000014 1.00852901782999992975   \n",
       "2017-11-02 20:00:00 0.00710032973439000091 1.00846886117999989807   \n",
       "2017-11-02 22:00:00 0.00710155733395000013 1.00840879082999990146   \n",
       "2017-11-03 12:00:00 0.00710492231883000022 1.00799067275000009403   \n",
       "\n",
       "                                     omega  \n",
       "Epoch_Time_of_Clock                         \n",
       "2017-11-02 12:00:00 0.63246247790400000888  \n",
       "2017-11-02 14:00:00 0.63229294617900000386  \n",
       "2017-11-02 16:00:00 0.63230983703100007709  \n",
       "2017-11-02 18:00:00 0.63230376299500001824  \n",
       "2017-11-02 20:00:00 0.63223729823800001171  \n",
       "2017-11-02 22:00:00 0.63230010131100000681  \n",
       "2017-11-03 12:00:00 0.63232765829899995058  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter number of entries per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "2017-11-25 12:00:00 2017-11-21 12:00:00\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "entry = 6\n",
    "print(df.shape[0])\n",
    "no_of_entries = df.shape[0]//entry\n",
    "valid = (no_of_entries * 70)//100\n",
    "test = (no_of_entries * 85)//100\n",
    "indexes = df.index\n",
    "#print(valid , test , indexes)\n",
    "valid_start_dt = indexes[int(valid)*int(entry)] \n",
    "test_start_dt = indexes [int(test)*int(entry)] \n",
    "test_start_dt = str(test_start_dt)\n",
    "valid_start_dt = str(valid_start_dt)\n",
    "print(test_start_dt,valid_start_dt)\n",
    "print(type(test_start_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load data into Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter lag and no. of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"total = len(df)\n",
    "t = total*70/100\n",
    "t = round(t)\n",
    "indexes = df.index\n",
    "valid_start_dt = str(indexes[t])\n",
    "t = total*85/100\n",
    "t = round(t)\n",
    "test_start_dt = str(indexes[t])\n",
    "print(valid_start_dt , test_start_dt)\n",
    "\"\"\"\n",
    "T = 6\n",
    "HORIZON = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training set containing only the model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>OMEGA</th>\n",
       "      <th>omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 12:00:00</th>\n",
       "      <td>0.00709870201535999987</td>\n",
       "      <td>1.00870849592999989319</td>\n",
       "      <td>0.63246247790400000888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 14:00:00</th>\n",
       "      <td>0.00709954684135000007</td>\n",
       "      <td>1.00864845192999985635</td>\n",
       "      <td>0.63229294617900000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 16:00:00</th>\n",
       "      <td>0.00709992146584999956</td>\n",
       "      <td>1.00858859223999997035</td>\n",
       "      <td>0.63230983703100007709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 18:00:00</th>\n",
       "      <td>0.00710008526221000014</td>\n",
       "      <td>1.00852901782999992975</td>\n",
       "      <td>0.63230376299500001824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 20:00:00</th>\n",
       "      <td>0.00710032973439000091</td>\n",
       "      <td>1.00846886117999989807</td>\n",
       "      <td>0.63223729823800001171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         e                  OMEGA  \\\n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-02 12:00:00 0.00709870201535999987 1.00870849592999989319   \n",
       "2017-11-02 14:00:00 0.00709954684135000007 1.00864845192999985635   \n",
       "2017-11-02 16:00:00 0.00709992146584999956 1.00858859223999997035   \n",
       "2017-11-02 18:00:00 0.00710008526221000014 1.00852901782999992975   \n",
       "2017-11-02 20:00:00 0.00710032973439000091 1.00846886117999989807   \n",
       "\n",
       "                                     omega  \n",
       "Epoch_Time_of_Clock                         \n",
       "2017-11-02 12:00:00 0.63246247790400000888  \n",
       "2017-11-02 14:00:00 0.63229294617900000386  \n",
       "2017-11-02 16:00:00 0.63230983703100007709  \n",
       "2017-11-02 18:00:00 0.63230376299500001824  \n",
       "2017-11-02 20:00:00 0.63223729823800001171  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.copy()[df.index < valid_start_dt][['e', 'OMEGA', 'omega' ]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>OMEGA</th>\n",
       "      <th>omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-20 12:00:00</th>\n",
       "      <td>0.00711646664422000037</td>\n",
       "      <td>0.63499334812499996783</td>\n",
       "      <td>0.63062970780100002788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 14:00:00</th>\n",
       "      <td>0.00711591739672999980</td>\n",
       "      <td>0.63493855599100001985</td>\n",
       "      <td>0.63059641032300006724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 16:00:00</th>\n",
       "      <td>0.00711673183832000050</td>\n",
       "      <td>0.63488355319700007229</td>\n",
       "      <td>0.63077053268500005156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 18:00:00</th>\n",
       "      <td>0.00711753428914000015</td>\n",
       "      <td>0.63482873911899995178</td>\n",
       "      <td>0.63056012995499999274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 20:00:00</th>\n",
       "      <td>0.00711681041867000090</td>\n",
       "      <td>0.63477265961800000138</td>\n",
       "      <td>0.63053029374000002516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 22:00:00</th>\n",
       "      <td>0.00711733335629000010</td>\n",
       "      <td>0.63471711993299995136</td>\n",
       "      <td>0.63069974500500003423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         e                  OMEGA  \\\n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-20 12:00:00 0.00711646664422000037 0.63499334812499996783   \n",
       "2017-11-20 14:00:00 0.00711591739672999980 0.63493855599100001985   \n",
       "2017-11-20 16:00:00 0.00711673183832000050 0.63488355319700007229   \n",
       "2017-11-20 18:00:00 0.00711753428914000015 0.63482873911899995178   \n",
       "2017-11-20 20:00:00 0.00711681041867000090 0.63477265961800000138   \n",
       "2017-11-20 22:00:00 0.00711733335629000010 0.63471711993299995136   \n",
       "\n",
       "                                     omega  \n",
       "Epoch_Time_of_Clock                         \n",
       "2017-11-20 12:00:00 0.63062970780100002788  \n",
       "2017-11-20 14:00:00 0.63059641032300006724  \n",
       "2017-11-20 16:00:00 0.63077053268500005156  \n",
       "2017-11-20 18:00:00 0.63056012995499999274  \n",
       "2017-11-20 20:00:00 0.63053029374000002516  \n",
       "2017-11-20 22:00:00 0.63069974500500003423  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data to be in range (0, 1). This transformation should be calibrated on the training set only. This is to prevent information from the validation or test sets leaking into the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter variable to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_scalar = StandardScaler()\n",
    "y_scalar.fit(train[[var_name]])\n",
    "\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "train[['e', 'OMEGA', 'omega']] = X_scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the TimeSeriesTensor convenience class to:\n",
    "1. Shift the values of the time series to create a Pandas dataframe containing all the data for a single training example\n",
    "2. Discard any samples with missing values\n",
    "3. Transform this Pandas dataframe into a numpy array of shape (samples, time steps, features) for input into Keras\n",
    "\n",
    "The class takes the following parameters:\n",
    "\n",
    "- **dataset**: original time series\n",
    "- **H**: the forecast horizon\n",
    "- **tensor_structure**: a dictionary discribing the tensor structure in the form { 'tensor_name' : (range(max_backward_shift, max_forward_shift), [feature, feature, ...] ) }\n",
    "- **freq**: time series frequency\n",
    "- **drop_incomplete**: (Boolean) whether to drop incomplete samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_structure = {'X':(range(-T+1, 1), ['e', 'OMEGA', 'omega'])}\n",
    "train_inputs = TimeSeriesTensor(train, var_name, HORIZON, {'X':(range(-T+1, 1), ['e', 'OMEGA', 'omega'])} ,freq = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"15\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"4\" halign=\"left\">e</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">OMEGA</th>\n",
       "      <th colspan=\"6\" halign=\"left\">omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>...</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 22:00:00</th>\n",
       "      <td>0.80658668930844723555</td>\n",
       "      <td>0.51029756466224507339</td>\n",
       "      <td>0.51543818338990998740</td>\n",
       "      <td>0.57610759556977997331</td>\n",
       "      <td>0.44407830463047193170</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>-2.92617496977098268118</td>\n",
       "      <td>-2.79400608189676713522</td>\n",
       "      <td>-2.73539791391728170922</td>\n",
       "      <td>-2.70977277572862673694</td>\n",
       "      <td>...</td>\n",
       "      <td>1.62356028775279348508</td>\n",
       "      <td>1.62301501526409786003</td>\n",
       "      <td>1.62246441365080329433</td>\n",
       "      <td>1.62191460192390102257</td>\n",
       "      <td>1.03938727375447315104</td>\n",
       "      <td>0.74664732200838213050</td>\n",
       "      <td>0.77581370487537448799</td>\n",
       "      <td>0.76532532575010336906</td>\n",
       "      <td>0.65055689667290861333</td>\n",
       "      <td>0.75900249031931454891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 12:00:00</th>\n",
       "      <td>0.51029756466224507339</td>\n",
       "      <td>0.51543818338990998740</td>\n",
       "      <td>0.57610759556977997331</td>\n",
       "      <td>0.44407830463047193170</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>-2.79400608189676713522</td>\n",
       "      <td>-2.73539791391728170922</td>\n",
       "      <td>-2.70977277572862673694</td>\n",
       "      <td>-2.67152630068010399356</td>\n",
       "      <td>...</td>\n",
       "      <td>1.62301501526409786003</td>\n",
       "      <td>1.62246441365080329433</td>\n",
       "      <td>1.62191460192390102257</td>\n",
       "      <td>1.61808765196250115004</td>\n",
       "      <td>0.74664732200838213050</td>\n",
       "      <td>0.77581370487537448799</td>\n",
       "      <td>0.76532532575010336906</td>\n",
       "      <td>0.65055689667290861333</td>\n",
       "      <td>0.75900249031931454891</td>\n",
       "      <td>0.80658668930844723555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 14:00:00</th>\n",
       "      <td>0.51543818338990998740</td>\n",
       "      <td>0.57610759556977997331</td>\n",
       "      <td>0.44407830463047193170</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>-2.73539791391728170922</td>\n",
       "      <td>-2.70977277572862673694</td>\n",
       "      <td>-2.67152630068010399356</td>\n",
       "      <td>-2.47947436274205035289</td>\n",
       "      <td>...</td>\n",
       "      <td>1.62246441365080329433</td>\n",
       "      <td>1.62191460192390102257</td>\n",
       "      <td>1.61808765196250115004</td>\n",
       "      <td>1.61754653035883078083</td>\n",
       "      <td>0.77581370487537448799</td>\n",
       "      <td>0.76532532575010336906</td>\n",
       "      <td>0.65055689667290861333</td>\n",
       "      <td>0.75900249031931454891</td>\n",
       "      <td>0.80658668930844723555</td>\n",
       "      <td>0.51029756466224507339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 16:00:00</th>\n",
       "      <td>0.57610759556977997331</td>\n",
       "      <td>0.44407830463047193170</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>-2.70977277572862673694</td>\n",
       "      <td>-2.67152630068010399356</td>\n",
       "      <td>-2.47947436274205035289</td>\n",
       "      <td>-1.95303896117129727195</td>\n",
       "      <td>...</td>\n",
       "      <td>1.62191460192390102257</td>\n",
       "      <td>1.61808765196250115004</td>\n",
       "      <td>1.61754653035883078083</td>\n",
       "      <td>1.61700740369882267622</td>\n",
       "      <td>0.76532532575010336906</td>\n",
       "      <td>0.65055689667290861333</td>\n",
       "      <td>0.75900249031931454891</td>\n",
       "      <td>0.80658668930844723555</td>\n",
       "      <td>0.51029756466224507339</td>\n",
       "      <td>0.51543818338990998740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 18:00:00</th>\n",
       "      <td>0.44407830463047193170</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>-2.67152630068010399356</td>\n",
       "      <td>-2.47947436274205035289</td>\n",
       "      <td>-1.95303896117129727195</td>\n",
       "      <td>-1.77839827418765583644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61808765196250115004</td>\n",
       "      <td>1.61754653035883078083</td>\n",
       "      <td>1.61700740369882267622</td>\n",
       "      <td>1.61647116932270606959</td>\n",
       "      <td>0.65055689667290861333</td>\n",
       "      <td>0.75900249031931454891</td>\n",
       "      <td>0.80658668930844723555</td>\n",
       "      <td>0.51029756466224507339</td>\n",
       "      <td>0.51543818338990998740</td>\n",
       "      <td>0.57610759556977997331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 20:00:00</th>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>-2.47947436274205035289</td>\n",
       "      <td>-1.95303896117129727195</td>\n",
       "      <td>-1.77839827418765583644</td>\n",
       "      <td>-1.72634664376002766595</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61754653035883078083</td>\n",
       "      <td>1.61700740369882267622</td>\n",
       "      <td>1.61647116932270606959</td>\n",
       "      <td>1.61592889611410139850</td>\n",
       "      <td>0.75900249031931454891</td>\n",
       "      <td>0.80658668930844723555</td>\n",
       "      <td>0.51029756466224507339</td>\n",
       "      <td>0.51543818338990998740</td>\n",
       "      <td>0.57610759556977997331</td>\n",
       "      <td>0.44407830463047193170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 22:00:00</th>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>-1.95303896117129727195</td>\n",
       "      <td>-1.77839827418765583644</td>\n",
       "      <td>-1.72634664376002766595</td>\n",
       "      <td>-1.69933734814780645905</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61700740369882267622</td>\n",
       "      <td>1.61647116932270606959</td>\n",
       "      <td>1.61592889611410139850</td>\n",
       "      <td>1.61538859121451916501</td>\n",
       "      <td>0.80658668930844723555</td>\n",
       "      <td>0.51029756466224507339</td>\n",
       "      <td>0.51543818338990998740</td>\n",
       "      <td>0.57610759556977997331</td>\n",
       "      <td>0.44407830463047193170</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 12:00:00</th>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>-1.77839827418765583644</td>\n",
       "      <td>-1.72634664376002766595</td>\n",
       "      <td>-1.69933734814780645905</td>\n",
       "      <td>-1.63533824623316559332</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61647116932270606959</td>\n",
       "      <td>1.61592889611410139850</td>\n",
       "      <td>1.61538859121451916501</td>\n",
       "      <td>1.61163470931397379005</td>\n",
       "      <td>0.51029756466224507339</td>\n",
       "      <td>0.51543818338990998740</td>\n",
       "      <td>0.57610759556977997331</td>\n",
       "      <td>0.44407830463047193170</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 14:00:00</th>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>-1.72634664376002766595</td>\n",
       "      <td>-1.69933734814780645905</td>\n",
       "      <td>-1.63533824623316559332</td>\n",
       "      <td>-1.50637477769096883584</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61592889611410139850</td>\n",
       "      <td>1.61538859121451916501</td>\n",
       "      <td>1.61163470931397379005</td>\n",
       "      <td>1.61110484848813784176</td>\n",
       "      <td>0.51543818338990998740</td>\n",
       "      <td>0.57610759556977997331</td>\n",
       "      <td>0.44407830463047193170</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 16:00:00</th>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>-1.69933734814780645905</td>\n",
       "      <td>-1.63533824623316559332</td>\n",
       "      <td>-1.50637477769096883584</td>\n",
       "      <td>-0.99163187025148802345</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61538859121451916501</td>\n",
       "      <td>1.61163470931397379005</td>\n",
       "      <td>1.61110484848813784176</td>\n",
       "      <td>1.61057712996599766697</td>\n",
       "      <td>0.57610759556977997331</td>\n",
       "      <td>0.44407830463047193170</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 18:00:00</th>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>-1.63533824623316559332</td>\n",
       "      <td>-1.50637477769096883584</td>\n",
       "      <td>-0.99163187025148802345</td>\n",
       "      <td>-0.82061549218194562716</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61163470931397379005</td>\n",
       "      <td>1.61110484848813784176</td>\n",
       "      <td>1.61057712996599766697</td>\n",
       "      <td>1.61005214300463062038</td>\n",
       "      <td>0.44407830463047193170</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 20:00:00</th>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>-1.50637477769096883584</td>\n",
       "      <td>-0.99163187025148802345</td>\n",
       "      <td>-0.82061549218194562716</td>\n",
       "      <td>-0.73887731173082793479</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61110484848813784176</td>\n",
       "      <td>1.61057712996599766697</td>\n",
       "      <td>1.61005214300463062038</td>\n",
       "      <td>1.60952111721077728568</td>\n",
       "      <td>0.53415155282490367483</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 22:00:00</th>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>-0.99163187025148802345</td>\n",
       "      <td>-0.82061549218194562716</td>\n",
       "      <td>-0.73887731173082793479</td>\n",
       "      <td>-0.74412254364959273811</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61057712996599766697</td>\n",
       "      <td>1.61005214300463062038</td>\n",
       "      <td>1.60952111721077728568</td>\n",
       "      <td>1.60899302379006448049</td>\n",
       "      <td>0.99120691392514004292</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 12:00:00</th>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>-0.82061549218194562716</td>\n",
       "      <td>-0.73887731173082793479</td>\n",
       "      <td>-0.74412254364959273811</td>\n",
       "      <td>-0.67536995275316269449</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61005214300463062038</td>\n",
       "      <td>1.60952111721077728568</td>\n",
       "      <td>1.60899302379006448049</td>\n",
       "      <td>0.50318622687538083760</td>\n",
       "      <td>0.78575139383041936458</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 14:00:00</th>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>-0.73887731173082793479</td>\n",
       "      <td>-0.74412254364959273811</td>\n",
       "      <td>-0.67536995275316269449</td>\n",
       "      <td>-0.59774782258829062265</td>\n",
       "      <td>...</td>\n",
       "      <td>1.60952111721077728568</td>\n",
       "      <td>1.60899302379006448049</td>\n",
       "      <td>0.50318622687538083760</td>\n",
       "      <td>0.50266746615002333431</td>\n",
       "      <td>0.76883408392202723380</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 16:00:00</th>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>-0.74412254364959273811</td>\n",
       "      <td>-0.67536995275316269449</td>\n",
       "      <td>-0.59774782258829062265</td>\n",
       "      <td>-0.22919750908055919192</td>\n",
       "      <td>...</td>\n",
       "      <td>1.60899302379006448049</td>\n",
       "      <td>0.50318622687538083760</td>\n",
       "      <td>0.50266746615002333431</td>\n",
       "      <td>0.50215059338128265054</td>\n",
       "      <td>0.79552235929752979082</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 18:00:00</th>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>-0.67536995275316269449</td>\n",
       "      <td>-0.59774782258829062265</td>\n",
       "      <td>-0.22919750908055919192</td>\n",
       "      <td>-0.11678930055519548548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50318622687538083760</td>\n",
       "      <td>0.50266746615002333431</td>\n",
       "      <td>0.50215059338128265054</td>\n",
       "      <td>0.50163637179345244022</td>\n",
       "      <td>0.71031438474593744381</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 20:00:00</th>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>-0.59774782258829062265</td>\n",
       "      <td>-0.22919750908055919192</td>\n",
       "      <td>-0.11678930055519548548</td>\n",
       "      <td>0.03078745392909986237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50266746615002333431</td>\n",
       "      <td>0.50215059338128265054</td>\n",
       "      <td>0.50163637179345244022</td>\n",
       "      <td>0.50111603106649127426</td>\n",
       "      <td>0.75033795768062416798</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 22:00:00</th>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>-0.22919750908055919192</td>\n",
       "      <td>-0.11678930055519548548</td>\n",
       "      <td>0.03078745392909986237</td>\n",
       "      <td>-0.02026248354428677664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50215059338128265054</td>\n",
       "      <td>0.50163637179345244022</td>\n",
       "      <td>0.50111603106649127426</td>\n",
       "      <td>0.50059917169744694299</td>\n",
       "      <td>1.29779243521901799241</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 12:00:00</th>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>0.88633322937738301395</td>\n",
       "      <td>-0.11678930055519548548</td>\n",
       "      <td>0.03078745392909986237</td>\n",
       "      <td>-0.02026248354428677664</td>\n",
       "      <td>0.01840288108604457018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50163637179345244022</td>\n",
       "      <td>0.50111603106649127426</td>\n",
       "      <td>0.50059917169744694299</td>\n",
       "      <td>0.49699706313551883508</td>\n",
       "      <td>1.24604269253540334006</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 14:00:00</th>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>0.88633322937738301395</td>\n",
       "      <td>1.10333051374924528965</td>\n",
       "      <td>0.03078745392909986237</td>\n",
       "      <td>-0.02026248354428677664</td>\n",
       "      <td>0.01840288108604457018</td>\n",
       "      <td>0.08735581088639735037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50111603106649127426</td>\n",
       "      <td>0.50059917169744694299</td>\n",
       "      <td>0.49699706313551883508</td>\n",
       "      <td>0.49648617560963975714</td>\n",
       "      <td>1.23522339440477324501</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 16:00:00</th>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>0.88633322937738301395</td>\n",
       "      <td>1.10333051374924528965</td>\n",
       "      <td>1.21126330916306468310</td>\n",
       "      <td>-0.02026248354428677664</td>\n",
       "      <td>0.01840288108604457018</td>\n",
       "      <td>0.08735581088639735037</td>\n",
       "      <td>0.25296304481097164896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50059917169744694299</td>\n",
       "      <td>0.49699706313551883508</td>\n",
       "      <td>0.49648617560963975714</td>\n",
       "      <td>0.49597637265360822179</td>\n",
       "      <td>1.09379447518572048281</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 18:00:00</th>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>0.88633322937738301395</td>\n",
       "      <td>1.10333051374924528965</td>\n",
       "      <td>1.21126330916306468310</td>\n",
       "      <td>0.68133493359471564155</td>\n",
       "      <td>0.01840288108604457018</td>\n",
       "      <td>0.08735581088639735037</td>\n",
       "      <td>0.25296304481097164896</td>\n",
       "      <td>0.26778810648378881254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49699706313551883508</td>\n",
       "      <td>0.49648617560963975714</td>\n",
       "      <td>0.49597637265360822179</td>\n",
       "      <td>0.49546970291054909241</td>\n",
       "      <td>1.10047854455312421607</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 20:00:00</th>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>0.88633322937738301395</td>\n",
       "      <td>1.10333051374924528965</td>\n",
       "      <td>1.21126330916306468310</td>\n",
       "      <td>0.68133493359471564155</td>\n",
       "      <td>0.81522090874259967030</td>\n",
       "      <td>0.08735581088639735037</td>\n",
       "      <td>0.25296304481097164896</td>\n",
       "      <td>0.26778810648378881254</td>\n",
       "      <td>0.49464433768609061826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49648617560963975714</td>\n",
       "      <td>0.49597637265360822179</td>\n",
       "      <td>0.49546970291054909241</td>\n",
       "      <td>0.49495574911525747064</td>\n",
       "      <td>1.08940158332075420766</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 22:00:00</th>\n",
       "      <td>0.88633322937738301395</td>\n",
       "      <td>1.10333051374924528965</td>\n",
       "      <td>1.21126330916306468310</td>\n",
       "      <td>0.68133493359471564155</td>\n",
       "      <td>0.81522090874259967030</td>\n",
       "      <td>0.82265270324617567610</td>\n",
       "      <td>0.25296304481097164896</td>\n",
       "      <td>0.26778810648378881254</td>\n",
       "      <td>0.49464433768609061826</td>\n",
       "      <td>0.41673080505272314111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49597637265360822179</td>\n",
       "      <td>0.49546970291054909241</td>\n",
       "      <td>0.49495574911525747064</td>\n",
       "      <td>0.49444550429879202680</td>\n",
       "      <td>1.33051304520138224952</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 12:00:00</th>\n",
       "      <td>1.10333051374924528965</td>\n",
       "      <td>1.21126330916306468310</td>\n",
       "      <td>0.68133493359471564155</td>\n",
       "      <td>0.81522090874259967030</td>\n",
       "      <td>0.82265270324617567610</td>\n",
       "      <td>0.10960713551924922138</td>\n",
       "      <td>0.26778810648378881254</td>\n",
       "      <td>0.49464433768609061826</td>\n",
       "      <td>0.41673080505272314111</td>\n",
       "      <td>0.39835428469199501977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49546970291054909241</td>\n",
       "      <td>0.49495574911525747064</td>\n",
       "      <td>0.49444550429879202680</td>\n",
       "      <td>0.49087480819152135014</td>\n",
       "      <td>1.43746318512219928287</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>0.88633322937738301395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 14:00:00</th>\n",
       "      <td>1.21126330916306468310</td>\n",
       "      <td>0.68133493359471564155</td>\n",
       "      <td>0.81522090874259967030</td>\n",
       "      <td>0.82265270324617567610</td>\n",
       "      <td>0.10960713551924922138</td>\n",
       "      <td>0.36331879759109730887</td>\n",
       "      <td>0.49464433768609061826</td>\n",
       "      <td>0.41673080505272314111</td>\n",
       "      <td>0.39835428469199501977</td>\n",
       "      <td>0.50655538249538389906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49495574911525747064</td>\n",
       "      <td>0.49444550429879202680</td>\n",
       "      <td>0.49087480819152135014</td>\n",
       "      <td>0.49036662539956998863</td>\n",
       "      <td>1.47237645253395066369</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>0.88633322937738301395</td>\n",
       "      <td>1.10333051374924528965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 16:00:00</th>\n",
       "      <td>0.68133493359471564155</td>\n",
       "      <td>0.81522090874259967030</td>\n",
       "      <td>0.82265270324617567610</td>\n",
       "      <td>0.10960713551924922138</td>\n",
       "      <td>0.36331879759109730887</td>\n",
       "      <td>0.54247506148076651833</td>\n",
       "      <td>0.41673080505272314111</td>\n",
       "      <td>0.39835428469199501977</td>\n",
       "      <td>0.50655538249538389906</td>\n",
       "      <td>0.51622627581428048860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49444550429879202680</td>\n",
       "      <td>0.49087480819152135014</td>\n",
       "      <td>0.49036662539956998863</td>\n",
       "      <td>0.48985817481507704096</td>\n",
       "      <td>1.10187042571692872528</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>0.88633322937738301395</td>\n",
       "      <td>1.10333051374924528965</td>\n",
       "      <td>1.21126330916306468310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 18:00:00</th>\n",
       "      <td>0.81522090874259967030</td>\n",
       "      <td>0.82265270324617567610</td>\n",
       "      <td>0.10960713551924922138</td>\n",
       "      <td>0.36331879759109730887</td>\n",
       "      <td>0.54247506148076651833</td>\n",
       "      <td>0.01593671751055668329</td>\n",
       "      <td>0.39835428469199501977</td>\n",
       "      <td>0.50655538249538389906</td>\n",
       "      <td>0.51622627581428048860</td>\n",
       "      <td>0.42709377864782482881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49087480819152135014</td>\n",
       "      <td>0.49036662539956998863</td>\n",
       "      <td>0.48985817481507704096</td>\n",
       "      <td>0.48935398217587849912</td>\n",
       "      <td>1.19661191258787025227</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>0.88633322937738301395</td>\n",
       "      <td>1.10333051374924528965</td>\n",
       "      <td>1.21126330916306468310</td>\n",
       "      <td>0.68133493359471564155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 20:00:00</th>\n",
       "      <td>0.82265270324617567610</td>\n",
       "      <td>0.10960713551924922138</td>\n",
       "      <td>0.36331879759109730887</td>\n",
       "      <td>0.54247506148076651833</td>\n",
       "      <td>0.01593671751055668329</td>\n",
       "      <td>0.12994226398614291962</td>\n",
       "      <td>0.50655538249538389906</td>\n",
       "      <td>0.51622627581428048860</td>\n",
       "      <td>0.42709377864782482881</td>\n",
       "      <td>0.71315919454097298491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49036662539956998863</td>\n",
       "      <td>0.48985817481507704096</td>\n",
       "      <td>0.48935398217587849912</td>\n",
       "      <td>0.48884001499919776945</td>\n",
       "      <td>1.16981754013331751274</td>\n",
       "      <td>0.88633322937738301395</td>\n",
       "      <td>1.10333051374924528965</td>\n",
       "      <td>1.21126330916306468310</td>\n",
       "      <td>0.68133493359471564155</td>\n",
       "      <td>0.81522090874259967030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 12:00:00</th>\n",
       "      <td>0.74153449024088802233</td>\n",
       "      <td>0.62209529624281612037</td>\n",
       "      <td>0.56352507736145363015</td>\n",
       "      <td>0.63713066122663675195</td>\n",
       "      <td>0.65222665089262377158</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.57246680675461392163</td>\n",
       "      <td>0.46566807971331197757</td>\n",
       "      <td>0.45284640394979230882</td>\n",
       "      <td>0.40527507493839998176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65755468387938431629</td>\n",
       "      <td>-0.65810306281827102293</td>\n",
       "      <td>-0.65864667500787377197</td>\n",
       "      <td>-0.66244412724316559249</td>\n",
       "      <td>0.78038089896137685653</td>\n",
       "      <td>0.69162627602713089292</td>\n",
       "      <td>0.79945550233279349950</td>\n",
       "      <td>0.85066465809674118859</td>\n",
       "      <td>0.85235462054390909170</td>\n",
       "      <td>0.73655554078628027170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 14:00:00</th>\n",
       "      <td>0.62209529624281612037</td>\n",
       "      <td>0.56352507736145363015</td>\n",
       "      <td>0.63713066122663675195</td>\n",
       "      <td>0.65222665089262377158</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.46566807971331197757</td>\n",
       "      <td>0.45284640394979230882</td>\n",
       "      <td>0.40527507493839998176</td>\n",
       "      <td>0.41707684440847908602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65810306281827102293</td>\n",
       "      <td>-0.65864667500787377197</td>\n",
       "      <td>-0.66244412724316559249</td>\n",
       "      <td>-0.66297737572973403086</td>\n",
       "      <td>0.69162627602713089292</td>\n",
       "      <td>0.79945550233279349950</td>\n",
       "      <td>0.85066465809674118859</td>\n",
       "      <td>0.85235462054390909170</td>\n",
       "      <td>0.73655554078628027170</td>\n",
       "      <td>0.74153449024088802233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 16:00:00</th>\n",
       "      <td>0.56352507736145363015</td>\n",
       "      <td>0.63713066122663675195</td>\n",
       "      <td>0.65222665089262377158</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>0.45284640394979230882</td>\n",
       "      <td>0.40527507493839998176</td>\n",
       "      <td>0.41707684440847908602</td>\n",
       "      <td>0.13906140039398354191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65864667500787377197</td>\n",
       "      <td>-0.66244412724316559249</td>\n",
       "      <td>-0.66297737572973403086</td>\n",
       "      <td>-0.66351639521048810799</td>\n",
       "      <td>0.79945550233279349950</td>\n",
       "      <td>0.85066465809674118859</td>\n",
       "      <td>0.85235462054390909170</td>\n",
       "      <td>0.73655554078628027170</td>\n",
       "      <td>0.74153449024088802233</td>\n",
       "      <td>0.62209529624281612037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 18:00:00</th>\n",
       "      <td>0.63713066122663675195</td>\n",
       "      <td>0.65222665089262377158</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.40527507493839998176</td>\n",
       "      <td>0.41707684440847908602</td>\n",
       "      <td>0.13906140039398354191</td>\n",
       "      <td>0.11345447554412074753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66244412724316559249</td>\n",
       "      <td>-0.66297737572973403086</td>\n",
       "      <td>-0.66351639521048810799</td>\n",
       "      <td>-0.66404975081223938105</td>\n",
       "      <td>0.85066465809674118859</td>\n",
       "      <td>0.85235462054390909170</td>\n",
       "      <td>0.73655554078628027170</td>\n",
       "      <td>0.74153449024088802233</td>\n",
       "      <td>0.62209529624281612037</td>\n",
       "      <td>0.56352507736145363015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 20:00:00</th>\n",
       "      <td>0.65222665089262377158</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.41707684440847908602</td>\n",
       "      <td>0.13906140039398354191</td>\n",
       "      <td>0.11345447554412074753</td>\n",
       "      <td>0.01537958731915608362</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66297737572973403086</td>\n",
       "      <td>-0.66351639521048810799</td>\n",
       "      <td>-0.66404975081223938105</td>\n",
       "      <td>-0.66459178300023724617</td>\n",
       "      <td>0.85235462054390909170</td>\n",
       "      <td>0.73655554078628027170</td>\n",
       "      <td>0.74153449024088802233</td>\n",
       "      <td>0.62209529624281612037</td>\n",
       "      <td>0.56352507736145363015</td>\n",
       "      <td>0.63713066122663675195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 22:00:00</th>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>0.13906140039398354191</td>\n",
       "      <td>0.11345447554412074753</td>\n",
       "      <td>0.01537958731915608362</td>\n",
       "      <td>0.00091877834102239194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66351639521048810799</td>\n",
       "      <td>-0.66404975081223938105</td>\n",
       "      <td>-0.66459178300023724617</td>\n",
       "      <td>-0.66512840570207776292</td>\n",
       "      <td>0.73655554078628027170</td>\n",
       "      <td>0.74153449024088802233</td>\n",
       "      <td>0.62209529624281612037</td>\n",
       "      <td>0.56352507736145363015</td>\n",
       "      <td>0.63713066122663675195</td>\n",
       "      <td>0.65222665089262377158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 12:00:00</th>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>0.11345447554412074753</td>\n",
       "      <td>0.01537958731915608362</td>\n",
       "      <td>0.00091877834102239194</td>\n",
       "      <td>-0.08078297699720485281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66404975081223938105</td>\n",
       "      <td>-0.66459178300023724617</td>\n",
       "      <td>-0.66512840570207776292</td>\n",
       "      <td>-0.66887997113035013719</td>\n",
       "      <td>0.74153449024088802233</td>\n",
       "      <td>0.62209529624281612037</td>\n",
       "      <td>0.56352507736145363015</td>\n",
       "      <td>0.63713066122663675195</td>\n",
       "      <td>0.65222665089262377158</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 14:00:00</th>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>0.01537958731915608362</td>\n",
       "      <td>0.00091877834102239194</td>\n",
       "      <td>-0.08078297699720485281</td>\n",
       "      <td>-0.04881985193498573905</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66459178300023724617</td>\n",
       "      <td>-0.66512840570207776292</td>\n",
       "      <td>-0.66887997113035013719</td>\n",
       "      <td>-0.66940629709101928668</td>\n",
       "      <td>0.62209529624281612037</td>\n",
       "      <td>0.56352507736145363015</td>\n",
       "      <td>0.63713066122663675195</td>\n",
       "      <td>0.65222665089262377158</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 16:00:00</th>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>0.00091877834102239194</td>\n",
       "      <td>-0.08078297699720485281</td>\n",
       "      <td>-0.04881985193498573905</td>\n",
       "      <td>-0.28928089961274144892</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66512840570207776292</td>\n",
       "      <td>-0.66887997113035013719</td>\n",
       "      <td>-0.66940629709101928668</td>\n",
       "      <td>-0.66993701490240276097</td>\n",
       "      <td>0.56352507736145363015</td>\n",
       "      <td>0.63713066122663675195</td>\n",
       "      <td>0.65222665089262377158</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 18:00:00</th>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.08078297699720485281</td>\n",
       "      <td>-0.04881985193498573905</td>\n",
       "      <td>-0.28928089961274144892</td>\n",
       "      <td>-0.32250069498507860644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66887997113035013719</td>\n",
       "      <td>-0.66940629709101928668</td>\n",
       "      <td>-0.66993701490240276097</td>\n",
       "      <td>-0.67046312662355056489</td>\n",
       "      <td>0.63713066122663675195</td>\n",
       "      <td>0.65222665089262377158</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 20:00:00</th>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.04881985193498573905</td>\n",
       "      <td>-0.28928089961274144892</td>\n",
       "      <td>-0.32250069498507860644</td>\n",
       "      <td>-0.37983398088034736606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66940629709101928668</td>\n",
       "      <td>-0.66993701490240276097</td>\n",
       "      <td>-0.67046312662355056489</td>\n",
       "      <td>-0.67099751323298362227</td>\n",
       "      <td>0.65222665089262377158</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 22:00:00</th>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-0.28928089961274144892</td>\n",
       "      <td>-0.32250069498507860644</td>\n",
       "      <td>-0.37983398088034736606</td>\n",
       "      <td>-0.37770310629331527164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66993701490240276097</td>\n",
       "      <td>-0.67046312662355056489</td>\n",
       "      <td>-0.67099751323298362227</td>\n",
       "      <td>-0.67152686529143834626</td>\n",
       "      <td>0.25278916983891314141</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 12:00:00</th>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-0.32250069498507860644</td>\n",
       "      <td>-0.37983398088034736606</td>\n",
       "      <td>-0.37770310629331527164</td>\n",
       "      <td>-0.49368098684551747768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67046312662355056489</td>\n",
       "      <td>-0.67099751323298362227</td>\n",
       "      <td>-0.67152686529143834626</td>\n",
       "      <td>-0.67522284968184875797</td>\n",
       "      <td>0.26873645720266514658</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 14:00:00</th>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-0.37983398088034736606</td>\n",
       "      <td>-0.37770310629331527164</td>\n",
       "      <td>-0.49368098684551747768</td>\n",
       "      <td>-0.42915372000985402279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67099751323298362227</td>\n",
       "      <td>-0.67152686529143834626</td>\n",
       "      <td>-0.67522284968184875797</td>\n",
       "      <td>-0.67574128905249997157</td>\n",
       "      <td>0.17080197715296660532</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 16:00:00</th>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-0.37770310629331527164</td>\n",
       "      <td>-0.49368098684551747768</td>\n",
       "      <td>-0.42915372000985402279</td>\n",
       "      <td>-0.56574827084875922711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67152686529143834626</td>\n",
       "      <td>-0.67522284968184875797</td>\n",
       "      <td>-0.67574128905249997157</td>\n",
       "      <td>-0.67626286163611948332</td>\n",
       "      <td>-0.06117509655565678967</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 18:00:00</th>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-0.49368098684551747768</td>\n",
       "      <td>-0.42915372000985402279</td>\n",
       "      <td>-0.56574827084875922711</td>\n",
       "      <td>-0.61807309040649816012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67522284968184875797</td>\n",
       "      <td>-0.67574128905249997157</td>\n",
       "      <td>-0.67626286163611948332</td>\n",
       "      <td>-0.67678084575579311455</td>\n",
       "      <td>0.01692947625373457388</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 20:00:00</th>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-0.42915372000985402279</td>\n",
       "      <td>-0.56574827084875922711</td>\n",
       "      <td>-0.61807309040649816012</td>\n",
       "      <td>-0.61639753051443624265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67574128905249997157</td>\n",
       "      <td>-0.67626286163611948332</td>\n",
       "      <td>-0.67678084575579311455</td>\n",
       "      <td>-0.67730756002388103898</td>\n",
       "      <td>0.08289107318357590015</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 22:00:00</th>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-0.56574827084875922711</td>\n",
       "      <td>-0.61807309040649816012</td>\n",
       "      <td>-0.61639753051443624265</td>\n",
       "      <td>-0.58352377606241256913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67626286163611948332</td>\n",
       "      <td>-0.67678084575579311455</td>\n",
       "      <td>-0.67730756002388103898</td>\n",
       "      <td>-0.67783107412628607058</td>\n",
       "      <td>-0.51452719076219877170</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 12:00:00</th>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-0.61807309040649816012</td>\n",
       "      <td>-0.61639753051443624265</td>\n",
       "      <td>-0.58352377606241256913</td>\n",
       "      <td>-0.72447113964009046683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67678084575579311455</td>\n",
       "      <td>-0.67730756002388103898</td>\n",
       "      <td>-0.67783107412628607058</td>\n",
       "      <td>-0.68147112935219744667</td>\n",
       "      <td>-0.49590981342955342548</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 14:00:00</th>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "      <td>-0.61639753051443624265</td>\n",
       "      <td>-0.58352377606241256913</td>\n",
       "      <td>-0.72447113964009046683</td>\n",
       "      <td>-0.63069442726636659735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67730756002388103898</td>\n",
       "      <td>-0.67783107412628607058</td>\n",
       "      <td>-0.68147112935219744667</td>\n",
       "      <td>-0.68198203026861814458</td>\n",
       "      <td>-0.52159522681151071488</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 16:00:00</th>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "      <td>-1.72967863716666281348</td>\n",
       "      <td>-0.58352377606241256913</td>\n",
       "      <td>-0.72447113964009046683</td>\n",
       "      <td>-0.63069442726636659735</td>\n",
       "      <td>-0.62756185972482148028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67783107412628607058</td>\n",
       "      <td>-0.68147112935219744667</td>\n",
       "      <td>-0.68198203026861814458</td>\n",
       "      <td>-0.68249518067714354252</td>\n",
       "      <td>-0.88853692153588392788</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 18:00:00</th>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "      <td>-1.72967863716666281348</td>\n",
       "      <td>-2.16029075477187593179</td>\n",
       "      <td>-0.72447113964009046683</td>\n",
       "      <td>-0.63069442726636659735</td>\n",
       "      <td>-0.62756185972482148028</td>\n",
       "      <td>-0.70104972782894203753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.68147112935219744667</td>\n",
       "      <td>-0.68198203026861814458</td>\n",
       "      <td>-0.68249518067714354252</td>\n",
       "      <td>-0.68300562634258765726</td>\n",
       "      <td>-0.82572537508552712460</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 20:00:00</th>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "      <td>-1.72967863716666281348</td>\n",
       "      <td>-2.16029075477187593179</td>\n",
       "      <td>-2.17038000974980249680</td>\n",
       "      <td>-0.63069442726636659735</td>\n",
       "      <td>-0.62756185972482148028</td>\n",
       "      <td>-0.70104972782894203753</td>\n",
       "      <td>-0.63914507695259314968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.68198203026861814458</td>\n",
       "      <td>-0.68249518067714354252</td>\n",
       "      <td>-0.68300562634258765726</td>\n",
       "      <td>-0.68352445402065553637</td>\n",
       "      <td>-0.68523364068911885028</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 22:00:00</th>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "      <td>-1.72967863716666281348</td>\n",
       "      <td>-2.16029075477187593179</td>\n",
       "      <td>-2.17038000974980249680</td>\n",
       "      <td>-1.89410772233276847309</td>\n",
       "      <td>-0.62756185972482148028</td>\n",
       "      <td>-0.70104972782894203753</td>\n",
       "      <td>-0.63914507695259314968</td>\n",
       "      <td>-0.56960934377939997919</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.68249518067714354252</td>\n",
       "      <td>-0.68300562634258765726</td>\n",
       "      <td>-0.68352445402065553637</td>\n",
       "      <td>-0.68404266576124161947</td>\n",
       "      <td>-1.31830026768808261650</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 12:00:00</th>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "      <td>-1.72967863716666281348</td>\n",
       "      <td>-2.16029075477187593179</td>\n",
       "      <td>-2.17038000974980249680</td>\n",
       "      <td>-1.89410772233276847309</td>\n",
       "      <td>-2.12535980537703927951</td>\n",
       "      <td>-0.70104972782894203753</td>\n",
       "      <td>-0.63914507695259314968</td>\n",
       "      <td>-0.56960934377939997919</td>\n",
       "      <td>-0.71928054773756555651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.68300562634258765726</td>\n",
       "      <td>-0.68352445402065553637</td>\n",
       "      <td>-0.68404266576124161947</td>\n",
       "      <td>-1.78977330157372338526</td>\n",
       "      <td>-1.31182839247877947031</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 14:00:00</th>\n",
       "      <td>-1.72967863716666281348</td>\n",
       "      <td>-2.16029075477187593179</td>\n",
       "      <td>-2.17038000974980249680</td>\n",
       "      <td>-1.89410772233276847309</td>\n",
       "      <td>-2.12535980537703927951</td>\n",
       "      <td>-2.18285643087869507539</td>\n",
       "      <td>-0.63914507695259314968</td>\n",
       "      <td>-0.56960934377939997919</td>\n",
       "      <td>-0.71928054773756555651</td>\n",
       "      <td>-0.60949495360695882251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.68352445402065553637</td>\n",
       "      <td>-0.68404266576124161947</td>\n",
       "      <td>-1.78977330157372338526</td>\n",
       "      <td>-1.79027824403752044979</td>\n",
       "      <td>-1.22957595968509281192</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 16:00:00</th>\n",
       "      <td>-2.16029075477187593179</td>\n",
       "      <td>-2.17038000974980249680</td>\n",
       "      <td>-1.89410772233276847309</td>\n",
       "      <td>-2.12535980537703927951</td>\n",
       "      <td>-2.18285643087869507539</td>\n",
       "      <td>-1.88218956837971873597</td>\n",
       "      <td>-0.56960934377939997919</td>\n",
       "      <td>-0.71928054773756555651</td>\n",
       "      <td>-0.60949495360695882251</td>\n",
       "      <td>-0.47084237684251628586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.68404266576124161947</td>\n",
       "      <td>-1.78977330157372338526</td>\n",
       "      <td>-1.79027824403752044979</td>\n",
       "      <td>-1.79078506106739254733</td>\n",
       "      <td>-1.66510892312715230723</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "      <td>-1.72967863716666281348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 18:00:00</th>\n",
       "      <td>-2.17038000974980249680</td>\n",
       "      <td>-1.89410772233276847309</td>\n",
       "      <td>-2.12535980537703927951</td>\n",
       "      <td>-2.18285643087869507539</td>\n",
       "      <td>-1.88218956837971873597</td>\n",
       "      <td>-2.24550378008529305518</td>\n",
       "      <td>-0.71928054773756555651</td>\n",
       "      <td>-0.60949495360695882251</td>\n",
       "      <td>-0.47084237684251628586</td>\n",
       "      <td>-0.55788042609965926566</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.78977330157372338526</td>\n",
       "      <td>-1.79027824403752044979</td>\n",
       "      <td>-1.79078506106739254733</td>\n",
       "      <td>-1.79128978929166704503</td>\n",
       "      <td>-1.63402522791579007944</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "      <td>-1.72967863716666281348</td>\n",
       "      <td>-2.16029075477187593179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 20:00:00</th>\n",
       "      <td>-1.89410772233276847309</td>\n",
       "      <td>-2.12535980537703927951</td>\n",
       "      <td>-2.18285643087869507539</td>\n",
       "      <td>-1.88218956837971873597</td>\n",
       "      <td>-2.24550378008529305518</td>\n",
       "      <td>-2.29702364835405647980</td>\n",
       "      <td>-0.60949495360695882251</td>\n",
       "      <td>-0.47084237684251628586</td>\n",
       "      <td>-0.55788042609965926566</td>\n",
       "      <td>-0.45044425723410597140</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.79027824403752044979</td>\n",
       "      <td>-1.79078506106739254733</td>\n",
       "      <td>-1.79128978929166704503</td>\n",
       "      <td>-1.79180418494739268631</td>\n",
       "      <td>-1.41494896192611729902</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "      <td>-1.72967863716666281348</td>\n",
       "      <td>-2.16029075477187593179</td>\n",
       "      <td>-2.17038000974980249680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 22:00:00</th>\n",
       "      <td>-2.12535980537703927951</td>\n",
       "      <td>-2.18285643087869507539</td>\n",
       "      <td>-1.88218956837971873597</td>\n",
       "      <td>-2.24550378008529305518</td>\n",
       "      <td>-2.29702364835405647980</td>\n",
       "      <td>-2.00442263140920307052</td>\n",
       "      <td>-0.47084237684251628586</td>\n",
       "      <td>-0.55788042609965926566</td>\n",
       "      <td>-0.45044425723410597140</td>\n",
       "      <td>-0.34748839134934617068</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.79078506106739254733</td>\n",
       "      <td>-1.79128978929166704503</td>\n",
       "      <td>-1.79180418494739268631</td>\n",
       "      <td>-1.79231758975791377608</td>\n",
       "      <td>-1.90960283614629178217</td>\n",
       "      <td>-1.93036992820924346859</td>\n",
       "      <td>-1.72967863716666281348</td>\n",
       "      <td>-2.16029075477187593179</td>\n",
       "      <td>-2.17038000974980249680</td>\n",
       "      <td>-1.89410772233276847309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                               target                          \\\n",
       "feature                                   y                           \n",
       "time step                               t+1                     t+2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  0.80658668930844723555  0.51029756466224507339   \n",
       "2017-11-03 12:00:00  0.51029756466224507339  0.51543818338990998740   \n",
       "2017-11-03 14:00:00  0.51543818338990998740  0.57610759556977997331   \n",
       "2017-11-03 16:00:00  0.57610759556977997331  0.44407830463047193170   \n",
       "2017-11-03 18:00:00  0.44407830463047193170  0.53415155282490367483   \n",
       "2017-11-03 20:00:00  0.53415155282490367483  0.99120691392514004292   \n",
       "2017-11-03 22:00:00  0.99120691392514004292  0.78575139383041936458   \n",
       "2017-11-04 12:00:00  0.78575139383041936458  0.76883408392202723380   \n",
       "2017-11-04 14:00:00  0.76883408392202723380  0.79552235929752979082   \n",
       "2017-11-04 16:00:00  0.79552235929752979082  0.71031438474593744381   \n",
       "2017-11-04 18:00:00  0.71031438474593744381  0.75033795768062416798   \n",
       "2017-11-04 20:00:00  0.75033795768062416798  1.29779243521901799241   \n",
       "2017-11-04 22:00:00  1.29779243521901799241  1.24604269253540334006   \n",
       "2017-11-05 12:00:00  1.24604269253540334006  1.23522339440477324501   \n",
       "2017-11-05 14:00:00  1.23522339440477324501  1.09379447518572048281   \n",
       "2017-11-05 16:00:00  1.09379447518572048281  1.10047854455312421607   \n",
       "2017-11-05 18:00:00  1.10047854455312421607  1.08940158332075420766   \n",
       "2017-11-05 20:00:00  1.08940158332075420766  1.33051304520138224952   \n",
       "2017-11-05 22:00:00  1.33051304520138224952  1.43746318512219928287   \n",
       "2017-11-06 12:00:00  1.43746318512219928287  1.47237645253395066369   \n",
       "2017-11-06 14:00:00  1.47237645253395066369  1.10187042571692872528   \n",
       "2017-11-06 16:00:00  1.10187042571692872528  1.19661191258787025227   \n",
       "2017-11-06 18:00:00  1.19661191258787025227  1.16981754013331751274   \n",
       "2017-11-06 20:00:00  1.16981754013331751274  0.88633322937738301395   \n",
       "2017-11-06 22:00:00  0.88633322937738301395  1.10333051374924528965   \n",
       "2017-11-07 12:00:00  1.10333051374924528965  1.21126330916306468310   \n",
       "2017-11-07 14:00:00  1.21126330916306468310  0.68133493359471564155   \n",
       "2017-11-07 16:00:00  0.68133493359471564155  0.81522090874259967030   \n",
       "2017-11-07 18:00:00  0.81522090874259967030  0.82265270324617567610   \n",
       "2017-11-07 20:00:00  0.82265270324617567610  0.10960713551924922138   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  0.74153449024088802233  0.62209529624281612037   \n",
       "2017-11-15 14:00:00  0.62209529624281612037  0.56352507736145363015   \n",
       "2017-11-15 16:00:00  0.56352507736145363015  0.63713066122663675195   \n",
       "2017-11-15 18:00:00  0.63713066122663675195  0.65222665089262377158   \n",
       "2017-11-15 20:00:00  0.65222665089262377158  0.25278916983891314141   \n",
       "2017-11-15 22:00:00  0.25278916983891314141  0.26873645720266514658   \n",
       "2017-11-16 12:00:00  0.26873645720266514658  0.17080197715296660532   \n",
       "2017-11-16 14:00:00  0.17080197715296660532 -0.06117509655565678967   \n",
       "2017-11-16 16:00:00 -0.06117509655565678967  0.01692947625373457388   \n",
       "2017-11-16 18:00:00  0.01692947625373457388  0.08289107318357590015   \n",
       "2017-11-16 20:00:00  0.08289107318357590015 -0.51452719076219877170   \n",
       "2017-11-16 22:00:00 -0.51452719076219877170 -0.49590981342955342548   \n",
       "2017-11-17 12:00:00 -0.49590981342955342548 -0.52159522681151071488   \n",
       "2017-11-17 14:00:00 -0.52159522681151071488 -0.88853692153588392788   \n",
       "2017-11-17 16:00:00 -0.88853692153588392788 -0.82572537508552712460   \n",
       "2017-11-17 18:00:00 -0.82572537508552712460 -0.68523364068911885028   \n",
       "2017-11-17 20:00:00 -0.68523364068911885028 -1.31830026768808261650   \n",
       "2017-11-17 22:00:00 -1.31830026768808261650 -1.31182839247877947031   \n",
       "2017-11-18 12:00:00 -1.31182839247877947031 -1.22957595968509281192   \n",
       "2017-11-18 14:00:00 -1.22957595968509281192 -1.66510892312715230723   \n",
       "2017-11-18 16:00:00 -1.66510892312715230723 -1.63402522791579007944   \n",
       "2017-11-18 18:00:00 -1.63402522791579007944 -1.41494896192611729902   \n",
       "2017-11-18 20:00:00 -1.41494896192611729902 -1.90960283614629178217   \n",
       "2017-11-18 22:00:00 -1.90960283614629178217 -1.93036992820924346859   \n",
       "2017-11-19 12:00:00 -1.93036992820924346859 -1.72967863716666281348   \n",
       "2017-11-19 14:00:00 -1.72967863716666281348 -2.16029075477187593179   \n",
       "2017-11-19 16:00:00 -2.16029075477187593179 -2.17038000974980249680   \n",
       "2017-11-19 18:00:00 -2.17038000974980249680 -1.89410772233276847309   \n",
       "2017-11-19 20:00:00 -1.89410772233276847309 -2.12535980537703927951   \n",
       "2017-11-19 22:00:00 -2.12535980537703927951 -2.18285643087869507539   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+3                     t+4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  0.51543818338990998740  0.57610759556977997331   \n",
       "2017-11-03 12:00:00  0.57610759556977997331  0.44407830463047193170   \n",
       "2017-11-03 14:00:00  0.44407830463047193170  0.53415155282490367483   \n",
       "2017-11-03 16:00:00  0.53415155282490367483  0.99120691392514004292   \n",
       "2017-11-03 18:00:00  0.99120691392514004292  0.78575139383041936458   \n",
       "2017-11-03 20:00:00  0.78575139383041936458  0.76883408392202723380   \n",
       "2017-11-03 22:00:00  0.76883408392202723380  0.79552235929752979082   \n",
       "2017-11-04 12:00:00  0.79552235929752979082  0.71031438474593744381   \n",
       "2017-11-04 14:00:00  0.71031438474593744381  0.75033795768062416798   \n",
       "2017-11-04 16:00:00  0.75033795768062416798  1.29779243521901799241   \n",
       "2017-11-04 18:00:00  1.29779243521901799241  1.24604269253540334006   \n",
       "2017-11-04 20:00:00  1.24604269253540334006  1.23522339440477324501   \n",
       "2017-11-04 22:00:00  1.23522339440477324501  1.09379447518572048281   \n",
       "2017-11-05 12:00:00  1.09379447518572048281  1.10047854455312421607   \n",
       "2017-11-05 14:00:00  1.10047854455312421607  1.08940158332075420766   \n",
       "2017-11-05 16:00:00  1.08940158332075420766  1.33051304520138224952   \n",
       "2017-11-05 18:00:00  1.33051304520138224952  1.43746318512219928287   \n",
       "2017-11-05 20:00:00  1.43746318512219928287  1.47237645253395066369   \n",
       "2017-11-05 22:00:00  1.47237645253395066369  1.10187042571692872528   \n",
       "2017-11-06 12:00:00  1.10187042571692872528  1.19661191258787025227   \n",
       "2017-11-06 14:00:00  1.19661191258787025227  1.16981754013331751274   \n",
       "2017-11-06 16:00:00  1.16981754013331751274  0.88633322937738301395   \n",
       "2017-11-06 18:00:00  0.88633322937738301395  1.10333051374924528965   \n",
       "2017-11-06 20:00:00  1.10333051374924528965  1.21126330916306468310   \n",
       "2017-11-06 22:00:00  1.21126330916306468310  0.68133493359471564155   \n",
       "2017-11-07 12:00:00  0.68133493359471564155  0.81522090874259967030   \n",
       "2017-11-07 14:00:00  0.81522090874259967030  0.82265270324617567610   \n",
       "2017-11-07 16:00:00  0.82265270324617567610  0.10960713551924922138   \n",
       "2017-11-07 18:00:00  0.10960713551924922138  0.36331879759109730887   \n",
       "2017-11-07 20:00:00  0.36331879759109730887  0.54247506148076651833   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  0.56352507736145363015  0.63713066122663675195   \n",
       "2017-11-15 14:00:00  0.63713066122663675195  0.65222665089262377158   \n",
       "2017-11-15 16:00:00  0.65222665089262377158  0.25278916983891314141   \n",
       "2017-11-15 18:00:00  0.25278916983891314141  0.26873645720266514658   \n",
       "2017-11-15 20:00:00  0.26873645720266514658  0.17080197715296660532   \n",
       "2017-11-15 22:00:00  0.17080197715296660532 -0.06117509655565678967   \n",
       "2017-11-16 12:00:00 -0.06117509655565678967  0.01692947625373457388   \n",
       "2017-11-16 14:00:00  0.01692947625373457388  0.08289107318357590015   \n",
       "2017-11-16 16:00:00  0.08289107318357590015 -0.51452719076219877170   \n",
       "2017-11-16 18:00:00 -0.51452719076219877170 -0.49590981342955342548   \n",
       "2017-11-16 20:00:00 -0.49590981342955342548 -0.52159522681151071488   \n",
       "2017-11-16 22:00:00 -0.52159522681151071488 -0.88853692153588392788   \n",
       "2017-11-17 12:00:00 -0.88853692153588392788 -0.82572537508552712460   \n",
       "2017-11-17 14:00:00 -0.82572537508552712460 -0.68523364068911885028   \n",
       "2017-11-17 16:00:00 -0.68523364068911885028 -1.31830026768808261650   \n",
       "2017-11-17 18:00:00 -1.31830026768808261650 -1.31182839247877947031   \n",
       "2017-11-17 20:00:00 -1.31182839247877947031 -1.22957595968509281192   \n",
       "2017-11-17 22:00:00 -1.22957595968509281192 -1.66510892312715230723   \n",
       "2017-11-18 12:00:00 -1.66510892312715230723 -1.63402522791579007944   \n",
       "2017-11-18 14:00:00 -1.63402522791579007944 -1.41494896192611729902   \n",
       "2017-11-18 16:00:00 -1.41494896192611729902 -1.90960283614629178217   \n",
       "2017-11-18 18:00:00 -1.90960283614629178217 -1.93036992820924346859   \n",
       "2017-11-18 20:00:00 -1.93036992820924346859 -1.72967863716666281348   \n",
       "2017-11-18 22:00:00 -1.72967863716666281348 -2.16029075477187593179   \n",
       "2017-11-19 12:00:00 -2.16029075477187593179 -2.17038000974980249680   \n",
       "2017-11-19 14:00:00 -2.17038000974980249680 -1.89410772233276847309   \n",
       "2017-11-19 16:00:00 -1.89410772233276847309 -2.12535980537703927951   \n",
       "2017-11-19 18:00:00 -2.12535980537703927951 -2.18285643087869507539   \n",
       "2017-11-19 20:00:00 -2.18285643087869507539 -1.88218956837971873597   \n",
       "2017-11-19 22:00:00 -1.88218956837971873597 -2.24550378008529305518   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+5                     t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  0.44407830463047193170  0.53415155282490367483   \n",
       "2017-11-03 12:00:00  0.53415155282490367483  0.99120691392514004292   \n",
       "2017-11-03 14:00:00  0.99120691392514004292  0.78575139383041936458   \n",
       "2017-11-03 16:00:00  0.78575139383041936458  0.76883408392202723380   \n",
       "2017-11-03 18:00:00  0.76883408392202723380  0.79552235929752979082   \n",
       "2017-11-03 20:00:00  0.79552235929752979082  0.71031438474593744381   \n",
       "2017-11-03 22:00:00  0.71031438474593744381  0.75033795768062416798   \n",
       "2017-11-04 12:00:00  0.75033795768062416798  1.29779243521901799241   \n",
       "2017-11-04 14:00:00  1.29779243521901799241  1.24604269253540334006   \n",
       "2017-11-04 16:00:00  1.24604269253540334006  1.23522339440477324501   \n",
       "2017-11-04 18:00:00  1.23522339440477324501  1.09379447518572048281   \n",
       "2017-11-04 20:00:00  1.09379447518572048281  1.10047854455312421607   \n",
       "2017-11-04 22:00:00  1.10047854455312421607  1.08940158332075420766   \n",
       "2017-11-05 12:00:00  1.08940158332075420766  1.33051304520138224952   \n",
       "2017-11-05 14:00:00  1.33051304520138224952  1.43746318512219928287   \n",
       "2017-11-05 16:00:00  1.43746318512219928287  1.47237645253395066369   \n",
       "2017-11-05 18:00:00  1.47237645253395066369  1.10187042571692872528   \n",
       "2017-11-05 20:00:00  1.10187042571692872528  1.19661191258787025227   \n",
       "2017-11-05 22:00:00  1.19661191258787025227  1.16981754013331751274   \n",
       "2017-11-06 12:00:00  1.16981754013331751274  0.88633322937738301395   \n",
       "2017-11-06 14:00:00  0.88633322937738301395  1.10333051374924528965   \n",
       "2017-11-06 16:00:00  1.10333051374924528965  1.21126330916306468310   \n",
       "2017-11-06 18:00:00  1.21126330916306468310  0.68133493359471564155   \n",
       "2017-11-06 20:00:00  0.68133493359471564155  0.81522090874259967030   \n",
       "2017-11-06 22:00:00  0.81522090874259967030  0.82265270324617567610   \n",
       "2017-11-07 12:00:00  0.82265270324617567610  0.10960713551924922138   \n",
       "2017-11-07 14:00:00  0.10960713551924922138  0.36331879759109730887   \n",
       "2017-11-07 16:00:00  0.36331879759109730887  0.54247506148076651833   \n",
       "2017-11-07 18:00:00  0.54247506148076651833  0.01593671751055668329   \n",
       "2017-11-07 20:00:00  0.01593671751055668329  0.12994226398614291962   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  0.65222665089262377158  0.25278916983891314141   \n",
       "2017-11-15 14:00:00  0.25278916983891314141  0.26873645720266514658   \n",
       "2017-11-15 16:00:00  0.26873645720266514658  0.17080197715296660532   \n",
       "2017-11-15 18:00:00  0.17080197715296660532 -0.06117509655565678967   \n",
       "2017-11-15 20:00:00 -0.06117509655565678967  0.01692947625373457388   \n",
       "2017-11-15 22:00:00  0.01692947625373457388  0.08289107318357590015   \n",
       "2017-11-16 12:00:00  0.08289107318357590015 -0.51452719076219877170   \n",
       "2017-11-16 14:00:00 -0.51452719076219877170 -0.49590981342955342548   \n",
       "2017-11-16 16:00:00 -0.49590981342955342548 -0.52159522681151071488   \n",
       "2017-11-16 18:00:00 -0.52159522681151071488 -0.88853692153588392788   \n",
       "2017-11-16 20:00:00 -0.88853692153588392788 -0.82572537508552712460   \n",
       "2017-11-16 22:00:00 -0.82572537508552712460 -0.68523364068911885028   \n",
       "2017-11-17 12:00:00 -0.68523364068911885028 -1.31830026768808261650   \n",
       "2017-11-17 14:00:00 -1.31830026768808261650 -1.31182839247877947031   \n",
       "2017-11-17 16:00:00 -1.31182839247877947031 -1.22957595968509281192   \n",
       "2017-11-17 18:00:00 -1.22957595968509281192 -1.66510892312715230723   \n",
       "2017-11-17 20:00:00 -1.66510892312715230723 -1.63402522791579007944   \n",
       "2017-11-17 22:00:00 -1.63402522791579007944 -1.41494896192611729902   \n",
       "2017-11-18 12:00:00 -1.41494896192611729902 -1.90960283614629178217   \n",
       "2017-11-18 14:00:00 -1.90960283614629178217 -1.93036992820924346859   \n",
       "2017-11-18 16:00:00 -1.93036992820924346859 -1.72967863716666281348   \n",
       "2017-11-18 18:00:00 -1.72967863716666281348 -2.16029075477187593179   \n",
       "2017-11-18 20:00:00 -2.16029075477187593179 -2.17038000974980249680   \n",
       "2017-11-18 22:00:00 -2.17038000974980249680 -1.89410772233276847309   \n",
       "2017-11-19 12:00:00 -1.89410772233276847309 -2.12535980537703927951   \n",
       "2017-11-19 14:00:00 -2.12535980537703927951 -2.18285643087869507539   \n",
       "2017-11-19 16:00:00 -2.18285643087869507539 -1.88218956837971873597   \n",
       "2017-11-19 18:00:00 -1.88218956837971873597 -2.24550378008529305518   \n",
       "2017-11-19 20:00:00 -2.24550378008529305518 -2.29702364835405647980   \n",
       "2017-11-19 22:00:00 -2.29702364835405647980 -2.00442263140920307052   \n",
       "\n",
       "tensor                                    X                          \\\n",
       "feature                                   e                           \n",
       "time step                               t-5                     t-4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00 -2.92617496977098268118 -2.79400608189676713522   \n",
       "2017-11-03 12:00:00 -2.79400608189676713522 -2.73539791391728170922   \n",
       "2017-11-03 14:00:00 -2.73539791391728170922 -2.70977277572862673694   \n",
       "2017-11-03 16:00:00 -2.70977277572862673694 -2.67152630068010399356   \n",
       "2017-11-03 18:00:00 -2.67152630068010399356 -2.47947436274205035289   \n",
       "2017-11-03 20:00:00 -2.47947436274205035289 -1.95303896117129727195   \n",
       "2017-11-03 22:00:00 -1.95303896117129727195 -1.77839827418765583644   \n",
       "2017-11-04 12:00:00 -1.77839827418765583644 -1.72634664376002766595   \n",
       "2017-11-04 14:00:00 -1.72634664376002766595 -1.69933734814780645905   \n",
       "2017-11-04 16:00:00 -1.69933734814780645905 -1.63533824623316559332   \n",
       "2017-11-04 18:00:00 -1.63533824623316559332 -1.50637477769096883584   \n",
       "2017-11-04 20:00:00 -1.50637477769096883584 -0.99163187025148802345   \n",
       "2017-11-04 22:00:00 -0.99163187025148802345 -0.82061549218194562716   \n",
       "2017-11-05 12:00:00 -0.82061549218194562716 -0.73887731173082793479   \n",
       "2017-11-05 14:00:00 -0.73887731173082793479 -0.74412254364959273811   \n",
       "2017-11-05 16:00:00 -0.74412254364959273811 -0.67536995275316269449   \n",
       "2017-11-05 18:00:00 -0.67536995275316269449 -0.59774782258829062265   \n",
       "2017-11-05 20:00:00 -0.59774782258829062265 -0.22919750908055919192   \n",
       "2017-11-05 22:00:00 -0.22919750908055919192 -0.11678930055519548548   \n",
       "2017-11-06 12:00:00 -0.11678930055519548548  0.03078745392909986237   \n",
       "2017-11-06 14:00:00  0.03078745392909986237 -0.02026248354428677664   \n",
       "2017-11-06 16:00:00 -0.02026248354428677664  0.01840288108604457018   \n",
       "2017-11-06 18:00:00  0.01840288108604457018  0.08735581088639735037   \n",
       "2017-11-06 20:00:00  0.08735581088639735037  0.25296304481097164896   \n",
       "2017-11-06 22:00:00  0.25296304481097164896  0.26778810648378881254   \n",
       "2017-11-07 12:00:00  0.26778810648378881254  0.49464433768609061826   \n",
       "2017-11-07 14:00:00  0.49464433768609061826  0.41673080505272314111   \n",
       "2017-11-07 16:00:00  0.41673080505272314111  0.39835428469199501977   \n",
       "2017-11-07 18:00:00  0.39835428469199501977  0.50655538249538389906   \n",
       "2017-11-07 20:00:00  0.50655538249538389906  0.51622627581428048860   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  0.57246680675461392163  0.46566807971331197757   \n",
       "2017-11-15 14:00:00  0.46566807971331197757  0.45284640394979230882   \n",
       "2017-11-15 16:00:00  0.45284640394979230882  0.40527507493839998176   \n",
       "2017-11-15 18:00:00  0.40527507493839998176  0.41707684440847908602   \n",
       "2017-11-15 20:00:00  0.41707684440847908602  0.13906140039398354191   \n",
       "2017-11-15 22:00:00  0.13906140039398354191  0.11345447554412074753   \n",
       "2017-11-16 12:00:00  0.11345447554412074753  0.01537958731915608362   \n",
       "2017-11-16 14:00:00  0.01537958731915608362  0.00091877834102239194   \n",
       "2017-11-16 16:00:00  0.00091877834102239194 -0.08078297699720485281   \n",
       "2017-11-16 18:00:00 -0.08078297699720485281 -0.04881985193498573905   \n",
       "2017-11-16 20:00:00 -0.04881985193498573905 -0.28928089961274144892   \n",
       "2017-11-16 22:00:00 -0.28928089961274144892 -0.32250069498507860644   \n",
       "2017-11-17 12:00:00 -0.32250069498507860644 -0.37983398088034736606   \n",
       "2017-11-17 14:00:00 -0.37983398088034736606 -0.37770310629331527164   \n",
       "2017-11-17 16:00:00 -0.37770310629331527164 -0.49368098684551747768   \n",
       "2017-11-17 18:00:00 -0.49368098684551747768 -0.42915372000985402279   \n",
       "2017-11-17 20:00:00 -0.42915372000985402279 -0.56574827084875922711   \n",
       "2017-11-17 22:00:00 -0.56574827084875922711 -0.61807309040649816012   \n",
       "2017-11-18 12:00:00 -0.61807309040649816012 -0.61639753051443624265   \n",
       "2017-11-18 14:00:00 -0.61639753051443624265 -0.58352377606241256913   \n",
       "2017-11-18 16:00:00 -0.58352377606241256913 -0.72447113964009046683   \n",
       "2017-11-18 18:00:00 -0.72447113964009046683 -0.63069442726636659735   \n",
       "2017-11-18 20:00:00 -0.63069442726636659735 -0.62756185972482148028   \n",
       "2017-11-18 22:00:00 -0.62756185972482148028 -0.70104972782894203753   \n",
       "2017-11-19 12:00:00 -0.70104972782894203753 -0.63914507695259314968   \n",
       "2017-11-19 14:00:00 -0.63914507695259314968 -0.56960934377939997919   \n",
       "2017-11-19 16:00:00 -0.56960934377939997919 -0.71928054773756555651   \n",
       "2017-11-19 18:00:00 -0.71928054773756555651 -0.60949495360695882251   \n",
       "2017-11-19 20:00:00 -0.60949495360695882251 -0.47084237684251628586   \n",
       "2017-11-19 22:00:00 -0.47084237684251628586 -0.55788042609965926566   \n",
       "\n",
       "tensor                                                               ...  \\\n",
       "feature                                                              ...   \n",
       "time step                               t-3                     t-2  ...   \n",
       "Epoch_Time_of_Clock                                                  ...   \n",
       "2017-11-02 22:00:00 -2.73539791391728170922 -2.70977277572862673694  ...   \n",
       "2017-11-03 12:00:00 -2.70977277572862673694 -2.67152630068010399356  ...   \n",
       "2017-11-03 14:00:00 -2.67152630068010399356 -2.47947436274205035289  ...   \n",
       "2017-11-03 16:00:00 -2.47947436274205035289 -1.95303896117129727195  ...   \n",
       "2017-11-03 18:00:00 -1.95303896117129727195 -1.77839827418765583644  ...   \n",
       "2017-11-03 20:00:00 -1.77839827418765583644 -1.72634664376002766595  ...   \n",
       "2017-11-03 22:00:00 -1.72634664376002766595 -1.69933734814780645905  ...   \n",
       "2017-11-04 12:00:00 -1.69933734814780645905 -1.63533824623316559332  ...   \n",
       "2017-11-04 14:00:00 -1.63533824623316559332 -1.50637477769096883584  ...   \n",
       "2017-11-04 16:00:00 -1.50637477769096883584 -0.99163187025148802345  ...   \n",
       "2017-11-04 18:00:00 -0.99163187025148802345 -0.82061549218194562716  ...   \n",
       "2017-11-04 20:00:00 -0.82061549218194562716 -0.73887731173082793479  ...   \n",
       "2017-11-04 22:00:00 -0.73887731173082793479 -0.74412254364959273811  ...   \n",
       "2017-11-05 12:00:00 -0.74412254364959273811 -0.67536995275316269449  ...   \n",
       "2017-11-05 14:00:00 -0.67536995275316269449 -0.59774782258829062265  ...   \n",
       "2017-11-05 16:00:00 -0.59774782258829062265 -0.22919750908055919192  ...   \n",
       "2017-11-05 18:00:00 -0.22919750908055919192 -0.11678930055519548548  ...   \n",
       "2017-11-05 20:00:00 -0.11678930055519548548  0.03078745392909986237  ...   \n",
       "2017-11-05 22:00:00  0.03078745392909986237 -0.02026248354428677664  ...   \n",
       "2017-11-06 12:00:00 -0.02026248354428677664  0.01840288108604457018  ...   \n",
       "2017-11-06 14:00:00  0.01840288108604457018  0.08735581088639735037  ...   \n",
       "2017-11-06 16:00:00  0.08735581088639735037  0.25296304481097164896  ...   \n",
       "2017-11-06 18:00:00  0.25296304481097164896  0.26778810648378881254  ...   \n",
       "2017-11-06 20:00:00  0.26778810648378881254  0.49464433768609061826  ...   \n",
       "2017-11-06 22:00:00  0.49464433768609061826  0.41673080505272314111  ...   \n",
       "2017-11-07 12:00:00  0.41673080505272314111  0.39835428469199501977  ...   \n",
       "2017-11-07 14:00:00  0.39835428469199501977  0.50655538249538389906  ...   \n",
       "2017-11-07 16:00:00  0.50655538249538389906  0.51622627581428048860  ...   \n",
       "2017-11-07 18:00:00  0.51622627581428048860  0.42709377864782482881  ...   \n",
       "2017-11-07 20:00:00  0.42709377864782482881  0.71315919454097298491  ...   \n",
       "...                                     ...                     ...  ...   \n",
       "2017-11-15 12:00:00  0.45284640394979230882  0.40527507493839998176  ...   \n",
       "2017-11-15 14:00:00  0.40527507493839998176  0.41707684440847908602  ...   \n",
       "2017-11-15 16:00:00  0.41707684440847908602  0.13906140039398354191  ...   \n",
       "2017-11-15 18:00:00  0.13906140039398354191  0.11345447554412074753  ...   \n",
       "2017-11-15 20:00:00  0.11345447554412074753  0.01537958731915608362  ...   \n",
       "2017-11-15 22:00:00  0.01537958731915608362  0.00091877834102239194  ...   \n",
       "2017-11-16 12:00:00  0.00091877834102239194 -0.08078297699720485281  ...   \n",
       "2017-11-16 14:00:00 -0.08078297699720485281 -0.04881985193498573905  ...   \n",
       "2017-11-16 16:00:00 -0.04881985193498573905 -0.28928089961274144892  ...   \n",
       "2017-11-16 18:00:00 -0.28928089961274144892 -0.32250069498507860644  ...   \n",
       "2017-11-16 20:00:00 -0.32250069498507860644 -0.37983398088034736606  ...   \n",
       "2017-11-16 22:00:00 -0.37983398088034736606 -0.37770310629331527164  ...   \n",
       "2017-11-17 12:00:00 -0.37770310629331527164 -0.49368098684551747768  ...   \n",
       "2017-11-17 14:00:00 -0.49368098684551747768 -0.42915372000985402279  ...   \n",
       "2017-11-17 16:00:00 -0.42915372000985402279 -0.56574827084875922711  ...   \n",
       "2017-11-17 18:00:00 -0.56574827084875922711 -0.61807309040649816012  ...   \n",
       "2017-11-17 20:00:00 -0.61807309040649816012 -0.61639753051443624265  ...   \n",
       "2017-11-17 22:00:00 -0.61639753051443624265 -0.58352377606241256913  ...   \n",
       "2017-11-18 12:00:00 -0.58352377606241256913 -0.72447113964009046683  ...   \n",
       "2017-11-18 14:00:00 -0.72447113964009046683 -0.63069442726636659735  ...   \n",
       "2017-11-18 16:00:00 -0.63069442726636659735 -0.62756185972482148028  ...   \n",
       "2017-11-18 18:00:00 -0.62756185972482148028 -0.70104972782894203753  ...   \n",
       "2017-11-18 20:00:00 -0.70104972782894203753 -0.63914507695259314968  ...   \n",
       "2017-11-18 22:00:00 -0.63914507695259314968 -0.56960934377939997919  ...   \n",
       "2017-11-19 12:00:00 -0.56960934377939997919 -0.71928054773756555651  ...   \n",
       "2017-11-19 14:00:00 -0.71928054773756555651 -0.60949495360695882251  ...   \n",
       "2017-11-19 16:00:00 -0.60949495360695882251 -0.47084237684251628586  ...   \n",
       "2017-11-19 18:00:00 -0.47084237684251628586 -0.55788042609965926566  ...   \n",
       "2017-11-19 20:00:00 -0.55788042609965926566 -0.45044425723410597140  ...   \n",
       "2017-11-19 22:00:00 -0.45044425723410597140 -0.34748839134934617068  ...   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                               OMEGA                           \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  1.62356028775279348508  1.62301501526409786003   \n",
       "2017-11-03 12:00:00  1.62301501526409786003  1.62246441365080329433   \n",
       "2017-11-03 14:00:00  1.62246441365080329433  1.62191460192390102257   \n",
       "2017-11-03 16:00:00  1.62191460192390102257  1.61808765196250115004   \n",
       "2017-11-03 18:00:00  1.61808765196250115004  1.61754653035883078083   \n",
       "2017-11-03 20:00:00  1.61754653035883078083  1.61700740369882267622   \n",
       "2017-11-03 22:00:00  1.61700740369882267622  1.61647116932270606959   \n",
       "2017-11-04 12:00:00  1.61647116932270606959  1.61592889611410139850   \n",
       "2017-11-04 14:00:00  1.61592889611410139850  1.61538859121451916501   \n",
       "2017-11-04 16:00:00  1.61538859121451916501  1.61163470931397379005   \n",
       "2017-11-04 18:00:00  1.61163470931397379005  1.61110484848813784176   \n",
       "2017-11-04 20:00:00  1.61110484848813784176  1.61057712996599766697   \n",
       "2017-11-04 22:00:00  1.61057712996599766697  1.61005214300463062038   \n",
       "2017-11-05 12:00:00  1.61005214300463062038  1.60952111721077728568   \n",
       "2017-11-05 14:00:00  1.60952111721077728568  1.60899302379006448049   \n",
       "2017-11-05 16:00:00  1.60899302379006448049  0.50318622687538083760   \n",
       "2017-11-05 18:00:00  0.50318622687538083760  0.50266746615002333431   \n",
       "2017-11-05 20:00:00  0.50266746615002333431  0.50215059338128265054   \n",
       "2017-11-05 22:00:00  0.50215059338128265054  0.50163637179345244022   \n",
       "2017-11-06 12:00:00  0.50163637179345244022  0.50111603106649127426   \n",
       "2017-11-06 14:00:00  0.50111603106649127426  0.50059917169744694299   \n",
       "2017-11-06 16:00:00  0.50059917169744694299  0.49699706313551883508   \n",
       "2017-11-06 18:00:00  0.49699706313551883508  0.49648617560963975714   \n",
       "2017-11-06 20:00:00  0.49648617560963975714  0.49597637265360822179   \n",
       "2017-11-06 22:00:00  0.49597637265360822179  0.49546970291054909241   \n",
       "2017-11-07 12:00:00  0.49546970291054909241  0.49495574911525747064   \n",
       "2017-11-07 14:00:00  0.49495574911525747064  0.49444550429879202680   \n",
       "2017-11-07 16:00:00  0.49444550429879202680  0.49087480819152135014   \n",
       "2017-11-07 18:00:00  0.49087480819152135014  0.49036662539956998863   \n",
       "2017-11-07 20:00:00  0.49036662539956998863  0.48985817481507704096   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00 -0.65755468387938431629 -0.65810306281827102293   \n",
       "2017-11-15 14:00:00 -0.65810306281827102293 -0.65864667500787377197   \n",
       "2017-11-15 16:00:00 -0.65864667500787377197 -0.66244412724316559249   \n",
       "2017-11-15 18:00:00 -0.66244412724316559249 -0.66297737572973403086   \n",
       "2017-11-15 20:00:00 -0.66297737572973403086 -0.66351639521048810799   \n",
       "2017-11-15 22:00:00 -0.66351639521048810799 -0.66404975081223938105   \n",
       "2017-11-16 12:00:00 -0.66404975081223938105 -0.66459178300023724617   \n",
       "2017-11-16 14:00:00 -0.66459178300023724617 -0.66512840570207776292   \n",
       "2017-11-16 16:00:00 -0.66512840570207776292 -0.66887997113035013719   \n",
       "2017-11-16 18:00:00 -0.66887997113035013719 -0.66940629709101928668   \n",
       "2017-11-16 20:00:00 -0.66940629709101928668 -0.66993701490240276097   \n",
       "2017-11-16 22:00:00 -0.66993701490240276097 -0.67046312662355056489   \n",
       "2017-11-17 12:00:00 -0.67046312662355056489 -0.67099751323298362227   \n",
       "2017-11-17 14:00:00 -0.67099751323298362227 -0.67152686529143834626   \n",
       "2017-11-17 16:00:00 -0.67152686529143834626 -0.67522284968184875797   \n",
       "2017-11-17 18:00:00 -0.67522284968184875797 -0.67574128905249997157   \n",
       "2017-11-17 20:00:00 -0.67574128905249997157 -0.67626286163611948332   \n",
       "2017-11-17 22:00:00 -0.67626286163611948332 -0.67678084575579311455   \n",
       "2017-11-18 12:00:00 -0.67678084575579311455 -0.67730756002388103898   \n",
       "2017-11-18 14:00:00 -0.67730756002388103898 -0.67783107412628607058   \n",
       "2017-11-18 16:00:00 -0.67783107412628607058 -0.68147112935219744667   \n",
       "2017-11-18 18:00:00 -0.68147112935219744667 -0.68198203026861814458   \n",
       "2017-11-18 20:00:00 -0.68198203026861814458 -0.68249518067714354252   \n",
       "2017-11-18 22:00:00 -0.68249518067714354252 -0.68300562634258765726   \n",
       "2017-11-19 12:00:00 -0.68300562634258765726 -0.68352445402065553637   \n",
       "2017-11-19 14:00:00 -0.68352445402065553637 -0.68404266576124161947   \n",
       "2017-11-19 16:00:00 -0.68404266576124161947 -1.78977330157372338526   \n",
       "2017-11-19 18:00:00 -1.78977330157372338526 -1.79027824403752044979   \n",
       "2017-11-19 20:00:00 -1.79027824403752044979 -1.79078506106739254733   \n",
       "2017-11-19 22:00:00 -1.79078506106739254733 -1.79128978929166704503   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-1                       t   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  1.62246441365080329433  1.62191460192390102257   \n",
       "2017-11-03 12:00:00  1.62191460192390102257  1.61808765196250115004   \n",
       "2017-11-03 14:00:00  1.61808765196250115004  1.61754653035883078083   \n",
       "2017-11-03 16:00:00  1.61754653035883078083  1.61700740369882267622   \n",
       "2017-11-03 18:00:00  1.61700740369882267622  1.61647116932270606959   \n",
       "2017-11-03 20:00:00  1.61647116932270606959  1.61592889611410139850   \n",
       "2017-11-03 22:00:00  1.61592889611410139850  1.61538859121451916501   \n",
       "2017-11-04 12:00:00  1.61538859121451916501  1.61163470931397379005   \n",
       "2017-11-04 14:00:00  1.61163470931397379005  1.61110484848813784176   \n",
       "2017-11-04 16:00:00  1.61110484848813784176  1.61057712996599766697   \n",
       "2017-11-04 18:00:00  1.61057712996599766697  1.61005214300463062038   \n",
       "2017-11-04 20:00:00  1.61005214300463062038  1.60952111721077728568   \n",
       "2017-11-04 22:00:00  1.60952111721077728568  1.60899302379006448049   \n",
       "2017-11-05 12:00:00  1.60899302379006448049  0.50318622687538083760   \n",
       "2017-11-05 14:00:00  0.50318622687538083760  0.50266746615002333431   \n",
       "2017-11-05 16:00:00  0.50266746615002333431  0.50215059338128265054   \n",
       "2017-11-05 18:00:00  0.50215059338128265054  0.50163637179345244022   \n",
       "2017-11-05 20:00:00  0.50163637179345244022  0.50111603106649127426   \n",
       "2017-11-05 22:00:00  0.50111603106649127426  0.50059917169744694299   \n",
       "2017-11-06 12:00:00  0.50059917169744694299  0.49699706313551883508   \n",
       "2017-11-06 14:00:00  0.49699706313551883508  0.49648617560963975714   \n",
       "2017-11-06 16:00:00  0.49648617560963975714  0.49597637265360822179   \n",
       "2017-11-06 18:00:00  0.49597637265360822179  0.49546970291054909241   \n",
       "2017-11-06 20:00:00  0.49546970291054909241  0.49495574911525747064   \n",
       "2017-11-06 22:00:00  0.49495574911525747064  0.49444550429879202680   \n",
       "2017-11-07 12:00:00  0.49444550429879202680  0.49087480819152135014   \n",
       "2017-11-07 14:00:00  0.49087480819152135014  0.49036662539956998863   \n",
       "2017-11-07 16:00:00  0.49036662539956998863  0.48985817481507704096   \n",
       "2017-11-07 18:00:00  0.48985817481507704096  0.48935398217587849912   \n",
       "2017-11-07 20:00:00  0.48935398217587849912  0.48884001499919776945   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00 -0.65864667500787377197 -0.66244412724316559249   \n",
       "2017-11-15 14:00:00 -0.66244412724316559249 -0.66297737572973403086   \n",
       "2017-11-15 16:00:00 -0.66297737572973403086 -0.66351639521048810799   \n",
       "2017-11-15 18:00:00 -0.66351639521048810799 -0.66404975081223938105   \n",
       "2017-11-15 20:00:00 -0.66404975081223938105 -0.66459178300023724617   \n",
       "2017-11-15 22:00:00 -0.66459178300023724617 -0.66512840570207776292   \n",
       "2017-11-16 12:00:00 -0.66512840570207776292 -0.66887997113035013719   \n",
       "2017-11-16 14:00:00 -0.66887997113035013719 -0.66940629709101928668   \n",
       "2017-11-16 16:00:00 -0.66940629709101928668 -0.66993701490240276097   \n",
       "2017-11-16 18:00:00 -0.66993701490240276097 -0.67046312662355056489   \n",
       "2017-11-16 20:00:00 -0.67046312662355056489 -0.67099751323298362227   \n",
       "2017-11-16 22:00:00 -0.67099751323298362227 -0.67152686529143834626   \n",
       "2017-11-17 12:00:00 -0.67152686529143834626 -0.67522284968184875797   \n",
       "2017-11-17 14:00:00 -0.67522284968184875797 -0.67574128905249997157   \n",
       "2017-11-17 16:00:00 -0.67574128905249997157 -0.67626286163611948332   \n",
       "2017-11-17 18:00:00 -0.67626286163611948332 -0.67678084575579311455   \n",
       "2017-11-17 20:00:00 -0.67678084575579311455 -0.67730756002388103898   \n",
       "2017-11-17 22:00:00 -0.67730756002388103898 -0.67783107412628607058   \n",
       "2017-11-18 12:00:00 -0.67783107412628607058 -0.68147112935219744667   \n",
       "2017-11-18 14:00:00 -0.68147112935219744667 -0.68198203026861814458   \n",
       "2017-11-18 16:00:00 -0.68198203026861814458 -0.68249518067714354252   \n",
       "2017-11-18 18:00:00 -0.68249518067714354252 -0.68300562634258765726   \n",
       "2017-11-18 20:00:00 -0.68300562634258765726 -0.68352445402065553637   \n",
       "2017-11-18 22:00:00 -0.68352445402065553637 -0.68404266576124161947   \n",
       "2017-11-19 12:00:00 -0.68404266576124161947 -1.78977330157372338526   \n",
       "2017-11-19 14:00:00 -1.78977330157372338526 -1.79027824403752044979   \n",
       "2017-11-19 16:00:00 -1.79027824403752044979 -1.79078506106739254733   \n",
       "2017-11-19 18:00:00 -1.79078506106739254733 -1.79128978929166704503   \n",
       "2017-11-19 20:00:00 -1.79128978929166704503 -1.79180418494739268631   \n",
       "2017-11-19 22:00:00 -1.79180418494739268631 -1.79231758975791377608   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                               omega                           \n",
       "time step                               t-5                     t-4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  1.03938727375447315104  0.74664732200838213050   \n",
       "2017-11-03 12:00:00  0.74664732200838213050  0.77581370487537448799   \n",
       "2017-11-03 14:00:00  0.77581370487537448799  0.76532532575010336906   \n",
       "2017-11-03 16:00:00  0.76532532575010336906  0.65055689667290861333   \n",
       "2017-11-03 18:00:00  0.65055689667290861333  0.75900249031931454891   \n",
       "2017-11-03 20:00:00  0.75900249031931454891  0.80658668930844723555   \n",
       "2017-11-03 22:00:00  0.80658668930844723555  0.51029756466224507339   \n",
       "2017-11-04 12:00:00  0.51029756466224507339  0.51543818338990998740   \n",
       "2017-11-04 14:00:00  0.51543818338990998740  0.57610759556977997331   \n",
       "2017-11-04 16:00:00  0.57610759556977997331  0.44407830463047193170   \n",
       "2017-11-04 18:00:00  0.44407830463047193170  0.53415155282490367483   \n",
       "2017-11-04 20:00:00  0.53415155282490367483  0.99120691392514004292   \n",
       "2017-11-04 22:00:00  0.99120691392514004292  0.78575139383041936458   \n",
       "2017-11-05 12:00:00  0.78575139383041936458  0.76883408392202723380   \n",
       "2017-11-05 14:00:00  0.76883408392202723380  0.79552235929752979082   \n",
       "2017-11-05 16:00:00  0.79552235929752979082  0.71031438474593744381   \n",
       "2017-11-05 18:00:00  0.71031438474593744381  0.75033795768062416798   \n",
       "2017-11-05 20:00:00  0.75033795768062416798  1.29779243521901799241   \n",
       "2017-11-05 22:00:00  1.29779243521901799241  1.24604269253540334006   \n",
       "2017-11-06 12:00:00  1.24604269253540334006  1.23522339440477324501   \n",
       "2017-11-06 14:00:00  1.23522339440477324501  1.09379447518572048281   \n",
       "2017-11-06 16:00:00  1.09379447518572048281  1.10047854455312421607   \n",
       "2017-11-06 18:00:00  1.10047854455312421607  1.08940158332075420766   \n",
       "2017-11-06 20:00:00  1.08940158332075420766  1.33051304520138224952   \n",
       "2017-11-06 22:00:00  1.33051304520138224952  1.43746318512219928287   \n",
       "2017-11-07 12:00:00  1.43746318512219928287  1.47237645253395066369   \n",
       "2017-11-07 14:00:00  1.47237645253395066369  1.10187042571692872528   \n",
       "2017-11-07 16:00:00  1.10187042571692872528  1.19661191258787025227   \n",
       "2017-11-07 18:00:00  1.19661191258787025227  1.16981754013331751274   \n",
       "2017-11-07 20:00:00  1.16981754013331751274  0.88633322937738301395   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  0.78038089896137685653  0.69162627602713089292   \n",
       "2017-11-15 14:00:00  0.69162627602713089292  0.79945550233279349950   \n",
       "2017-11-15 16:00:00  0.79945550233279349950  0.85066465809674118859   \n",
       "2017-11-15 18:00:00  0.85066465809674118859  0.85235462054390909170   \n",
       "2017-11-15 20:00:00  0.85235462054390909170  0.73655554078628027170   \n",
       "2017-11-15 22:00:00  0.73655554078628027170  0.74153449024088802233   \n",
       "2017-11-16 12:00:00  0.74153449024088802233  0.62209529624281612037   \n",
       "2017-11-16 14:00:00  0.62209529624281612037  0.56352507736145363015   \n",
       "2017-11-16 16:00:00  0.56352507736145363015  0.63713066122663675195   \n",
       "2017-11-16 18:00:00  0.63713066122663675195  0.65222665089262377158   \n",
       "2017-11-16 20:00:00  0.65222665089262377158  0.25278916983891314141   \n",
       "2017-11-16 22:00:00  0.25278916983891314141  0.26873645720266514658   \n",
       "2017-11-17 12:00:00  0.26873645720266514658  0.17080197715296660532   \n",
       "2017-11-17 14:00:00  0.17080197715296660532 -0.06117509655565678967   \n",
       "2017-11-17 16:00:00 -0.06117509655565678967  0.01692947625373457388   \n",
       "2017-11-17 18:00:00  0.01692947625373457388  0.08289107318357590015   \n",
       "2017-11-17 20:00:00  0.08289107318357590015 -0.51452719076219877170   \n",
       "2017-11-17 22:00:00 -0.51452719076219877170 -0.49590981342955342548   \n",
       "2017-11-18 12:00:00 -0.49590981342955342548 -0.52159522681151071488   \n",
       "2017-11-18 14:00:00 -0.52159522681151071488 -0.88853692153588392788   \n",
       "2017-11-18 16:00:00 -0.88853692153588392788 -0.82572537508552712460   \n",
       "2017-11-18 18:00:00 -0.82572537508552712460 -0.68523364068911885028   \n",
       "2017-11-18 20:00:00 -0.68523364068911885028 -1.31830026768808261650   \n",
       "2017-11-18 22:00:00 -1.31830026768808261650 -1.31182839247877947031   \n",
       "2017-11-19 12:00:00 -1.31182839247877947031 -1.22957595968509281192   \n",
       "2017-11-19 14:00:00 -1.22957595968509281192 -1.66510892312715230723   \n",
       "2017-11-19 16:00:00 -1.66510892312715230723 -1.63402522791579007944   \n",
       "2017-11-19 18:00:00 -1.63402522791579007944 -1.41494896192611729902   \n",
       "2017-11-19 20:00:00 -1.41494896192611729902 -1.90960283614629178217   \n",
       "2017-11-19 22:00:00 -1.90960283614629178217 -1.93036992820924346859   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  0.77581370487537448799  0.76532532575010336906   \n",
       "2017-11-03 12:00:00  0.76532532575010336906  0.65055689667290861333   \n",
       "2017-11-03 14:00:00  0.65055689667290861333  0.75900249031931454891   \n",
       "2017-11-03 16:00:00  0.75900249031931454891  0.80658668930844723555   \n",
       "2017-11-03 18:00:00  0.80658668930844723555  0.51029756466224507339   \n",
       "2017-11-03 20:00:00  0.51029756466224507339  0.51543818338990998740   \n",
       "2017-11-03 22:00:00  0.51543818338990998740  0.57610759556977997331   \n",
       "2017-11-04 12:00:00  0.57610759556977997331  0.44407830463047193170   \n",
       "2017-11-04 14:00:00  0.44407830463047193170  0.53415155282490367483   \n",
       "2017-11-04 16:00:00  0.53415155282490367483  0.99120691392514004292   \n",
       "2017-11-04 18:00:00  0.99120691392514004292  0.78575139383041936458   \n",
       "2017-11-04 20:00:00  0.78575139383041936458  0.76883408392202723380   \n",
       "2017-11-04 22:00:00  0.76883408392202723380  0.79552235929752979082   \n",
       "2017-11-05 12:00:00  0.79552235929752979082  0.71031438474593744381   \n",
       "2017-11-05 14:00:00  0.71031438474593744381  0.75033795768062416798   \n",
       "2017-11-05 16:00:00  0.75033795768062416798  1.29779243521901799241   \n",
       "2017-11-05 18:00:00  1.29779243521901799241  1.24604269253540334006   \n",
       "2017-11-05 20:00:00  1.24604269253540334006  1.23522339440477324501   \n",
       "2017-11-05 22:00:00  1.23522339440477324501  1.09379447518572048281   \n",
       "2017-11-06 12:00:00  1.09379447518572048281  1.10047854455312421607   \n",
       "2017-11-06 14:00:00  1.10047854455312421607  1.08940158332075420766   \n",
       "2017-11-06 16:00:00  1.08940158332075420766  1.33051304520138224952   \n",
       "2017-11-06 18:00:00  1.33051304520138224952  1.43746318512219928287   \n",
       "2017-11-06 20:00:00  1.43746318512219928287  1.47237645253395066369   \n",
       "2017-11-06 22:00:00  1.47237645253395066369  1.10187042571692872528   \n",
       "2017-11-07 12:00:00  1.10187042571692872528  1.19661191258787025227   \n",
       "2017-11-07 14:00:00  1.19661191258787025227  1.16981754013331751274   \n",
       "2017-11-07 16:00:00  1.16981754013331751274  0.88633322937738301395   \n",
       "2017-11-07 18:00:00  0.88633322937738301395  1.10333051374924528965   \n",
       "2017-11-07 20:00:00  1.10333051374924528965  1.21126330916306468310   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  0.79945550233279349950  0.85066465809674118859   \n",
       "2017-11-15 14:00:00  0.85066465809674118859  0.85235462054390909170   \n",
       "2017-11-15 16:00:00  0.85235462054390909170  0.73655554078628027170   \n",
       "2017-11-15 18:00:00  0.73655554078628027170  0.74153449024088802233   \n",
       "2017-11-15 20:00:00  0.74153449024088802233  0.62209529624281612037   \n",
       "2017-11-15 22:00:00  0.62209529624281612037  0.56352507736145363015   \n",
       "2017-11-16 12:00:00  0.56352507736145363015  0.63713066122663675195   \n",
       "2017-11-16 14:00:00  0.63713066122663675195  0.65222665089262377158   \n",
       "2017-11-16 16:00:00  0.65222665089262377158  0.25278916983891314141   \n",
       "2017-11-16 18:00:00  0.25278916983891314141  0.26873645720266514658   \n",
       "2017-11-16 20:00:00  0.26873645720266514658  0.17080197715296660532   \n",
       "2017-11-16 22:00:00  0.17080197715296660532 -0.06117509655565678967   \n",
       "2017-11-17 12:00:00 -0.06117509655565678967  0.01692947625373457388   \n",
       "2017-11-17 14:00:00  0.01692947625373457388  0.08289107318357590015   \n",
       "2017-11-17 16:00:00  0.08289107318357590015 -0.51452719076219877170   \n",
       "2017-11-17 18:00:00 -0.51452719076219877170 -0.49590981342955342548   \n",
       "2017-11-17 20:00:00 -0.49590981342955342548 -0.52159522681151071488   \n",
       "2017-11-17 22:00:00 -0.52159522681151071488 -0.88853692153588392788   \n",
       "2017-11-18 12:00:00 -0.88853692153588392788 -0.82572537508552712460   \n",
       "2017-11-18 14:00:00 -0.82572537508552712460 -0.68523364068911885028   \n",
       "2017-11-18 16:00:00 -0.68523364068911885028 -1.31830026768808261650   \n",
       "2017-11-18 18:00:00 -1.31830026768808261650 -1.31182839247877947031   \n",
       "2017-11-18 20:00:00 -1.31182839247877947031 -1.22957595968509281192   \n",
       "2017-11-18 22:00:00 -1.22957595968509281192 -1.66510892312715230723   \n",
       "2017-11-19 12:00:00 -1.66510892312715230723 -1.63402522791579007944   \n",
       "2017-11-19 14:00:00 -1.63402522791579007944 -1.41494896192611729902   \n",
       "2017-11-19 16:00:00 -1.41494896192611729902 -1.90960283614629178217   \n",
       "2017-11-19 18:00:00 -1.90960283614629178217 -1.93036992820924346859   \n",
       "2017-11-19 20:00:00 -1.93036992820924346859 -1.72967863716666281348   \n",
       "2017-11-19 22:00:00 -1.72967863716666281348 -2.16029075477187593179   \n",
       "\n",
       "tensor                                                               \n",
       "feature                                                              \n",
       "time step                               t-1                       t  \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-02 22:00:00  0.65055689667290861333  0.75900249031931454891  \n",
       "2017-11-03 12:00:00  0.75900249031931454891  0.80658668930844723555  \n",
       "2017-11-03 14:00:00  0.80658668930844723555  0.51029756466224507339  \n",
       "2017-11-03 16:00:00  0.51029756466224507339  0.51543818338990998740  \n",
       "2017-11-03 18:00:00  0.51543818338990998740  0.57610759556977997331  \n",
       "2017-11-03 20:00:00  0.57610759556977997331  0.44407830463047193170  \n",
       "2017-11-03 22:00:00  0.44407830463047193170  0.53415155282490367483  \n",
       "2017-11-04 12:00:00  0.53415155282490367483  0.99120691392514004292  \n",
       "2017-11-04 14:00:00  0.99120691392514004292  0.78575139383041936458  \n",
       "2017-11-04 16:00:00  0.78575139383041936458  0.76883408392202723380  \n",
       "2017-11-04 18:00:00  0.76883408392202723380  0.79552235929752979082  \n",
       "2017-11-04 20:00:00  0.79552235929752979082  0.71031438474593744381  \n",
       "2017-11-04 22:00:00  0.71031438474593744381  0.75033795768062416798  \n",
       "2017-11-05 12:00:00  0.75033795768062416798  1.29779243521901799241  \n",
       "2017-11-05 14:00:00  1.29779243521901799241  1.24604269253540334006  \n",
       "2017-11-05 16:00:00  1.24604269253540334006  1.23522339440477324501  \n",
       "2017-11-05 18:00:00  1.23522339440477324501  1.09379447518572048281  \n",
       "2017-11-05 20:00:00  1.09379447518572048281  1.10047854455312421607  \n",
       "2017-11-05 22:00:00  1.10047854455312421607  1.08940158332075420766  \n",
       "2017-11-06 12:00:00  1.08940158332075420766  1.33051304520138224952  \n",
       "2017-11-06 14:00:00  1.33051304520138224952  1.43746318512219928287  \n",
       "2017-11-06 16:00:00  1.43746318512219928287  1.47237645253395066369  \n",
       "2017-11-06 18:00:00  1.47237645253395066369  1.10187042571692872528  \n",
       "2017-11-06 20:00:00  1.10187042571692872528  1.19661191258787025227  \n",
       "2017-11-06 22:00:00  1.19661191258787025227  1.16981754013331751274  \n",
       "2017-11-07 12:00:00  1.16981754013331751274  0.88633322937738301395  \n",
       "2017-11-07 14:00:00  0.88633322937738301395  1.10333051374924528965  \n",
       "2017-11-07 16:00:00  1.10333051374924528965  1.21126330916306468310  \n",
       "2017-11-07 18:00:00  1.21126330916306468310  0.68133493359471564155  \n",
       "2017-11-07 20:00:00  0.68133493359471564155  0.81522090874259967030  \n",
       "...                                     ...                     ...  \n",
       "2017-11-15 12:00:00  0.85235462054390909170  0.73655554078628027170  \n",
       "2017-11-15 14:00:00  0.73655554078628027170  0.74153449024088802233  \n",
       "2017-11-15 16:00:00  0.74153449024088802233  0.62209529624281612037  \n",
       "2017-11-15 18:00:00  0.62209529624281612037  0.56352507736145363015  \n",
       "2017-11-15 20:00:00  0.56352507736145363015  0.63713066122663675195  \n",
       "2017-11-15 22:00:00  0.63713066122663675195  0.65222665089262377158  \n",
       "2017-11-16 12:00:00  0.65222665089262377158  0.25278916983891314141  \n",
       "2017-11-16 14:00:00  0.25278916983891314141  0.26873645720266514658  \n",
       "2017-11-16 16:00:00  0.26873645720266514658  0.17080197715296660532  \n",
       "2017-11-16 18:00:00  0.17080197715296660532 -0.06117509655565678967  \n",
       "2017-11-16 20:00:00 -0.06117509655565678967  0.01692947625373457388  \n",
       "2017-11-16 22:00:00  0.01692947625373457388  0.08289107318357590015  \n",
       "2017-11-17 12:00:00  0.08289107318357590015 -0.51452719076219877170  \n",
       "2017-11-17 14:00:00 -0.51452719076219877170 -0.49590981342955342548  \n",
       "2017-11-17 16:00:00 -0.49590981342955342548 -0.52159522681151071488  \n",
       "2017-11-17 18:00:00 -0.52159522681151071488 -0.88853692153588392788  \n",
       "2017-11-17 20:00:00 -0.88853692153588392788 -0.82572537508552712460  \n",
       "2017-11-17 22:00:00 -0.82572537508552712460 -0.68523364068911885028  \n",
       "2017-11-18 12:00:00 -0.68523364068911885028 -1.31830026768808261650  \n",
       "2017-11-18 14:00:00 -1.31830026768808261650 -1.31182839247877947031  \n",
       "2017-11-18 16:00:00 -1.31182839247877947031 -1.22957595968509281192  \n",
       "2017-11-18 18:00:00 -1.22957595968509281192 -1.66510892312715230723  \n",
       "2017-11-18 20:00:00 -1.66510892312715230723 -1.63402522791579007944  \n",
       "2017-11-18 22:00:00 -1.63402522791579007944 -1.41494896192611729902  \n",
       "2017-11-19 12:00:00 -1.41494896192611729902 -1.90960283614629178217  \n",
       "2017-11-19 14:00:00 -1.90960283614629178217 -1.93036992820924346859  \n",
       "2017-11-19 16:00:00 -1.93036992820924346859 -1.72967863716666281348  \n",
       "2017-11-19 18:00:00 -1.72967863716666281348 -2.16029075477187593179  \n",
       "2017-11-19 20:00:00 -2.16029075477187593179 -2.17038000974980249680  \n",
       "2017-11-19 22:00:00 -2.17038000974980249680 -1.89410772233276847309  \n",
       "\n",
       "[103 rows x 24 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 6)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs['target'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct validation set (keeping T hours from the training set in order to construct initial features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"15\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"4\" halign=\"left\">e</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">OMEGA</th>\n",
       "      <th colspan=\"6\" halign=\"left\">omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>...</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-21 22:00:00</th>\n",
       "      <td>-1.43936121899152835724</td>\n",
       "      <td>-1.56127852048089832415</td>\n",
       "      <td>-1.20631307814756572050</td>\n",
       "      <td>-1.34324048045132515838</td>\n",
       "      <td>-1.45532366382071676192</td>\n",
       "      <td>-1.27910525625223980839</td>\n",
       "      <td>0.25846325095622635359</td>\n",
       "      <td>0.19109481904778460870</td>\n",
       "      <td>0.30945855154248819163</td>\n",
       "      <td>0.44261091903228955147</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.80296687995149529371</td>\n",
       "      <td>-1.80346860885906878913</td>\n",
       "      <td>-1.80398083537509745788</td>\n",
       "      <td>-1.80449141493681186610</td>\n",
       "      <td>-1.93405803763711836574</td>\n",
       "      <td>-2.02853428440031935409</td>\n",
       "      <td>-1.67146712425083010523</td>\n",
       "      <td>-1.92789939945047184011</td>\n",
       "      <td>-2.01630036718062921608</td>\n",
       "      <td>-1.75592484569535667305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 12:00:00</th>\n",
       "      <td>-1.56127852048089832415</td>\n",
       "      <td>-1.20631307814756572050</td>\n",
       "      <td>-1.34324048045132515838</td>\n",
       "      <td>-1.45532366382071676192</td>\n",
       "      <td>-1.27910525625223980839</td>\n",
       "      <td>-0.84551486587092894442</td>\n",
       "      <td>0.19109481904778460870</td>\n",
       "      <td>0.30945855154248819163</td>\n",
       "      <td>0.44261091903228955147</td>\n",
       "      <td>0.37249238248652727368</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.80346860885906878913</td>\n",
       "      <td>-1.80398083537509745788</td>\n",
       "      <td>-1.80449141493681186610</td>\n",
       "      <td>-1.80805118497488681584</td>\n",
       "      <td>-2.02853428440031935409</td>\n",
       "      <td>-1.67146712425083010523</td>\n",
       "      <td>-1.92789939945047184011</td>\n",
       "      <td>-2.01630036718062921608</td>\n",
       "      <td>-1.75592484569535667305</td>\n",
       "      <td>-1.43936121899152835724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 14:00:00</th>\n",
       "      <td>-1.20631307814756572050</td>\n",
       "      <td>-1.34324048045132515838</td>\n",
       "      <td>-1.45532366382071676192</td>\n",
       "      <td>-1.27910525625223980839</td>\n",
       "      <td>-0.84551486587092894442</td>\n",
       "      <td>-0.97781192143543460560</td>\n",
       "      <td>0.30945855154248819163</td>\n",
       "      <td>0.44261091903228955147</td>\n",
       "      <td>0.37249238248652727368</td>\n",
       "      <td>0.41203195295805883358</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.80398083537509745788</td>\n",
       "      <td>-1.80449141493681186610</td>\n",
       "      <td>-1.80805118497488681584</td>\n",
       "      <td>-1.80855488218232896003</td>\n",
       "      <td>-1.67146712425083010523</td>\n",
       "      <td>-1.92789939945047184011</td>\n",
       "      <td>-2.01630036718062921608</td>\n",
       "      <td>-1.75592484569535667305</td>\n",
       "      <td>-1.43936121899152835724</td>\n",
       "      <td>-1.56127852048089832415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 16:00:00</th>\n",
       "      <td>-1.34324048045132515838</td>\n",
       "      <td>-1.45532366382071676192</td>\n",
       "      <td>-1.27910525625223980839</td>\n",
       "      <td>-0.84551486587092894442</td>\n",
       "      <td>-0.97781192143543460560</td>\n",
       "      <td>-0.68450865002476635190</td>\n",
       "      <td>0.44261091903228955147</td>\n",
       "      <td>0.37249238248652727368</td>\n",
       "      <td>0.41203195295805883358</td>\n",
       "      <td>0.65739219163440154592</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.80449141493681186610</td>\n",
       "      <td>-1.80805118497488681584</td>\n",
       "      <td>-1.80855488218232896003</td>\n",
       "      <td>-1.80906141802911979433</td>\n",
       "      <td>-1.92789939945047184011</td>\n",
       "      <td>-2.01630036718062921608</td>\n",
       "      <td>-1.75592484569535667305</td>\n",
       "      <td>-1.43936121899152835724</td>\n",
       "      <td>-1.56127852048089832415</td>\n",
       "      <td>-1.20631307814756572050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 18:00:00</th>\n",
       "      <td>-1.45532366382071676192</td>\n",
       "      <td>-1.27910525625223980839</td>\n",
       "      <td>-0.84551486587092894442</td>\n",
       "      <td>-0.97781192143543460560</td>\n",
       "      <td>-0.68450865002476635190</td>\n",
       "      <td>-0.71440507604446212842</td>\n",
       "      <td>0.37249238248652727368</td>\n",
       "      <td>0.41203195295805883358</td>\n",
       "      <td>0.65739219163440154592</td>\n",
       "      <td>0.62400848403586117730</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.80805118497488681584</td>\n",
       "      <td>-1.80855488218232896003</td>\n",
       "      <td>-1.80906141802911979433</td>\n",
       "      <td>-1.80956613286285272757</td>\n",
       "      <td>-2.01630036718062921608</td>\n",
       "      <td>-1.75592484569535667305</td>\n",
       "      <td>-1.43936121899152835724</td>\n",
       "      <td>-1.56127852048089832415</td>\n",
       "      <td>-1.20631307814756572050</td>\n",
       "      <td>-1.34324048045132515838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                               target                          \\\n",
       "feature                                   y                           \n",
       "time step                               t+1                     t+2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -1.43936121899152835724 -1.56127852048089832415   \n",
       "2017-11-22 12:00:00 -1.56127852048089832415 -1.20631307814756572050   \n",
       "2017-11-22 14:00:00 -1.20631307814756572050 -1.34324048045132515838   \n",
       "2017-11-22 16:00:00 -1.34324048045132515838 -1.45532366382071676192   \n",
       "2017-11-22 18:00:00 -1.45532366382071676192 -1.27910525625223980839   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+3                     t+4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -1.20631307814756572050 -1.34324048045132515838   \n",
       "2017-11-22 12:00:00 -1.34324048045132515838 -1.45532366382071676192   \n",
       "2017-11-22 14:00:00 -1.45532366382071676192 -1.27910525625223980839   \n",
       "2017-11-22 16:00:00 -1.27910525625223980839 -0.84551486587092894442   \n",
       "2017-11-22 18:00:00 -0.84551486587092894442 -0.97781192143543460560   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+5                     t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -1.45532366382071676192 -1.27910525625223980839   \n",
       "2017-11-22 12:00:00 -1.27910525625223980839 -0.84551486587092894442   \n",
       "2017-11-22 14:00:00 -0.84551486587092894442 -0.97781192143543460560   \n",
       "2017-11-22 16:00:00 -0.97781192143543460560 -0.68450865002476635190   \n",
       "2017-11-22 18:00:00 -0.68450865002476635190 -0.71440507604446212842   \n",
       "\n",
       "tensor                                   X                         \\\n",
       "feature                                  e                          \n",
       "time step                              t-5                    t-4   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-21 22:00:00 0.25846325095622635359 0.19109481904778460870   \n",
       "2017-11-22 12:00:00 0.19109481904778460870 0.30945855154248819163   \n",
       "2017-11-22 14:00:00 0.30945855154248819163 0.44261091903228955147   \n",
       "2017-11-22 16:00:00 0.44261091903228955147 0.37249238248652727368   \n",
       "2017-11-22 18:00:00 0.37249238248652727368 0.41203195295805883358   \n",
       "\n",
       "tensor                                                             ...  \\\n",
       "feature                                                            ...   \n",
       "time step                              t-3                    t-2  ...   \n",
       "Epoch_Time_of_Clock                                                ...   \n",
       "2017-11-21 22:00:00 0.30945855154248819163 0.44261091903228955147  ...   \n",
       "2017-11-22 12:00:00 0.44261091903228955147 0.37249238248652727368  ...   \n",
       "2017-11-22 14:00:00 0.37249238248652727368 0.41203195295805883358  ...   \n",
       "2017-11-22 16:00:00 0.41203195295805883358 0.65739219163440154592  ...   \n",
       "2017-11-22 18:00:00 0.65739219163440154592 0.62400848403586117730  ...   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                               OMEGA                           \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -1.80296687995149529371 -1.80346860885906878913   \n",
       "2017-11-22 12:00:00 -1.80346860885906878913 -1.80398083537509745788   \n",
       "2017-11-22 14:00:00 -1.80398083537509745788 -1.80449141493681186610   \n",
       "2017-11-22 16:00:00 -1.80449141493681186610 -1.80805118497488681584   \n",
       "2017-11-22 18:00:00 -1.80805118497488681584 -1.80855488218232896003   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-1                       t   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -1.80398083537509745788 -1.80449141493681186610   \n",
       "2017-11-22 12:00:00 -1.80449141493681186610 -1.80805118497488681584   \n",
       "2017-11-22 14:00:00 -1.80805118497488681584 -1.80855488218232896003   \n",
       "2017-11-22 16:00:00 -1.80855488218232896003 -1.80906141802911979433   \n",
       "2017-11-22 18:00:00 -1.80906141802911979433 -1.80956613286285272757   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                               omega                           \n",
       "time step                               t-5                     t-4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -1.93405803763711836574 -2.02853428440031935409   \n",
       "2017-11-22 12:00:00 -2.02853428440031935409 -1.67146712425083010523   \n",
       "2017-11-22 14:00:00 -1.67146712425083010523 -1.92789939945047184011   \n",
       "2017-11-22 16:00:00 -1.92789939945047184011 -2.01630036718062921608   \n",
       "2017-11-22 18:00:00 -2.01630036718062921608 -1.75592484569535667305   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -1.67146712425083010523 -1.92789939945047184011   \n",
       "2017-11-22 12:00:00 -1.92789939945047184011 -2.01630036718062921608   \n",
       "2017-11-22 14:00:00 -2.01630036718062921608 -1.75592484569535667305   \n",
       "2017-11-22 16:00:00 -1.75592484569535667305 -1.43936121899152835724   \n",
       "2017-11-22 18:00:00 -1.43936121899152835724 -1.56127852048089832415   \n",
       "\n",
       "tensor                                                               \n",
       "feature                                                              \n",
       "time step                               t-1                       t  \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-21 22:00:00 -2.01630036718062921608 -1.75592484569535667305  \n",
       "2017-11-22 12:00:00 -1.75592484569535667305 -1.43936121899152835724  \n",
       "2017-11-22 14:00:00 -1.43936121899152835724 -1.56127852048089832415  \n",
       "2017-11-22 16:00:00 -1.56127852048089832415 -1.20631307814756572050  \n",
       "2017-11-22 18:00:00 -1.20631307814756572050 -1.34324048045132515838  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back_dt = dt.datetime.strptime(valid_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1)\n",
    "valid = df.copy()[(df.index >=look_back_dt) & (df.index < test_start_dt)][['e', 'OMEGA', 'omega']]\n",
    "valid[['e', 'OMEGA', 'omega']] = X_scaler.transform(valid)\n",
    "valid_inputs = TimeSeriesTensor(valid, var_name, HORIZON, tensor_structure,freq = None)\n",
    "valid_inputs.dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a RNN forecasting model with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image('./images/simple_encoder_decoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Flatten\n",
    "from keras.callbacks import EarlyStopping ,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(LATENT_DIM, input_shape=(T,3 ) , return_sequences=True))\n",
    "model.add(LSTM(LATENT_DIM , return_sequences=True) )\n",
    "model.add(LSTM(LATENT_DIM))\n",
    "model.add(RepeatVector(HORIZON))\n",
    "model.add(LSTM(LATENT_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='RMSprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 6, 64)             17408     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 6, 64)             33024     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 6, 64)             33024     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 6, 1)              65        \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 116,545\n",
      "Trainable params: 116,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val = ModelCheckpoint(str(sat_var) +'_' +  var_name + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103 samples, validate on 13 samples\n",
      "Epoch 1/2000\n",
      "103/103 [==============================] - 3s 31ms/step - loss: 0.9260 - val_loss: 0.4185\n",
      "Epoch 2/2000\n",
      "103/103 [==============================] - 0s 851us/step - loss: 0.7130 - val_loss: 0.4779\n",
      "Epoch 3/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 2.9151\n",
      "Epoch 4/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.3549\n",
      "Epoch 5/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 1.6153\n",
      "Epoch 6/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2916 - val_loss: 0.5255\n",
      "Epoch 7/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3077 - val_loss: 1.3543\n",
      "Epoch 8/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2708 - val_loss: 0.5092\n",
      "Epoch 9/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2836 - val_loss: 1.2195\n",
      "Epoch 10/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2500 - val_loss: 0.4299\n",
      "Epoch 11/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2651 - val_loss: 1.1399\n",
      "Epoch 12/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2282 - val_loss: 0.3410\n",
      "Epoch 13/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2491 - val_loss: 1.0869\n",
      "Epoch 14/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2083 - val_loss: 0.2606\n",
      "Epoch 15/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2355 - val_loss: 1.0529\n",
      "Epoch 16/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1933 - val_loss: 0.1937\n",
      "Epoch 17/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2235 - val_loss: 0.9625\n",
      "Epoch 18/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1818 - val_loss: 0.1595\n",
      "Epoch 19/2000\n",
      "103/103 [==============================] - 0s 932us/step - loss: 0.2099 - val_loss: 0.7034\n",
      "Epoch 20/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1714 - val_loss: 0.1504\n",
      "Epoch 21/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1931 - val_loss: 0.4631\n",
      "Epoch 22/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1614 - val_loss: 0.1541\n",
      "Epoch 23/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1776 - val_loss: 0.3134\n",
      "Epoch 24/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1522 - val_loss: 0.2465\n",
      "Epoch 25/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1636 - val_loss: 0.4059\n",
      "Epoch 26/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1579 - val_loss: 0.7517\n",
      "Epoch 27/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1649 - val_loss: 0.2339\n",
      "Epoch 28/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1633 - val_loss: 0.1357\n",
      "Epoch 29/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1393 - val_loss: 0.1732\n",
      "Epoch 30/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1311 - val_loss: 0.2176\n",
      "Epoch 31/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1202 - val_loss: 0.3184\n",
      "Epoch 32/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1292 - val_loss: 0.5288\n",
      "Epoch 33/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1398 - val_loss: 0.4040\n",
      "Epoch 34/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1536 - val_loss: 0.5073\n",
      "Epoch 35/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1190 - val_loss: 0.4746\n",
      "Epoch 36/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1061 - val_loss: 0.5345\n",
      "Epoch 37/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0960 - val_loss: 0.5990\n",
      "Epoch 38/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0967 - val_loss: 0.7088\n",
      "Epoch 39/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1131 - val_loss: 0.7045\n",
      "Epoch 40/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1245 - val_loss: 0.9315\n",
      "Epoch 41/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0979 - val_loss: 0.9209\n",
      "Epoch 42/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0862 - val_loss: 0.8575\n",
      "Epoch 43/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0685 - val_loss: 1.0560\n",
      "Epoch 44/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0836 - val_loss: 0.9186\n",
      "Epoch 45/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 1.2209\n",
      "Epoch 46/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1038 - val_loss: 1.0792\n",
      "Epoch 47/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0682 - val_loss: 1.3535\n",
      "Epoch 48/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0681 - val_loss: 1.1085\n",
      "Epoch 49/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0740 - val_loss: 1.4644\n",
      "Epoch 50/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0866 - val_loss: 1.1287\n",
      "Epoch 51/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0736 - val_loss: 1.3157\n",
      "Epoch 52/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0649 - val_loss: 1.2321\n",
      "Epoch 53/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0599 - val_loss: 1.4711\n",
      "Epoch 54/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0636 - val_loss: 1.1686\n",
      "Epoch 55/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0768 - val_loss: 1.4561\n",
      "Epoch 56/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0786 - val_loss: 1.2447\n",
      "Epoch 57/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0566 - val_loss: 1.3524\n",
      "Epoch 58/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0523 - val_loss: 1.2711\n",
      "Epoch 59/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0545 - val_loss: 1.4475\n",
      "Epoch 60/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0609 - val_loss: 1.1797\n",
      "Epoch 61/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0706 - val_loss: 1.3383\n",
      "Epoch 62/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0668 - val_loss: 1.2453\n",
      "Epoch 63/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0503 - val_loss: 1.2970\n",
      "Epoch 64/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0475 - val_loss: 1.2458\n",
      "Epoch 65/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0516 - val_loss: 1.3725\n",
      "Epoch 66/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 1.1794\n",
      "Epoch 67/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0635 - val_loss: 1.2437\n",
      "Epoch 68/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 1.2413\n",
      "Epoch 69/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0465 - val_loss: 1.2374\n",
      "Epoch 70/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 1.2573\n",
      "Epoch 71/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 1.3237\n",
      "Epoch 72/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0537 - val_loss: 1.1792\n",
      "Epoch 73/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0622 - val_loss: 1.1652\n",
      "Epoch 74/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0533 - val_loss: 1.2656\n",
      "Epoch 75/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0423 - val_loss: 1.1658\n",
      "Epoch 76/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 1.4159\n",
      "Epoch 77/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 1.3602\n",
      "Epoch 78/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0380 - val_loss: 1.3389\n",
      "Epoch 79/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.9972\n",
      "Epoch 80/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0546 - val_loss: 1.2016\n",
      "Epoch 81/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0544 - val_loss: 1.2670\n",
      "Epoch 82/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0539 - val_loss: 1.0803\n",
      "Epoch 83/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0342 - val_loss: 1.1769\n",
      "Epoch 84/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0403 - val_loss: 1.0210\n",
      "Epoch 85/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0418 - val_loss: 1.3489\n",
      "Epoch 86/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0500 - val_loss: 0.8830\n",
      "Epoch 87/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0383 - val_loss: 1.4790\n",
      "Epoch 88/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0625 - val_loss: 0.9433\n",
      "Epoch 89/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0532 - val_loss: 1.2714\n",
      "Epoch 90/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0395 - val_loss: 1.1016\n",
      "Epoch 91/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0306 - val_loss: 1.1819\n",
      "Epoch 92/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0299 - val_loss: 1.4161\n",
      "Epoch 93/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0328 - val_loss: 1.0759\n",
      "Epoch 94/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0443 - val_loss: 1.4747\n",
      "Epoch 95/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.7416\n",
      "Epoch 96/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0487 - val_loss: 1.4779\n",
      "Epoch 97/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0598 - val_loss: 0.9517\n",
      "Epoch 98/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0430 - val_loss: 1.0749\n",
      "Epoch 99/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0398 - val_loss: 1.0428\n",
      "Epoch 100/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0334 - val_loss: 1.2318\n",
      "Epoch 101/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.9496\n",
      "Epoch 102/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0359 - val_loss: 1.3010\n",
      "Epoch 103/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0343 - val_loss: 0.7850\n",
      "Epoch 104/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0464 - val_loss: 1.4773\n",
      "Epoch 105/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0638 - val_loss: 0.9574\n",
      "Epoch 106/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0390 - val_loss: 1.1518\n",
      "Epoch 107/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0442 - val_loss: 0.9183\n",
      "Epoch 108/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0368 - val_loss: 1.3483\n",
      "Epoch 109/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0418 - val_loss: 0.9007\n",
      "Epoch 110/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0381 - val_loss: 1.2839\n",
      "Epoch 111/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 0.8853\n",
      "Epoch 112/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0402 - val_loss: 1.1856\n",
      "Epoch 113/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0388 - val_loss: 0.9830\n",
      "Epoch 114/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 1.4563\n",
      "Epoch 115/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.9737\n",
      "Epoch 116/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 1.3700\n",
      "Epoch 117/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0536 - val_loss: 0.8242\n",
      "Epoch 118/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0411 - val_loss: 1.4456\n",
      "Epoch 119/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0427 - val_loss: 0.9664\n",
      "Epoch 120/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0339 - val_loss: 1.2122\n",
      "Epoch 121/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0299 - val_loss: 0.8831\n",
      "Epoch 122/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 1.2520\n",
      "Epoch 123/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.9915\n",
      "Epoch 124/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 1.3083\n",
      "Epoch 125/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0371 - val_loss: 0.9642\n",
      "Epoch 126/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0391 - val_loss: 1.4379\n",
      "Epoch 127/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0417 - val_loss: 0.8349\n",
      "Epoch 128/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0445 - val_loss: 1.3705\n",
      "Epoch 129/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0465 - val_loss: 0.9815\n",
      "Epoch 130/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0301 - val_loss: 1.2205\n",
      "Epoch 131/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.7865\n",
      "Epoch 132/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0381 - val_loss: 1.5106\n",
      "Epoch 133/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0622 - val_loss: 1.0375\n",
      "Epoch 134/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0358 - val_loss: 1.0840\n",
      "Epoch 135/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 1.0494\n",
      "Epoch 136/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 1.0818\n",
      "Epoch 137/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0268 - val_loss: 1.3197\n",
      "Epoch 138/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0328 - val_loss: 1.0843\n",
      "Epoch 139/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0398 - val_loss: 1.3050\n",
      "Epoch 140/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0499 - val_loss: 0.7876\n",
      "Epoch 141/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 1.5043\n",
      "Epoch 142/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0542 - val_loss: 0.9691\n",
      "Epoch 143/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0332 - val_loss: 1.1438\n",
      "Epoch 144/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0300 - val_loss: 0.9196\n",
      "Epoch 145/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 1.2200\n",
      "Epoch 146/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0329 - val_loss: 1.0504\n",
      "Epoch 147/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0403 - val_loss: 1.1900\n",
      "Epoch 148/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0364 - val_loss: 0.9541\n",
      "Epoch 149/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 1.4382\n",
      "Epoch 150/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.9380\n",
      "Epoch 151/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0408 - val_loss: 1.1334\n",
      "Epoch 152/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0328 - val_loss: 0.9443\n",
      "Epoch 153/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 1.1131\n",
      "Epoch 154/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0306 - val_loss: 1.2588\n",
      "Epoch 155/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 1.0595\n",
      "Epoch 156/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 1.3233\n",
      "Epoch 157/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.7453\n",
      "Epoch 158/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0439 - val_loss: 1.3966\n",
      "Epoch 159/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0471 - val_loss: 0.9451\n",
      "Epoch 160/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0304 - val_loss: 1.1535\n",
      "Epoch 161/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0282 - val_loss: 0.8851\n",
      "Epoch 162/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0297 - val_loss: 1.2357\n",
      "Epoch 163/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0352 - val_loss: 0.9513\n",
      "Epoch 164/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 1.2668\n",
      "Epoch 165/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.8716\n",
      "Epoch 166/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0352 - val_loss: 1.4030\n",
      "Epoch 167/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0490 - val_loss: 0.9384\n",
      "Epoch 168/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0358 - val_loss: 1.1369\n",
      "Epoch 169/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.8751\n",
      "Epoch 170/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 1.2093\n",
      "Epoch 171/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0340 - val_loss: 1.0121\n",
      "Epoch 172/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 1.1427\n",
      "Epoch 173/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0370 - val_loss: 0.9419\n",
      "Epoch 174/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0338 - val_loss: 1.3264\n",
      "Epoch 175/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0384 - val_loss: 0.9191\n",
      "Epoch 176/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 1.2049\n",
      "Epoch 177/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0281 - val_loss: 0.8300\n",
      "Epoch 178/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0357 - val_loss: 1.2656\n",
      "Epoch 179/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0434 - val_loss: 0.9044\n",
      "Epoch 180/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 1.2109\n",
      "Epoch 181/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.8442\n",
      "Epoch 182/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0325 - val_loss: 1.3334\n",
      "Epoch 183/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.9359\n",
      "Epoch 184/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0352 - val_loss: 1.0974\n",
      "Epoch 185/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0310 - val_loss: 0.8790\n",
      "Epoch 186/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 1.1165\n",
      "Epoch 187/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 1.0982\n",
      "Epoch 188/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 1.1315\n",
      "Epoch 189/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0316 - val_loss: 0.9835\n",
      "Epoch 190/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0308 - val_loss: 1.3270\n",
      "Epoch 191/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0443 - val_loss: 0.8746\n",
      "Epoch 192/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 1.1876\n",
      "Epoch 193/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 0.8351\n",
      "Epoch 194/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 1.2446\n",
      "Epoch 195/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.8706\n",
      "Epoch 196/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 1.2237\n",
      "Epoch 197/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0304 - val_loss: 0.7699\n",
      "Epoch 198/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 1.2894\n",
      "Epoch 199/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0466 - val_loss: 0.8852\n",
      "Epoch 200/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0313 - val_loss: 1.0901\n",
      "Epoch 201/2000\n",
      "103/103 [==============================] - 0s 881us/step - loss: 0.0271 - val_loss: 0.8356\n",
      "Epoch 202/2000\n",
      "103/103 [==============================] - 0s 986us/step - loss: 0.0296 - val_loss: 1.1781\n",
      "Epoch 203/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.8969\n",
      "Epoch 204/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 1.1677\n",
      "Epoch 205/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.8482\n",
      "Epoch 206/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 1.3043\n",
      "Epoch 207/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.8769\n",
      "Epoch 208/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 1.0978\n",
      "Epoch 209/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0322 - val_loss: 0.8130\n",
      "Epoch 210/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 1.2142\n",
      "Epoch 211/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.9033\n",
      "Epoch 212/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0377 - val_loss: 1.0908\n",
      "Epoch 213/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.8551\n",
      "Epoch 214/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 1.2613\n",
      "Epoch 215/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.9006\n",
      "Epoch 216/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 1.1158\n",
      "Epoch 217/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.7977\n",
      "Epoch 218/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 1.2097\n",
      "Epoch 219/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.8930\n",
      "Epoch 220/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 1.0989\n",
      "Epoch 221/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.8255\n",
      "Epoch 222/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 1.2545\n",
      "Epoch 223/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.8836\n",
      "Epoch 224/2000\n",
      "103/103 [==============================] - 0s 982us/step - loss: 0.0320 - val_loss: 1.1075\n",
      "Epoch 225/2000\n",
      "103/103 [==============================] - 0s 918us/step - loss: 0.0284 - val_loss: 0.7989\n",
      "Epoch 226/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0324 - val_loss: 1.1663\n",
      "Epoch 227/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0378 - val_loss: 0.8621\n",
      "Epoch 228/2000\n",
      "103/103 [==============================] - 0s 978us/step - loss: 0.0364 - val_loss: 1.1690\n",
      "Epoch 229/2000\n",
      "103/103 [==============================] - 0s 951us/step - loss: 0.0313 - val_loss: 0.7962\n",
      "Epoch 230/2000\n",
      "103/103 [==============================] - 0s 921us/step - loss: 0.0311 - val_loss: 1.2521\n",
      "Epoch 231/2000\n",
      "103/103 [==============================] - 0s 911us/step - loss: 0.0382 - val_loss: 0.8784\n",
      "Epoch 232/2000\n",
      "103/103 [==============================] - 0s 884us/step - loss: 0.0321 - val_loss: 1.0631\n",
      "Epoch 233/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.8266\n",
      "Epoch 234/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 1.1114\n",
      "Epoch 235/2000\n",
      "103/103 [==============================] - 0s 873us/step - loss: 0.0325 - val_loss: 0.9253\n",
      "Epoch 236/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 1.1825\n",
      "Epoch 237/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.8102\n",
      "Epoch 238/2000\n",
      "103/103 [==============================] - 0s 750us/step - loss: 0.0339 - val_loss: 1.2585\n",
      "Epoch 239/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.8823\n",
      "Epoch 240/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 1.0838\n",
      "Epoch 241/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.8559\n",
      "Epoch 242/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0313 - val_loss: 1.0604\n",
      "Epoch 243/2000\n",
      "103/103 [==============================] - 0s 595us/step - loss: 0.0323 - val_loss: 0.8921\n",
      "Epoch 244/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 1.3687\n",
      "Epoch 245/2000\n",
      "103/103 [==============================] - 0s 961us/step - loss: 0.0352 - val_loss: 0.8463\n",
      "Epoch 246/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 1.1146\n",
      "Epoch 247/2000\n",
      "103/103 [==============================] - 0s 997us/step - loss: 0.0380 - val_loss: 0.8128\n",
      "Epoch 248/2000\n",
      "103/103 [==============================] - 0s 818us/step - loss: 0.0274 - val_loss: 1.2089\n",
      "Epoch 249/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0294 - val_loss: 0.9221\n",
      "Epoch 250/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0297 - val_loss: 1.1023\n",
      "Epoch 251/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.7588\n",
      "Epoch 252/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0318 - val_loss: 1.2660\n",
      "Epoch 253/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.9029\n",
      "Epoch 254/2000\n",
      "103/103 [==============================] - 0s 924us/step - loss: 0.0328 - val_loss: 1.0230\n",
      "Epoch 255/2000\n",
      "103/103 [==============================] - 0s 908us/step - loss: 0.0283 - val_loss: 0.9136\n",
      "Epoch 256/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 1.1193\n",
      "Epoch 257/2000\n",
      "103/103 [==============================] - 0s 784us/step - loss: 0.0262 - val_loss: 0.9856\n",
      "Epoch 258/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0319 - val_loss: 1.2134\n",
      "Epoch 259/2000\n",
      "103/103 [==============================] - 0s 721us/step - loss: 0.0297 - val_loss: 0.7142\n",
      "Epoch 260/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0412 - val_loss: 1.2903\n",
      "Epoch 261/2000\n",
      "103/103 [==============================] - 0s 708us/step - loss: 0.0423 - val_loss: 0.8855\n",
      "Epoch 262/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0274 - val_loss: 1.0311\n",
      "Epoch 263/2000\n",
      "103/103 [==============================] - 0s 532us/step - loss: 0.0244 - val_loss: 0.9200\n",
      "Epoch 264/2000\n",
      "103/103 [==============================] - 0s 526us/step - loss: 0.0277 - val_loss: 0.9913\n",
      "Epoch 265/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0292 - val_loss: 1.1049\n",
      "Epoch 266/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0273 - val_loss: 1.1513\n",
      "Epoch 267/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0313 - val_loss: 1.0111\n",
      "Epoch 268/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0352 - val_loss: 0.9945\n",
      "Epoch 269/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0308 - val_loss: 1.0277\n",
      "Epoch 270/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0239 - val_loss: 1.1132\n",
      "Epoch 271/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0241 - val_loss: 1.1171\n",
      "Epoch 272/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0235 - val_loss: 0.7995\n",
      "Epoch 273/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0332 - val_loss: 1.3080\n",
      "Epoch 274/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0511 - val_loss: 0.7428\n",
      "Epoch 275/2000\n",
      "103/103 [==============================] - 0s 675us/step - loss: 0.0304 - val_loss: 1.1044\n",
      "Epoch 276/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0305 - val_loss: 0.8648\n",
      "Epoch 277/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0342 - val_loss: 1.0122\n",
      "Epoch 278/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0295 - val_loss: 0.8705\n",
      "Epoch 279/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0251 - val_loss: 1.2226\n",
      "Epoch 280/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0280 - val_loss: 0.8319\n",
      "Epoch 281/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0334 - val_loss: 1.1540\n",
      "Epoch 282/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0355 - val_loss: 0.6887\n",
      "Epoch 283/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0363 - val_loss: 1.1661\n",
      "Epoch 284/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0378 - val_loss: 0.8705\n",
      "Epoch 285/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0273 - val_loss: 1.0382\n",
      "Epoch 286/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0247 - val_loss: 0.8559\n",
      "Epoch 287/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0254 - val_loss: 1.1259\n",
      "Epoch 288/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0302 - val_loss: 0.8415\n",
      "Epoch 289/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0346 - val_loss: 1.2610\n",
      "Epoch 290/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0305 - val_loss: 0.7800\n",
      "Epoch 291/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0396 - val_loss: 1.0991\n",
      "Epoch 292/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0381 - val_loss: 0.8000\n",
      "Epoch 293/2000\n",
      "103/103 [==============================] - 0s 595us/step - loss: 0.0271 - val_loss: 1.1845\n",
      "Epoch 294/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0266 - val_loss: 0.8915\n",
      "Epoch 295/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0277 - val_loss: 1.0920\n",
      "Epoch 296/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0311 - val_loss: 0.7509\n",
      "Epoch 297/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0318 - val_loss: 1.2579\n",
      "Epoch 298/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0419 - val_loss: 0.9240\n",
      "Epoch 299/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0298 - val_loss: 1.0194\n",
      "Epoch 300/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0266 - val_loss: 0.9231\n",
      "Epoch 301/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0248 - val_loss: 1.1175\n",
      "Epoch 302/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0250 - val_loss: 1.0281\n",
      "Epoch 303/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0316 - val_loss: 1.1467\n",
      "Epoch 304/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0282 - val_loss: 0.7610\n",
      "Epoch 305/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0330 - val_loss: 1.3129\n",
      "Epoch 306/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0418 - val_loss: 0.8913\n",
      "Epoch 307/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0284 - val_loss: 1.0335\n",
      "Epoch 308/2000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0243 - val_loss: 0.9191\n",
      "Epoch 309/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0260 - val_loss: 1.0289\n",
      "Epoch 310/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0278 - val_loss: 1.1155\n",
      "Epoch 311/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0288 - val_loss: 1.1299\n",
      "Epoch 312/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0262 - val_loss: 1.0237\n",
      "Epoch 313/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0306 - val_loss: 0.7604\n",
      "Epoch 314/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 539us/step - loss: 0.0358 - val_loss: 1.3409\n",
      "Epoch 315/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0422 - val_loss: 0.9156\n",
      "Epoch 316/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0282 - val_loss: 0.9784\n",
      "Epoch 317/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0238 - val_loss: 0.9268\n",
      "Epoch 318/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0243 - val_loss: 0.9727\n",
      "Epoch 319/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0231 - val_loss: 1.1230\n",
      "Epoch 320/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0271 - val_loss: 1.0618\n",
      "Epoch 321/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0285 - val_loss: 1.0427\n",
      "Epoch 322/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0340 - val_loss: 0.7037\n",
      "Epoch 323/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0409 - val_loss: 1.2937\n",
      "Epoch 324/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0381 - val_loss: 0.8916\n",
      "Epoch 325/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0256 - val_loss: 0.9861\n",
      "Epoch 326/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0231 - val_loss: 0.9138\n",
      "Epoch 327/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0239 - val_loss: 0.9799\n",
      "Epoch 328/2000\n",
      "103/103 [==============================] - 0s 578us/step - loss: 0.0249 - val_loss: 1.0617\n",
      "Epoch 329/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0275 - val_loss: 1.1026\n",
      "Epoch 330/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0304 - val_loss: 0.9837\n",
      "Epoch 331/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0374 - val_loss: 0.8452\n",
      "Epoch 332/2000\n",
      "103/103 [==============================] - 0s 532us/step - loss: 0.0325 - val_loss: 1.2056\n",
      "Epoch 333/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0344 - val_loss: 0.8291\n",
      "Epoch 334/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0283 - val_loss: 1.0819\n",
      "Epoch 335/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0249 - val_loss: 0.8030\n",
      "Epoch 336/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0265 - val_loss: 1.1150\n",
      "Epoch 337/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0322 - val_loss: 0.7733\n",
      "Epoch 338/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0321 - val_loss: 1.1608\n",
      "Epoch 339/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0312 - val_loss: 0.7467\n",
      "Epoch 340/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0341 - val_loss: 1.0597\n",
      "Epoch 341/2000\n",
      "103/103 [==============================] - 0s 527us/step - loss: 0.0348 - val_loss: 0.7594\n",
      "Epoch 342/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0270 - val_loss: 1.1745\n",
      "Epoch 343/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0281 - val_loss: 0.8486\n",
      "Epoch 344/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0293 - val_loss: 1.0281\n",
      "Epoch 345/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0314 - val_loss: 0.7655\n",
      "Epoch 346/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0277 - val_loss: 1.2402\n",
      "Epoch 347/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0351 - val_loss: 0.8809\n",
      "Epoch 348/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0310 - val_loss: 1.0260\n",
      "Epoch 349/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0282 - val_loss: 0.8303\n",
      "Epoch 350/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0251 - val_loss: 1.2233\n",
      "Epoch 351/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0307 - val_loss: 0.9174\n",
      "Epoch 352/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0328 - val_loss: 1.0342\n",
      "Epoch 353/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0285 - val_loss: 0.8496\n",
      "Epoch 354/2000\n",
      "103/103 [==============================] - 0s 532us/step - loss: 0.0247 - val_loss: 1.2301\n",
      "Epoch 355/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0305 - val_loss: 0.9131\n",
      "Epoch 356/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0341 - val_loss: 1.0559\n",
      "Epoch 357/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0287 - val_loss: 0.8385\n",
      "Epoch 358/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0248 - val_loss: 1.2456\n",
      "Epoch 359/2000\n",
      "103/103 [==============================] - 0s 530us/step - loss: 0.0318 - val_loss: 0.8909\n",
      "Epoch 360/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0334 - val_loss: 1.0664\n",
      "Epoch 361/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0293 - val_loss: 0.8155\n",
      "Epoch 362/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0262 - val_loss: 1.2483\n",
      "Epoch 363/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0331 - val_loss: 0.9004\n",
      "Epoch 364/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0311 - val_loss: 1.0656\n",
      "Epoch 365/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0275 - val_loss: 0.8414\n",
      "Epoch 366/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0256 - val_loss: 1.2386\n",
      "Epoch 367/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0323 - val_loss: 0.9262\n",
      "Epoch 368/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0328 - val_loss: 1.0537\n",
      "Epoch 369/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0276 - val_loss: 0.8803\n",
      "Epoch 370/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0242 - val_loss: 1.2316\n",
      "Epoch 371/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0292 - val_loss: 0.9350\n",
      "Epoch 372/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0339 - val_loss: 1.0820\n",
      "Epoch 373/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0293 - val_loss: 0.8378\n",
      "Epoch 374/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0253 - val_loss: 1.2644\n",
      "Epoch 375/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0330 - val_loss: 0.9020\n",
      "Epoch 376/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0321 - val_loss: 1.0770\n",
      "Epoch 377/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0279 - val_loss: 0.8482\n",
      "Epoch 378/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0256 - val_loss: 1.2481\n",
      "Epoch 379/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0321 - val_loss: 0.9260\n",
      "Epoch 380/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0317 - val_loss: 1.0771\n",
      "Epoch 381/2000\n",
      "103/103 [==============================] - 0s 669us/step - loss: 0.0279 - val_loss: 0.8613\n",
      "Epoch 382/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0252 - val_loss: 1.2546\n",
      "Epoch 383/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0317 - val_loss: 0.9385\n",
      "Epoch 384/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0324 - val_loss: 1.0740\n",
      "Epoch 385/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0275 - val_loss: 0.8877\n",
      "Epoch 386/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0243 - val_loss: 1.2477\n",
      "Epoch 387/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0298 - val_loss: 0.9422\n",
      "Epoch 388/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0335 - val_loss: 1.0882\n",
      "Epoch 389/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0290 - val_loss: 0.8612\n",
      "Epoch 390/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0250 - val_loss: 1.2687\n",
      "Epoch 391/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 546us/step - loss: 0.0318 - val_loss: 0.9225\n",
      "Epoch 392/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0318 - val_loss: 1.0967\n",
      "Epoch 393/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0280 - val_loss: 0.8582\n",
      "Epoch 394/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0257 - val_loss: 1.2609\n",
      "Epoch 395/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0323 - val_loss: 0.9394\n",
      "Epoch 396/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0312 - val_loss: 1.0892\n",
      "Epoch 397/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0274 - val_loss: 0.8855\n",
      "Epoch 398/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0248 - val_loss: 1.2582\n",
      "Epoch 399/2000\n",
      "103/103 [==============================] - 0s 530us/step - loss: 0.0309 - val_loss: 0.9588\n",
      "Epoch 400/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0324 - val_loss: 1.0882\n",
      "Epoch 401/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0277 - val_loss: 0.9018\n",
      "Epoch 402/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0241 - val_loss: 1.2623\n",
      "Epoch 403/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0296 - val_loss: 0.9515\n",
      "Epoch 404/2000\n",
      "103/103 [==============================] - 0s 527us/step - loss: 0.0329 - val_loss: 1.1091\n",
      "Epoch 405/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0288 - val_loss: 0.8707\n",
      "Epoch 406/2000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0252 - val_loss: 1.2758\n",
      "Epoch 407/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0320 - val_loss: 0.9373\n",
      "Epoch 408/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0312 - val_loss: 1.1069\n",
      "Epoch 409/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0276 - val_loss: 0.8804\n",
      "Epoch 410/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0254 - val_loss: 1.2676\n",
      "Epoch 411/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0317 - val_loss: 0.9581\n",
      "Epoch 412/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0312 - val_loss: 1.1032\n",
      "Epoch 413/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0274 - val_loss: 0.9036\n",
      "Epoch 414/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0246 - val_loss: 1.2691\n",
      "Epoch 415/2000\n",
      "103/103 [==============================] - 0s 643us/step - loss: 0.0304 - val_loss: 0.9712\n",
      "Epoch 416/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0321 - val_loss: 1.1051\n",
      "Epoch 417/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0277 - val_loss: 0.9113\n",
      "Epoch 418/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0242 - val_loss: 1.2727\n",
      "Epoch 419/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0298 - val_loss: 0.9609\n",
      "Epoch 420/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0324 - val_loss: 1.1216\n",
      "Epoch 421/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0286 - val_loss: 0.8883\n",
      "Epoch 422/2000\n",
      "103/103 [==============================] - 0s 532us/step - loss: 0.0251 - val_loss: 1.2834\n",
      "Epoch 423/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0315 - val_loss: 0.9534\n",
      "Epoch 424/2000\n",
      "103/103 [==============================] - 0s 528us/step - loss: 0.0308 - val_loss: 1.1211\n",
      "Epoch 425/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0275 - val_loss: 0.8956\n",
      "Epoch 426/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0253 - val_loss: 1.2747\n",
      "Epoch 427/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0315 - val_loss: 0.9723\n",
      "Epoch 428/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0309 - val_loss: 1.1160\n",
      "Epoch 429/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0273 - val_loss: 0.9233\n",
      "Epoch 430/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0243 - val_loss: 1.2770\n",
      "Epoch 431/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0299 - val_loss: 0.9851\n",
      "Epoch 432/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0319 - val_loss: 1.1208\n",
      "Epoch 433/2000\n",
      "103/103 [==============================] - 0s 894us/step - loss: 0.0279 - val_loss: 0.9217\n",
      "Epoch 434/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0241 - val_loss: 1.2818\n",
      "Epoch 435/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0298 - val_loss: 0.9703\n",
      "Epoch 436/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0319 - val_loss: 1.1353\n",
      "Epoch 437/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0283 - val_loss: 0.9012\n",
      "Epoch 438/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0251 - val_loss: 1.2895\n",
      "Epoch 439/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0313 - val_loss: 0.9670\n",
      "Epoch 440/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0304 - val_loss: 1.1336\n",
      "Epoch 441/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0274 - val_loss: 0.9138\n",
      "Epoch 442/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0251 - val_loss: 1.2790\n",
      "Epoch 443/2000\n",
      "103/103 [==============================] - 0s 532us/step - loss: 0.0310 - val_loss: 0.9875\n",
      "Epoch 444/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0307 - val_loss: 1.1263\n",
      "Epoch 445/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0272 - val_loss: 0.9385\n",
      "Epoch 446/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0241 - val_loss: 1.2850\n",
      "Epoch 447/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0296 - val_loss: 0.9940\n",
      "Epoch 448/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0315 - val_loss: 1.1407\n",
      "Epoch 449/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0280 - val_loss: 0.9296\n",
      "Epoch 450/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0242 - val_loss: 1.2905\n",
      "Epoch 451/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0299 - val_loss: 0.9804\n",
      "Epoch 452/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0312 - val_loss: 1.1397\n",
      "Epoch 453/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0277 - val_loss: 0.9144\n",
      "Epoch 454/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0249 - val_loss: 1.2868\n",
      "Epoch 455/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0312 - val_loss: 0.9813\n",
      "Epoch 456/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0304 - val_loss: 1.1497\n",
      "Epoch 457/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0275 - val_loss: 0.9372\n",
      "Epoch 458/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0246 - val_loss: 1.2960\n",
      "Epoch 459/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0301 - val_loss: 1.0004\n",
      "Epoch 460/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0306 - val_loss: 1.1259\n",
      "Epoch 461/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0273 - val_loss: 0.9425\n",
      "Epoch 462/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0241 - val_loss: 1.2749\n",
      "Epoch 463/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0291 - val_loss: 1.0029\n",
      "Epoch 464/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0317 - val_loss: 1.1471\n",
      "Epoch 465/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0282 - val_loss: 0.9481\n",
      "Epoch 466/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0239 - val_loss: 1.3125\n",
      "Epoch 467/2000\n",
      "103/103 [==============================] - 0s 602us/step - loss: 0.0292 - val_loss: 0.9783\n",
      "Epoch 468/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 548us/step - loss: 0.0302 - val_loss: 1.1838\n",
      "Epoch 469/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0281 - val_loss: 0.9085\n",
      "Epoch 470/2000\n",
      "103/103 [==============================] - 0s 739us/step - loss: 0.0262 - val_loss: 1.2737\n",
      "Epoch 471/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0318 - val_loss: 1.0006\n",
      "Epoch 472/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0290 - val_loss: 1.1349\n",
      "Epoch 473/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0260 - val_loss: 0.9494\n",
      "Epoch 474/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0244 - val_loss: 1.2927\n",
      "Epoch 475/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0304 - val_loss: 1.0141\n",
      "Epoch 476/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0307 - val_loss: 1.1567\n",
      "Epoch 477/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0275 - val_loss: 0.9689\n",
      "Epoch 478/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0235 - val_loss: 1.2983\n",
      "Epoch 479/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0283 - val_loss: 1.0087\n",
      "Epoch 480/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0310 - val_loss: 1.1533\n",
      "Epoch 481/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0282 - val_loss: 0.9320\n",
      "Epoch 482/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0245 - val_loss: 1.2879\n",
      "Epoch 483/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0300 - val_loss: 0.9896\n",
      "Epoch 484/2000\n",
      "103/103 [==============================] - 0s 532us/step - loss: 0.0302 - val_loss: 1.1626\n",
      "Epoch 485/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0277 - val_loss: 0.9426\n",
      "Epoch 486/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0249 - val_loss: 1.3105\n",
      "Epoch 487/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0304 - val_loss: 1.0019\n",
      "Epoch 488/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0294 - val_loss: 1.1506\n",
      "Epoch 489/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0274 - val_loss: 0.9527\n",
      "Epoch 490/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0246 - val_loss: 1.2842\n",
      "Epoch 491/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0298 - val_loss: 1.0245\n",
      "Epoch 492/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0302 - val_loss: 1.1539\n",
      "Epoch 493/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0275 - val_loss: 0.9724\n",
      "Epoch 494/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0237 - val_loss: 1.3143\n",
      "Epoch 495/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0288 - val_loss: 1.0080\n",
      "Epoch 496/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0299 - val_loss: 1.2004\n",
      "Epoch 497/2000\n",
      "103/103 [==============================] - 0s 522us/step - loss: 0.0279 - val_loss: 0.9338\n",
      "Epoch 498/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0252 - val_loss: 1.2993\n",
      "Epoch 499/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0309 - val_loss: 1.0189\n",
      "Epoch 500/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0292 - val_loss: 1.1297\n",
      "Epoch 501/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0260 - val_loss: 0.9716\n",
      "Epoch 502/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0236 - val_loss: 1.2787\n",
      "Epoch 503/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0286 - val_loss: 1.0317\n",
      "Epoch 504/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0317 - val_loss: 1.1649\n",
      "Epoch 505/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0285 - val_loss: 0.9904\n",
      "Epoch 506/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0231 - val_loss: 1.3178\n",
      "Epoch 507/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0272 - val_loss: 1.0010\n",
      "Epoch 508/2000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0292 - val_loss: 1.2497\n",
      "Epoch 509/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0279 - val_loss: 0.8870\n",
      "Epoch 510/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0266 - val_loss: 1.3195\n",
      "Epoch 511/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0311 - val_loss: 0.9972\n",
      "Epoch 512/2000\n",
      "103/103 [==============================] - 0s 530us/step - loss: 0.0263 - val_loss: 1.1669\n",
      "Epoch 513/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0247 - val_loss: 1.0053\n",
      "Epoch 514/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0238 - val_loss: 1.2357\n",
      "Epoch 515/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0298 - val_loss: 1.1088\n",
      "Epoch 516/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0320 - val_loss: 1.1286\n",
      "Epoch 517/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0276 - val_loss: 0.9894\n",
      "Epoch 518/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0227 - val_loss: 1.3082\n",
      "Epoch 519/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0268 - val_loss: 0.9653\n",
      "Epoch 520/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0296 - val_loss: 1.2579\n",
      "Epoch 521/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0278 - val_loss: 0.8678\n",
      "Epoch 522/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0273 - val_loss: 1.2842\n",
      "Epoch 523/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0303 - val_loss: 0.9840\n",
      "Epoch 524/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0261 - val_loss: 1.1708\n",
      "Epoch 525/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0245 - val_loss: 0.9540\n",
      "Epoch 526/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0245 - val_loss: 1.2744\n",
      "Epoch 527/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0293 - val_loss: 1.0120\n",
      "Epoch 528/2000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0291 - val_loss: 1.1915\n",
      "Epoch 529/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0264 - val_loss: 0.9618\n",
      "Epoch 530/2000\n",
      "103/103 [==============================] - 0s 637us/step - loss: 0.0265 - val_loss: 1.2974\n",
      "Epoch 531/2000\n",
      "103/103 [==============================] - 0s 644us/step - loss: 0.0283 - val_loss: 1.0115\n",
      "Epoch 532/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0278 - val_loss: 1.2121\n",
      "Epoch 533/2000\n",
      "103/103 [==============================] - 0s 599us/step - loss: 0.0268 - val_loss: 0.9235\n",
      "Epoch 534/2000\n",
      "103/103 [==============================] - 0s 578us/step - loss: 0.0259 - val_loss: 1.3286\n",
      "Epoch 535/2000\n",
      "103/103 [==============================] - 0s 581us/step - loss: 0.0297 - val_loss: 0.9914\n",
      "Epoch 536/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0268 - val_loss: 1.2188\n",
      "Epoch 537/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0242 - val_loss: 0.9477\n",
      "Epoch 538/2000\n",
      "103/103 [==============================] - 0s 532us/step - loss: 0.0251 - val_loss: 1.2765\n",
      "Epoch 539/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0290 - val_loss: 0.9937\n",
      "Epoch 540/2000\n",
      "103/103 [==============================] - 0s 582us/step - loss: 0.0293 - val_loss: 1.1958\n",
      "Epoch 541/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0253 - val_loss: 0.9954\n",
      "Epoch 542/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0255 - val_loss: 1.2608\n",
      "Epoch 543/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0264 - val_loss: 1.0374\n",
      "Epoch 544/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0287 - val_loss: 1.2336\n",
      "Epoch 545/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 537us/step - loss: 0.0280 - val_loss: 0.9099\n",
      "Epoch 546/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0271 - val_loss: 1.3339\n",
      "Epoch 547/2000\n",
      "103/103 [==============================] - 0s 570us/step - loss: 0.0294 - val_loss: 0.9918\n",
      "Epoch 548/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0258 - val_loss: 1.2037\n",
      "Epoch 549/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0237 - val_loss: 0.9618\n",
      "Epoch 550/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0245 - val_loss: 1.2347\n",
      "Epoch 551/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0277 - val_loss: 0.9911\n",
      "Epoch 552/2000\n",
      "103/103 [==============================] - 0s 581us/step - loss: 0.0301 - val_loss: 1.2386\n",
      "Epoch 553/2000\n",
      "103/103 [==============================] - 0s 584us/step - loss: 0.0247 - val_loss: 0.9108\n",
      "Epoch 554/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0248 - val_loss: 1.3033\n",
      "Epoch 555/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0313 - val_loss: 0.9520\n",
      "Epoch 556/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0270 - val_loss: 1.2106\n",
      "Epoch 557/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0241 - val_loss: 0.9240\n",
      "Epoch 558/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0262 - val_loss: 1.2883\n",
      "Epoch 559/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0298 - val_loss: 0.9978\n",
      "Epoch 560/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0276 - val_loss: 1.1436\n",
      "Epoch 561/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0266 - val_loss: 1.0338\n",
      "Epoch 562/2000\n",
      "103/103 [==============================] - 0s 525us/step - loss: 0.0255 - val_loss: 1.2262\n",
      "Epoch 563/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0252 - val_loss: 1.0358\n",
      "Epoch 564/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0273 - val_loss: 1.2443\n",
      "Epoch 565/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0276 - val_loss: 0.8488\n",
      "Epoch 566/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0278 - val_loss: 1.2819\n",
      "Epoch 567/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0310 - val_loss: 0.9833\n",
      "Epoch 568/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0254 - val_loss: 1.1629\n",
      "Epoch 569/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0234 - val_loss: 0.9994\n",
      "Epoch 570/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0232 - val_loss: 1.2144\n",
      "Epoch 571/2000\n",
      "103/103 [==============================] - 0s 748us/step - loss: 0.0274 - val_loss: 1.0555\n",
      "Epoch 572/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0307 - val_loss: 1.1821\n",
      "Epoch 573/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0273 - val_loss: 0.9073\n",
      "Epoch 574/2000\n",
      "103/103 [==============================] - 0s 572us/step - loss: 0.0241 - val_loss: 1.3140\n",
      "Epoch 575/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0294 - val_loss: 0.9431\n",
      "Epoch 576/2000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0269 - val_loss: 1.2034\n",
      "Epoch 577/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0246 - val_loss: 0.9084\n",
      "Epoch 578/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0245 - val_loss: 1.2795\n",
      "Epoch 579/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0297 - val_loss: 0.9766\n",
      "Epoch 580/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0283 - val_loss: 1.1269\n",
      "Epoch 581/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0273 - val_loss: 1.0015\n",
      "Epoch 582/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0226 - val_loss: 1.2067\n",
      "Epoch 583/2000\n",
      "103/103 [==============================] - 0s 530us/step - loss: 0.0250 - val_loss: 1.0878\n",
      "Epoch 584/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0282 - val_loss: 1.1363\n",
      "Epoch 585/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0277 - val_loss: 0.8856\n",
      "Epoch 586/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0251 - val_loss: 1.3144\n",
      "Epoch 587/2000\n",
      "103/103 [==============================] - 0s 676us/step - loss: 0.0305 - val_loss: 0.9223\n",
      "Epoch 588/2000\n",
      "103/103 [==============================] - 0s 583us/step - loss: 0.0272 - val_loss: 1.1735\n",
      "Epoch 589/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0252 - val_loss: 0.9351\n",
      "Epoch 590/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0241 - val_loss: 1.2614\n",
      "Epoch 591/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0276 - val_loss: 0.9851\n",
      "Epoch 592/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0274 - val_loss: 1.1524\n",
      "Epoch 593/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0283 - val_loss: 0.9201\n",
      "Epoch 594/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0248 - val_loss: 1.2519\n",
      "Epoch 595/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0287 - val_loss: 0.9979\n",
      "Epoch 596/2000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0272 - val_loss: 1.1741\n",
      "Epoch 597/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0251 - val_loss: 0.9458\n",
      "Epoch 598/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0236 - val_loss: 1.2864\n",
      "Epoch 599/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0283 - val_loss: 0.9788\n",
      "Epoch 600/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0285 - val_loss: 1.1673\n",
      "Epoch 601/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0266 - val_loss: 0.9302\n",
      "Epoch 602/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0241 - val_loss: 1.2609\n",
      "Epoch 603/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0281 - val_loss: 1.0154\n",
      "Epoch 604/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0279 - val_loss: 1.1373\n",
      "Epoch 605/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0256 - val_loss: 0.9653\n",
      "Epoch 606/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0230 - val_loss: 1.2845\n",
      "Epoch 607/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0281 - val_loss: 1.0049\n",
      "Epoch 608/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0290 - val_loss: 1.2026\n",
      "Epoch 609/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0269 - val_loss: 0.9494\n",
      "Epoch 610/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0236 - val_loss: 1.2953\n",
      "Epoch 611/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0269 - val_loss: 0.9976\n",
      "Epoch 612/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0271 - val_loss: 1.1669\n",
      "Epoch 613/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0262 - val_loss: 0.9470\n",
      "Epoch 614/2000\n",
      "103/103 [==============================] - 0s 840us/step - loss: 0.0237 - val_loss: 1.2679\n",
      "Epoch 615/2000\n",
      "103/103 [==============================] - 0s 589us/step - loss: 0.0275 - val_loss: 1.0205\n",
      "Epoch 616/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0283 - val_loss: 1.1539\n",
      "Epoch 617/2000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0267 - val_loss: 0.9768\n",
      "Epoch 618/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0236 - val_loss: 1.2874\n",
      "Epoch 619/2000\n",
      "103/103 [==============================] - 0s 615us/step - loss: 0.0280 - val_loss: 1.0116\n",
      "Epoch 620/2000\n",
      "103/103 [==============================] - 0s 667us/step - loss: 0.0283 - val_loss: 1.2002\n",
      "Epoch 621/2000\n",
      "103/103 [==============================] - 0s 773us/step - loss: 0.0264 - val_loss: 0.9549\n",
      "Epoch 622/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 613us/step - loss: 0.0239 - val_loss: 1.2740\n",
      "Epoch 623/2000\n",
      "103/103 [==============================] - 0s 617us/step - loss: 0.0265 - val_loss: 1.0187\n",
      "Epoch 624/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 1.1632\n",
      "Epoch 625/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.9579\n",
      "Epoch 626/2000\n",
      "103/103 [==============================] - 0s 799us/step - loss: 0.0236 - val_loss: 1.2887\n",
      "Epoch 627/2000\n",
      "103/103 [==============================] - 0s 609us/step - loss: 0.0283 - val_loss: 1.0180\n",
      "Epoch 628/2000\n",
      "103/103 [==============================] - 0s 581us/step - loss: 0.0284 - val_loss: 1.1964\n",
      "Epoch 629/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0262 - val_loss: 0.9842\n",
      "Epoch 630/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0231 - val_loss: 1.3043\n",
      "Epoch 631/2000\n",
      "103/103 [==============================] - 0s 966us/step - loss: 0.0265 - val_loss: 1.0228\n",
      "Epoch 632/2000\n",
      "103/103 [==============================] - 0s 685us/step - loss: 0.0267 - val_loss: 1.1980\n",
      "Epoch 633/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0261 - val_loss: 0.9514\n",
      "Epoch 634/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0234 - val_loss: 1.2827\n",
      "Epoch 635/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0268 - val_loss: 1.0311\n",
      "Epoch 636/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0277 - val_loss: 1.1422\n",
      "Epoch 637/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0262 - val_loss: 1.0244\n",
      "Epoch 638/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0227 - val_loss: 1.2641\n",
      "Epoch 639/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0267 - val_loss: 1.0415\n",
      "Epoch 640/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0294 - val_loss: 1.2234\n",
      "Epoch 641/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0277 - val_loss: 0.9644\n",
      "Epoch 642/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0238 - val_loss: 1.2833\n",
      "Epoch 643/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0253 - val_loss: 1.0110\n",
      "Epoch 644/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0261 - val_loss: 1.1842\n",
      "Epoch 645/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0259 - val_loss: 0.9839\n",
      "Epoch 646/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0233 - val_loss: 1.3024\n",
      "Epoch 647/2000\n",
      "103/103 [==============================] - 0s 579us/step - loss: 0.0259 - val_loss: 1.0005\n",
      "Epoch 648/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0292 - val_loss: 1.1803\n",
      "Epoch 649/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0293 - val_loss: 0.9865\n",
      "Epoch 650/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0237 - val_loss: 1.2882\n",
      "Epoch 651/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0236 - val_loss: 0.9763\n",
      "Epoch 652/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0267 - val_loss: 1.2535\n",
      "Epoch 653/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0279 - val_loss: 0.9747\n",
      "Epoch 654/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0237 - val_loss: 1.2922\n",
      "Epoch 655/2000\n",
      "103/103 [==============================] - 0s 731us/step - loss: 0.0238 - val_loss: 0.9498\n",
      "Epoch 656/2000\n",
      "103/103 [==============================] - 0s 598us/step - loss: 0.0272 - val_loss: 1.2540\n",
      "Epoch 657/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0286 - val_loss: 0.9817\n",
      "Epoch 658/2000\n",
      "103/103 [==============================] - 0s 605us/step - loss: 0.0254 - val_loss: 1.2766\n",
      "Epoch 659/2000\n",
      "103/103 [==============================] - 0s 751us/step - loss: 0.0238 - val_loss: 0.9914\n",
      "Epoch 660/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0243 - val_loss: 1.2370\n",
      "Epoch 661/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0273 - val_loss: 0.9383\n",
      "Epoch 662/2000\n",
      "103/103 [==============================] - 0s 774us/step - loss: 0.0257 - val_loss: 1.2934\n",
      "Epoch 663/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0250 - val_loss: 0.9539\n",
      "Epoch 664/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0273 - val_loss: 1.2233\n",
      "Epoch 665/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0279 - val_loss: 0.9715\n",
      "Epoch 666/2000\n",
      "103/103 [==============================] - 0s 597us/step - loss: 0.0238 - val_loss: 1.2514\n",
      "Epoch 667/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.9506\n",
      "Epoch 668/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0248 - val_loss: 1.2452\n",
      "Epoch 669/2000\n",
      "103/103 [==============================] - 0s 888us/step - loss: 0.0274 - val_loss: 0.9307\n",
      "Epoch 670/2000\n",
      "103/103 [==============================] - 0s 599us/step - loss: 0.0250 - val_loss: 1.2964\n",
      "Epoch 671/2000\n",
      "103/103 [==============================] - 0s 775us/step - loss: 0.0242 - val_loss: 0.8989\n",
      "Epoch 672/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0286 - val_loss: 1.2124\n",
      "Epoch 673/2000\n",
      "103/103 [==============================] - 0s 649us/step - loss: 0.0288 - val_loss: 0.9593\n",
      "Epoch 674/2000\n",
      "103/103 [==============================] - 0s 742us/step - loss: 0.0238 - val_loss: 1.1997\n",
      "Epoch 675/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0217 - val_loss: 0.9859\n",
      "Epoch 676/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0233 - val_loss: 1.1790\n",
      "Epoch 677/2000\n",
      "103/103 [==============================] - 0s 682us/step - loss: 0.0258 - val_loss: 0.9411\n",
      "Epoch 678/2000\n",
      "103/103 [==============================] - 0s 599us/step - loss: 0.0260 - val_loss: 1.3450\n",
      "Epoch 679/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0270 - val_loss: 0.8945\n",
      "Epoch 680/2000\n",
      "103/103 [==============================] - 0s 602us/step - loss: 0.0284 - val_loss: 1.2145\n",
      "Epoch 681/2000\n",
      "103/103 [==============================] - 0s 824us/step - loss: 0.0269 - val_loss: 0.9443\n",
      "Epoch 682/2000\n",
      "103/103 [==============================] - 0s 733us/step - loss: 0.0228 - val_loss: 1.2300\n",
      "Epoch 683/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.9761\n",
      "Epoch 684/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 1.1812\n",
      "Epoch 685/2000\n",
      "103/103 [==============================] - 0s 636us/step - loss: 0.0269 - val_loss: 0.9369\n",
      "Epoch 686/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0256 - val_loss: 1.2989\n",
      "Epoch 687/2000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0248 - val_loss: 0.8964\n",
      "Epoch 688/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0279 - val_loss: 1.2212\n",
      "Epoch 689/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0278 - val_loss: 0.9501\n",
      "Epoch 690/2000\n",
      "103/103 [==============================] - 0s 832us/step - loss: 0.0235 - val_loss: 1.1939\n",
      "Epoch 691/2000\n",
      "103/103 [==============================] - 0s 771us/step - loss: 0.0222 - val_loss: 0.9882\n",
      "Epoch 692/2000\n",
      "103/103 [==============================] - 0s 681us/step - loss: 0.0234 - val_loss: 1.1643\n",
      "Epoch 693/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0256 - val_loss: 0.9246\n",
      "Epoch 694/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0272 - val_loss: 1.2414\n",
      "Epoch 695/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0282 - val_loss: 0.9464\n",
      "Epoch 696/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0272 - val_loss: 1.1493\n",
      "Epoch 697/2000\n",
      "103/103 [==============================] - 0s 888us/step - loss: 0.0262 - val_loss: 0.9842\n",
      "Epoch 698/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 1.2080\n",
      "Epoch 699/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.9533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 1.2277\n",
      "Epoch 701/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.8908\n",
      "Epoch 702/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 1.2667\n",
      "Epoch 703/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.9267\n",
      "Epoch 704/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0272 - val_loss: 1.1702\n",
      "Epoch 705/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.9840\n",
      "Epoch 706/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 1.1749\n",
      "Epoch 707/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.9850\n",
      "Epoch 708/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 1.1864\n",
      "Epoch 709/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.9314\n",
      "Epoch 710/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 1.2386\n",
      "Epoch 711/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0271 - val_loss: 0.9356\n",
      "Epoch 712/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 1.1647\n",
      "Epoch 713/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.9864\n",
      "Epoch 714/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 1.1870\n",
      "Epoch 715/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.9850\n",
      "Epoch 716/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 1.1741\n",
      "Epoch 717/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0261 - val_loss: 0.9053\n",
      "Epoch 718/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0264 - val_loss: 1.2514\n",
      "Epoch 719/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.9433\n",
      "Epoch 720/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0268 - val_loss: 1.1632\n",
      "Epoch 721/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.9770\n",
      "Epoch 722/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 1.2095\n",
      "Epoch 723/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.9533\n",
      "Epoch 724/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 1.2176\n",
      "Epoch 725/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.9347\n",
      "Epoch 726/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 1.2661\n",
      "Epoch 727/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.8996\n",
      "Epoch 728/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 1.2122\n",
      "Epoch 729/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0265 - val_loss: 0.9318\n",
      "Epoch 730/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 1.2403\n",
      "Epoch 731/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.9201\n",
      "Epoch 732/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 1.1953\n",
      "Epoch 733/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.9465\n",
      "Epoch 734/2000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0245 - val_loss: 1.2159\n",
      "Epoch 735/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0222 - val_loss: 0.9362\n",
      "Epoch 736/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0240 - val_loss: 1.2108\n",
      "Epoch 737/2000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0264 - val_loss: 0.9336\n",
      "Epoch 738/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0242 - val_loss: 1.2473\n",
      "Epoch 739/2000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.0230 - val_loss: 0.9054\n",
      "Epoch 740/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 1.1995\n",
      "Epoch 741/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.9294\n",
      "Epoch 742/2000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.0234 - val_loss: 1.1965\n",
      "Epoch 743/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0225 - val_loss: 0.9623\n",
      "Epoch 744/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0252 - val_loss: 1.1257\n",
      "Epoch 745/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0284 - val_loss: 0.9645\n",
      "Epoch 746/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 1.2270\n",
      "Epoch 747/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.8993\n",
      "Epoch 748/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0253 - val_loss: 1.2064\n",
      "Epoch 749/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.9304\n",
      "Epoch 750/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 1.2224\n",
      "Epoch 751/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.9301\n",
      "Epoch 752/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0241 - val_loss: 1.1657\n",
      "Epoch 753/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.9303\n",
      "Epoch 754/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 1.2222\n",
      "Epoch 755/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0230 - val_loss: 0.9388\n",
      "Epoch 756/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 1.1373\n",
      "Epoch 757/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.9657\n",
      "Epoch 758/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 1.1666\n",
      "Epoch 759/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 0.9687\n",
      "Epoch 760/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 1.1861\n",
      "Epoch 761/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.9341\n",
      "Epoch 762/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 1.2287\n",
      "Epoch 763/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.9463\n",
      "Epoch 764/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 1.1280\n",
      "Epoch 765/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0257 - val_loss: 0.9693\n",
      "Epoch 766/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 1.2177\n",
      "Epoch 767/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.9692\n",
      "Epoch 768/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 1.1481\n",
      "Epoch 769/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.9417\n",
      "Epoch 770/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0228 - val_loss: 1.2312\n",
      "Epoch 771/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.9698\n",
      "Epoch 772/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 1.1482\n",
      "Epoch 773/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.9681\n",
      "Epoch 774/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0225 - val_loss: 1.2302\n",
      "Epoch 775/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0220 - val_loss: 0.9680\n",
      "Epoch 776/2000\n",
      "103/103 [==============================] - 0s 740us/step - loss: 0.0239 - val_loss: 1.1526\n",
      "Epoch 777/2000\n",
      "103/103 [==============================] - 0s 672us/step - loss: 0.0265 - val_loss: 0.9432\n",
      "Epoch 778/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 1.2331\n",
      "Epoch 779/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.9715\n",
      "Epoch 780/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 1.1492\n",
      "Epoch 781/2000\n",
      "103/103 [==============================] - 0s 920us/step - loss: 0.0273 - val_loss: 0.9548\n",
      "Epoch 782/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0239 - val_loss: 1.1972\n",
      "Epoch 783/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.9786\n",
      "Epoch 784/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 1.1926\n",
      "Epoch 785/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0256 - val_loss: 0.9572\n",
      "Epoch 786/2000\n",
      "103/103 [==============================] - 0s 578us/step - loss: 0.0224 - val_loss: 1.2491\n",
      "Epoch 787/2000\n",
      "103/103 [==============================] - 0s 663us/step - loss: 0.0227 - val_loss: 0.9699\n",
      "Epoch 788/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 1.1971\n",
      "Epoch 789/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0252 - val_loss: 0.9504\n",
      "Epoch 790/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0230 - val_loss: 1.2987\n",
      "Epoch 791/2000\n",
      "103/103 [==============================] - 0s 617us/step - loss: 0.0240 - val_loss: 0.9624\n",
      "Epoch 792/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 1.2071\n",
      "Epoch 793/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.9641\n",
      "Epoch 794/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 1.2798\n",
      "Epoch 795/2000\n",
      "103/103 [==============================] - 0s 741us/step - loss: 0.0230 - val_loss: 0.9917\n",
      "Epoch 796/2000\n",
      "103/103 [==============================] - 0s 695us/step - loss: 0.0248 - val_loss: 1.2180\n",
      "Epoch 797/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0250 - val_loss: 0.9452\n",
      "Epoch 798/2000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0223 - val_loss: 1.2933\n",
      "Epoch 799/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0238 - val_loss: 0.9468\n",
      "Epoch 800/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 1.2196\n",
      "Epoch 801/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0248 - val_loss: 0.9683\n",
      "Epoch 802/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0219 - val_loss: 1.2563\n",
      "Epoch 803/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0218 - val_loss: 0.9739\n",
      "Epoch 804/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0243 - val_loss: 1.1845\n",
      "Epoch 805/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0269 - val_loss: 0.9865\n",
      "Epoch 806/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0232 - val_loss: 1.2131\n",
      "Epoch 807/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0230 - val_loss: 1.0031\n",
      "Epoch 808/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0242 - val_loss: 1.1983\n",
      "Epoch 809/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0251 - val_loss: 0.9866\n",
      "Epoch 810/2000\n",
      "103/103 [==============================] - 0s 811us/step - loss: 0.0226 - val_loss: 1.2608\n",
      "Epoch 811/2000\n",
      "103/103 [==============================] - 0s 606us/step - loss: 0.0228 - val_loss: 0.9638\n",
      "Epoch 812/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0248 - val_loss: 1.1704\n",
      "Epoch 813/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0267 - val_loss: 0.9999\n",
      "Epoch 814/2000\n",
      "103/103 [==============================] - 0s 589us/step - loss: 0.0225 - val_loss: 1.2183\n",
      "Epoch 815/2000\n",
      "103/103 [==============================] - 0s 581us/step - loss: 0.0220 - val_loss: 1.0013\n",
      "Epoch 816/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 1.1357\n",
      "Epoch 817/2000\n",
      "103/103 [==============================] - 0s 627us/step - loss: 0.0263 - val_loss: 0.9842\n",
      "Epoch 818/2000\n",
      "103/103 [==============================] - 0s 615us/step - loss: 0.0221 - val_loss: 1.2619\n",
      "Epoch 819/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0220 - val_loss: 0.9728\n",
      "Epoch 820/2000\n",
      "103/103 [==============================] - 0s 570us/step - loss: 0.0239 - val_loss: 1.2024\n",
      "Epoch 821/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0254 - val_loss: 0.9642\n",
      "Epoch 822/2000\n",
      "103/103 [==============================] - 0s 636us/step - loss: 0.0218 - val_loss: 1.2533\n",
      "Epoch 823/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0224 - val_loss: 0.9657\n",
      "Epoch 824/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0244 - val_loss: 1.2085\n",
      "Epoch 825/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0248 - val_loss: 0.9784\n",
      "Epoch 826/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0226 - val_loss: 1.2485\n",
      "Epoch 827/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0220 - val_loss: 0.9813\n",
      "Epoch 828/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0244 - val_loss: 1.1905\n",
      "Epoch 829/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0270 - val_loss: 1.0096\n",
      "Epoch 830/2000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0225 - val_loss: 1.2446\n",
      "Epoch 831/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0212 - val_loss: 1.0098\n",
      "Epoch 832/2000\n",
      "103/103 [==============================] - 0s 617us/step - loss: 0.0220 - val_loss: 1.2039\n",
      "Epoch 833/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0238 - val_loss: 0.9558\n",
      "Epoch 834/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0231 - val_loss: 1.2918\n",
      "Epoch 835/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0233 - val_loss: 0.9861\n",
      "Epoch 836/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0254 - val_loss: 1.1501\n",
      "Epoch 837/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0252 - val_loss: 0.9971\n",
      "Epoch 838/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0219 - val_loss: 1.2204\n",
      "Epoch 839/2000\n",
      "103/103 [==============================] - 0s 569us/step - loss: 0.0204 - val_loss: 0.9970\n",
      "Epoch 840/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0226 - val_loss: 1.1977\n",
      "Epoch 841/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0254 - val_loss: 0.9689\n",
      "Epoch 842/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0231 - val_loss: 1.2628\n",
      "Epoch 843/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0226 - val_loss: 0.9351\n",
      "Epoch 844/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0246 - val_loss: 1.2014\n",
      "Epoch 845/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0244 - val_loss: 0.9758\n",
      "Epoch 846/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0214 - val_loss: 1.2238\n",
      "Epoch 847/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0215 - val_loss: 0.9918\n",
      "Epoch 848/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0229 - val_loss: 1.1976\n",
      "Epoch 849/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0245 - val_loss: 0.9539\n",
      "Epoch 850/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0230 - val_loss: 1.2682\n",
      "Epoch 851/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0231 - val_loss: 0.9518\n",
      "Epoch 852/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0234 - val_loss: 1.2197\n",
      "Epoch 853/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0248 - val_loss: 0.9678\n",
      "Epoch 854/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0220 - val_loss: 1.2365\n",
      "Epoch 855/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 584us/step - loss: 0.0219 - val_loss: 0.9661\n",
      "Epoch 856/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0240 - val_loss: 1.2081\n",
      "Epoch 857/2000\n",
      "103/103 [==============================] - 0s 697us/step - loss: 0.0250 - val_loss: 0.9929\n",
      "Epoch 858/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0220 - val_loss: 1.2451\n",
      "Epoch 859/2000\n",
      "103/103 [==============================] - 0s 610us/step - loss: 0.0218 - val_loss: 0.9603\n",
      "Epoch 860/2000\n",
      "103/103 [==============================] - 0s 600us/step - loss: 0.0236 - val_loss: 1.2218\n",
      "Epoch 861/2000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0239 - val_loss: 0.9760\n",
      "Epoch 862/2000\n",
      "103/103 [==============================] - 0s 601us/step - loss: 0.0217 - val_loss: 1.2620\n",
      "Epoch 863/2000\n",
      "103/103 [==============================] - 0s 591us/step - loss: 0.0218 - val_loss: 0.9738\n",
      "Epoch 864/2000\n",
      "103/103 [==============================] - 0s 619us/step - loss: 0.0243 - val_loss: 1.1790\n",
      "Epoch 865/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0250 - val_loss: 0.9674\n",
      "Epoch 866/2000\n",
      "103/103 [==============================] - 0s 612us/step - loss: 0.0217 - val_loss: 1.2394\n",
      "Epoch 867/2000\n",
      "103/103 [==============================] - 0s 603us/step - loss: 0.0218 - val_loss: 0.9752\n",
      "Epoch 868/2000\n",
      "103/103 [==============================] - 0s 614us/step - loss: 0.0232 - val_loss: 1.2112\n",
      "Epoch 869/2000\n",
      "103/103 [==============================] - 0s 606us/step - loss: 0.0237 - val_loss: 0.9583\n",
      "Epoch 870/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0220 - val_loss: 1.2276\n",
      "Epoch 871/2000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0214 - val_loss: 0.9688\n",
      "Epoch 872/2000\n",
      "103/103 [==============================] - 0s 601us/step - loss: 0.0228 - val_loss: 1.2183\n",
      "Epoch 873/2000\n",
      "103/103 [==============================] - 0s 597us/step - loss: 0.0246 - val_loss: 0.9934\n",
      "Epoch 874/2000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0227 - val_loss: 1.2265\n",
      "Epoch 875/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0227 - val_loss: 1.0148\n",
      "Epoch 876/2000\n",
      "103/103 [==============================] - 0s 582us/step - loss: 0.0232 - val_loss: 1.1873\n",
      "Epoch 877/2000\n",
      "103/103 [==============================] - 0s 599us/step - loss: 0.0239 - val_loss: 1.0176\n",
      "Epoch 878/2000\n",
      "103/103 [==============================] - 0s 583us/step - loss: 0.0224 - val_loss: 1.2494\n",
      "Epoch 879/2000\n",
      "103/103 [==============================] - 0s 612us/step - loss: 0.0215 - val_loss: 0.9765\n",
      "Epoch 880/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0233 - val_loss: 1.2001\n",
      "Epoch 881/2000\n",
      "103/103 [==============================] - 0s 606us/step - loss: 0.0242 - val_loss: 1.0049\n",
      "Epoch 882/2000\n",
      "103/103 [==============================] - 0s 596us/step - loss: 0.0227 - val_loss: 1.2033\n",
      "Epoch 883/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0205 - val_loss: 1.0064\n",
      "Epoch 884/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0212 - val_loss: 1.1865\n",
      "Epoch 885/2000\n",
      "103/103 [==============================] - 0s 581us/step - loss: 0.0240 - val_loss: 1.0445\n",
      "Epoch 886/2000\n",
      "103/103 [==============================] - 0s 601us/step - loss: 0.0238 - val_loss: 1.2101\n",
      "Epoch 887/2000\n",
      "103/103 [==============================] - 0s 569us/step - loss: 0.0229 - val_loss: 0.9750\n",
      "Epoch 888/2000\n",
      "103/103 [==============================] - 0s 584us/step - loss: 0.0232 - val_loss: 1.1876\n",
      "Epoch 889/2000\n",
      "103/103 [==============================] - 0s 619us/step - loss: 0.0246 - val_loss: 1.0154\n",
      "Epoch 890/2000\n",
      "103/103 [==============================] - 0s 602us/step - loss: 0.0224 - val_loss: 1.2011\n",
      "Epoch 891/2000\n",
      "103/103 [==============================] - 0s 761us/step - loss: 0.0210 - val_loss: 0.9963\n",
      "Epoch 892/2000\n",
      "103/103 [==============================] - 0s 707us/step - loss: 0.0217 - val_loss: 1.1980\n",
      "Epoch 893/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0234 - val_loss: 1.0246\n",
      "Epoch 894/2000\n",
      "103/103 [==============================] - 0s 622us/step - loss: 0.0232 - val_loss: 1.2573\n",
      "Epoch 895/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0223 - val_loss: 0.9769\n",
      "Epoch 896/2000\n",
      "103/103 [==============================] - 0s 574us/step - loss: 0.0244 - val_loss: 1.1557\n",
      "Epoch 897/2000\n",
      "103/103 [==============================] - 0s 582us/step - loss: 0.0266 - val_loss: 0.9961\n",
      "Epoch 898/2000\n",
      "103/103 [==============================] - 0s 605us/step - loss: 0.0217 - val_loss: 1.1795\n",
      "Epoch 899/2000\n",
      "103/103 [==============================] - 0s 592us/step - loss: 0.0198 - val_loss: 1.0143\n",
      "Epoch 900/2000\n",
      "103/103 [==============================] - 0s 616us/step - loss: 0.0205 - val_loss: 1.1748\n",
      "Epoch 901/2000\n",
      "103/103 [==============================] - 0s 611us/step - loss: 0.0236 - val_loss: 0.9907\n",
      "Epoch 902/2000\n",
      "103/103 [==============================] - 0s 609us/step - loss: 0.0231 - val_loss: 1.2450\n",
      "Epoch 903/2000\n",
      "103/103 [==============================] - 0s 601us/step - loss: 0.0239 - val_loss: 1.0112\n",
      "Epoch 904/2000\n",
      "103/103 [==============================] - 0s 716us/step - loss: 0.0230 - val_loss: 1.2113\n",
      "Epoch 905/2000\n",
      "103/103 [==============================] - 0s 621us/step - loss: 0.0226 - val_loss: 1.0107\n",
      "Epoch 906/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0216 - val_loss: 1.2686\n",
      "Epoch 907/2000\n",
      "103/103 [==============================] - 0s 795us/step - loss: 0.0228 - val_loss: 0.9914\n",
      "Epoch 908/2000\n",
      "103/103 [==============================] - 0s 610us/step - loss: 0.0227 - val_loss: 1.2173\n",
      "Epoch 909/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0229 - val_loss: 0.9736\n",
      "Epoch 910/2000\n",
      "103/103 [==============================] - 0s 590us/step - loss: 0.0224 - val_loss: 1.2816\n",
      "Epoch 911/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0216 - val_loss: 0.9474\n",
      "Epoch 912/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0234 - val_loss: 1.1960\n",
      "Epoch 913/2000\n",
      "103/103 [==============================] - 0s 752us/step - loss: 0.0237 - val_loss: 0.9753\n",
      "Epoch 914/2000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0212 - val_loss: 1.2166\n",
      "Epoch 915/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0210 - val_loss: 0.9645\n",
      "Epoch 916/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0229 - val_loss: 1.1929\n",
      "Epoch 917/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0238 - val_loss: 0.9421\n",
      "Epoch 918/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0216 - val_loss: 1.1861\n",
      "Epoch 919/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0212 - val_loss: 0.9912\n",
      "Epoch 920/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0215 - val_loss: 1.2109\n",
      "Epoch 921/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0233 - val_loss: 0.9972\n",
      "Epoch 922/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0228 - val_loss: 1.2444\n",
      "Epoch 923/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0224 - val_loss: 0.9676\n",
      "Epoch 924/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0230 - val_loss: 1.2075\n",
      "Epoch 925/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0239 - val_loss: 0.9938\n",
      "Epoch 926/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0215 - val_loss: 1.2418\n",
      "Epoch 927/2000\n",
      "103/103 [==============================] - 0s 953us/step - loss: 0.0205 - val_loss: 1.0146\n",
      "Epoch 928/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 1.2126\n",
      "Epoch 929/2000\n",
      "103/103 [==============================] - 0s 612us/step - loss: 0.0233 - val_loss: 1.0396\n",
      "Epoch 930/2000\n",
      "103/103 [==============================] - 0s 604us/step - loss: 0.0227 - val_loss: 1.2336\n",
      "Epoch 931/2000\n",
      "103/103 [==============================] - 0s 624us/step - loss: 0.0211 - val_loss: 1.0135\n",
      "Epoch 932/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 587us/step - loss: 0.0205 - val_loss: 1.2620\n",
      "Epoch 933/2000\n",
      "103/103 [==============================] - 0s 607us/step - loss: 0.0228 - val_loss: 1.0421\n",
      "Epoch 934/2000\n",
      "103/103 [==============================] - 0s 578us/step - loss: 0.0236 - val_loss: 1.2362\n",
      "Epoch 935/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0220 - val_loss: 0.9988\n",
      "Epoch 936/2000\n",
      "103/103 [==============================] - 0s 629us/step - loss: 0.0220 - val_loss: 1.2315\n",
      "Epoch 937/2000\n",
      "103/103 [==============================] - 0s 572us/step - loss: 0.0226 - val_loss: 1.0757\n",
      "Epoch 938/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0226 - val_loss: 1.2022\n",
      "Epoch 939/2000\n",
      "103/103 [==============================] - 0s 879us/step - loss: 0.0216 - val_loss: 1.0550\n",
      "Epoch 940/2000\n",
      "103/103 [==============================] - 0s 978us/step - loss: 0.0198 - val_loss: 1.2170\n",
      "Epoch 941/2000\n",
      "103/103 [==============================] - 0s 773us/step - loss: 0.0204 - val_loss: 1.0993\n",
      "Epoch 942/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 1.2537\n",
      "Epoch 943/2000\n",
      "103/103 [==============================] - 0s 855us/step - loss: 0.0236 - val_loss: 0.9736\n",
      "Epoch 944/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0234 - val_loss: 1.2520\n",
      "Epoch 945/2000\n",
      "103/103 [==============================] - 0s 759us/step - loss: 0.0243 - val_loss: 1.0629\n",
      "Epoch 946/2000\n",
      "103/103 [==============================] - 0s 820us/step - loss: 0.0211 - val_loss: 1.2092\n",
      "Epoch 947/2000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0208 - val_loss: 1.0481\n",
      "Epoch 948/2000\n",
      "103/103 [==============================] - 0s 603us/step - loss: 0.0207 - val_loss: 1.2529\n",
      "Epoch 949/2000\n",
      "103/103 [==============================] - 0s 842us/step - loss: 0.0209 - val_loss: 1.0892\n",
      "Epoch 950/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0219 - val_loss: 1.2628\n",
      "Epoch 951/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0232 - val_loss: 0.9406\n",
      "Epoch 952/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0246 - val_loss: 1.3040\n",
      "Epoch 953/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0234 - val_loss: 1.0597\n",
      "Epoch 954/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0213 - val_loss: 1.2396\n",
      "Epoch 955/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0203 - val_loss: 1.0813\n",
      "Epoch 956/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0195 - val_loss: 1.2472\n",
      "Epoch 957/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0202 - val_loss: 1.1051\n",
      "Epoch 958/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0221 - val_loss: 1.2971\n",
      "Epoch 959/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0234 - val_loss: 1.0039\n",
      "Epoch 960/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0241 - val_loss: 1.2887\n",
      "Epoch 961/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0256 - val_loss: 1.0846\n",
      "Epoch 962/2000\n",
      "103/103 [==============================] - 0s 621us/step - loss: 0.0216 - val_loss: 1.2021\n",
      "Epoch 963/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0205 - val_loss: 1.0946\n",
      "Epoch 964/2000\n",
      "103/103 [==============================] - 0s 658us/step - loss: 0.0197 - val_loss: 1.2741\n",
      "Epoch 965/2000\n",
      "103/103 [==============================] - 0s 973us/step - loss: 0.0206 - val_loss: 1.1126\n",
      "Epoch 966/2000\n",
      "103/103 [==============================] - 0s 524us/step - loss: 0.0204 - val_loss: 1.2813\n",
      "Epoch 967/2000\n",
      "103/103 [==============================] - 0s 791us/step - loss: 0.0222 - val_loss: 1.0067\n",
      "Epoch 968/2000\n",
      "103/103 [==============================] - 0s 654us/step - loss: 0.0247 - val_loss: 1.3146\n",
      "Epoch 969/2000\n",
      "103/103 [==============================] - 0s 671us/step - loss: 0.0231 - val_loss: 1.0424\n",
      "Epoch 970/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0223 - val_loss: 1.2732\n",
      "Epoch 971/2000\n",
      "103/103 [==============================] - 0s 588us/step - loss: 0.0226 - val_loss: 1.0635\n",
      "Epoch 972/2000\n",
      "103/103 [==============================] - 0s 758us/step - loss: 0.0209 - val_loss: 1.2317\n",
      "Epoch 973/2000\n",
      "103/103 [==============================] - 0s 603us/step - loss: 0.0205 - val_loss: 1.0680\n",
      "Epoch 974/2000\n",
      "103/103 [==============================] - 0s 609us/step - loss: 0.0219 - val_loss: 1.2095\n",
      "Epoch 975/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0244 - val_loss: 1.0197\n",
      "Epoch 976/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0227 - val_loss: 1.2114\n",
      "Epoch 977/2000\n",
      "103/103 [==============================] - 0s 598us/step - loss: 0.0225 - val_loss: 1.0583\n",
      "Epoch 978/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 1.2431\n",
      "Epoch 979/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 1.1068\n",
      "Epoch 980/2000\n",
      "103/103 [==============================] - 0s 598us/step - loss: 0.0207 - val_loss: 1.2540\n",
      "Epoch 981/2000\n",
      "103/103 [==============================] - 0s 598us/step - loss: 0.0213 - val_loss: 1.0748\n",
      "Epoch 982/2000\n",
      "103/103 [==============================] - 0s 578us/step - loss: 0.0217 - val_loss: 1.2867\n",
      "Epoch 983/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0231 - val_loss: 1.0193\n",
      "Epoch 984/2000\n",
      "103/103 [==============================] - 0s 574us/step - loss: 0.0224 - val_loss: 1.2651\n",
      "Epoch 985/2000\n",
      "103/103 [==============================] - 0s 778us/step - loss: 0.0228 - val_loss: 1.0616\n",
      "Epoch 986/2000\n",
      "103/103 [==============================] - 0s 627us/step - loss: 0.0213 - val_loss: 1.2135\n",
      "Epoch 987/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0214 - val_loss: 1.0320\n",
      "Epoch 988/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0201 - val_loss: 1.2738\n",
      "Epoch 989/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 1.0949\n",
      "Epoch 990/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 1.2175\n",
      "Epoch 991/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 1.0967\n",
      "Epoch 992/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 1.2951\n",
      "Epoch 993/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 1.1348\n",
      "Epoch 994/2000\n",
      "103/103 [==============================] - 0s 736us/step - loss: 0.0217 - val_loss: 1.2440\n",
      "Epoch 995/2000\n",
      "103/103 [==============================] - 0s 818us/step - loss: 0.0212 - val_loss: 1.0480\n",
      "Epoch 996/2000\n",
      "103/103 [==============================] - 0s 955us/step - loss: 0.0215 - val_loss: 1.2732\n",
      "Epoch 997/2000\n",
      "103/103 [==============================] - 0s 579us/step - loss: 0.0228 - val_loss: 1.1241\n",
      "Epoch 998/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0226 - val_loss: 1.2426\n",
      "Epoch 999/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0206 - val_loss: 1.1030\n",
      "Epoch 1000/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0200 - val_loss: 1.2259\n",
      "Epoch 1001/2000\n",
      "103/103 [==============================] - 0s 625us/step - loss: 0.0207 - val_loss: 1.1634\n",
      "Epoch 1002/2000\n",
      "103/103 [==============================] - 0s 999us/step - loss: 0.0215 - val_loss: 1.2956\n",
      "Epoch 1003/2000\n",
      "103/103 [==============================] - 0s 949us/step - loss: 0.0216 - val_loss: 1.0446\n",
      "Epoch 1004/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 1.2955\n",
      "Epoch 1005/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0241 - val_loss: 1.1158\n",
      "Epoch 1006/2000\n",
      "103/103 [==============================] - 0s 837us/step - loss: 0.0219 - val_loss: 1.2621\n",
      "Epoch 1007/2000\n",
      "103/103 [==============================] - 0s 879us/step - loss: 0.0213 - val_loss: 1.1264\n",
      "Epoch 1008/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0201 - val_loss: 1.2695\n",
      "Epoch 1009/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 542us/step - loss: 0.0203 - val_loss: 1.1082\n",
      "Epoch 1010/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0214 - val_loss: 1.2749\n",
      "Epoch 1011/2000\n",
      "103/103 [==============================] - 0s 644us/step - loss: 0.0234 - val_loss: 1.1149\n",
      "Epoch 1012/2000\n",
      "103/103 [==============================] - 0s 578us/step - loss: 0.0217 - val_loss: 1.2605\n",
      "Epoch 1013/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0214 - val_loss: 1.1671\n",
      "Epoch 1014/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0209 - val_loss: 1.2897\n",
      "Epoch 1015/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0215 - val_loss: 1.1253\n",
      "Epoch 1016/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0208 - val_loss: 1.3098\n",
      "Epoch 1017/2000\n",
      "103/103 [==============================] - 0s 590us/step - loss: 0.0209 - val_loss: 1.1440\n",
      "Epoch 1018/2000\n",
      "103/103 [==============================] - 0s 607us/step - loss: 0.0216 - val_loss: 1.3715\n",
      "Epoch 1019/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0215 - val_loss: 1.0688\n",
      "Epoch 1020/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0237 - val_loss: 1.3676\n",
      "Epoch 1021/2000\n",
      "103/103 [==============================] - 0s 636us/step - loss: 0.0231 - val_loss: 1.1452\n",
      "Epoch 1022/2000\n",
      "103/103 [==============================] - 0s 661us/step - loss: 0.0206 - val_loss: 1.2971\n",
      "Epoch 1023/2000\n",
      "103/103 [==============================] - 0s 610us/step - loss: 0.0193 - val_loss: 1.1309\n",
      "Epoch 1024/2000\n",
      "103/103 [==============================] - 0s 620us/step - loss: 0.0197 - val_loss: 1.3139\n",
      "Epoch 1025/2000\n",
      "103/103 [==============================] - 0s 612us/step - loss: 0.0210 - val_loss: 1.1643\n",
      "Epoch 1026/2000\n",
      "103/103 [==============================] - 0s 611us/step - loss: 0.0225 - val_loss: 1.2985\n",
      "Epoch 1027/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 1.1058\n",
      "Epoch 1028/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 1.3462\n",
      "Epoch 1029/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 1.1421\n",
      "Epoch 1030/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 1.3328\n",
      "Epoch 1031/2000\n",
      "103/103 [==============================] - 0s 580us/step - loss: 0.0201 - val_loss: 1.1588\n",
      "Epoch 1032/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 1.3227\n",
      "Epoch 1033/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 1.1428\n",
      "Epoch 1034/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0211 - val_loss: 1.3772\n",
      "Epoch 1035/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 1.1302\n",
      "Epoch 1036/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0226 - val_loss: 1.3436\n",
      "Epoch 1037/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0241 - val_loss: 1.1542\n",
      "Epoch 1038/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 1.2723\n",
      "Epoch 1039/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 1.2371\n",
      "Epoch 1040/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 1.2999\n",
      "Epoch 1041/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0198 - val_loss: 1.2323\n",
      "Epoch 1042/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 1.2753\n",
      "Epoch 1043/2000\n",
      "103/103 [==============================] - 0s 843us/step - loss: 0.0263 - val_loss: 1.1374\n",
      "Epoch 1044/2000\n",
      "103/103 [==============================] - 0s 850us/step - loss: 0.0232 - val_loss: 1.3356\n",
      "Epoch 1045/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 1.1260\n",
      "Epoch 1046/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0208 - val_loss: 1.2927\n",
      "Epoch 1047/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0209 - val_loss: 1.1685\n",
      "Epoch 1048/2000\n",
      "103/103 [==============================] - 0s 877us/step - loss: 0.0194 - val_loss: 1.3430\n",
      "Epoch 1049/2000\n",
      "103/103 [==============================] - 0s 767us/step - loss: 0.0206 - val_loss: 1.2160\n",
      "Epoch 1050/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0216 - val_loss: 1.2891\n",
      "Epoch 1051/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0215 - val_loss: 1.1756\n",
      "Epoch 1052/2000\n",
      "103/103 [==============================] - 0s 578us/step - loss: 0.0200 - val_loss: 1.3687\n",
      "Epoch 1053/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 1.1431\n",
      "Epoch 1054/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0219 - val_loss: 1.3471\n",
      "Epoch 1055/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0226 - val_loss: 1.1343\n",
      "Epoch 1056/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 1.3211\n",
      "Epoch 1057/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 1.2349\n",
      "Epoch 1058/2000\n",
      "103/103 [==============================] - 0s 840us/step - loss: 0.0195 - val_loss: 1.2934\n",
      "Epoch 1059/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0204 - val_loss: 1.1716\n",
      "Epoch 1060/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 1.3578\n",
      "Epoch 1061/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0236 - val_loss: 1.2061\n",
      "Epoch 1062/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0228 - val_loss: 1.3202\n",
      "Epoch 1063/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 1.1827\n",
      "Epoch 1064/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 1.3294\n",
      "Epoch 1065/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 1.2283\n",
      "Epoch 1066/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0193 - val_loss: 1.3643\n",
      "Epoch 1067/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 1.1965\n",
      "Epoch 1068/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 1.3597\n",
      "Epoch 1069/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 1.1419\n",
      "Epoch 1070/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0227 - val_loss: 1.2982\n",
      "Epoch 1071/2000\n",
      "103/103 [==============================] - 0s 590us/step - loss: 0.0229 - val_loss: 1.2430\n",
      "Epoch 1072/2000\n",
      "103/103 [==============================] - 0s 620us/step - loss: 0.0216 - val_loss: 1.2497\n",
      "Epoch 1073/2000\n",
      "103/103 [==============================] - 0s 596us/step - loss: 0.0216 - val_loss: 1.1981\n",
      "Epoch 1074/2000\n",
      "103/103 [==============================] - 0s 707us/step - loss: 0.0202 - val_loss: 1.3347\n",
      "Epoch 1075/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 1.1813\n",
      "Epoch 1076/2000\n",
      "103/103 [==============================] - 0s 648us/step - loss: 0.0218 - val_loss: 1.3073\n",
      "Epoch 1077/2000\n",
      "103/103 [==============================] - 0s 594us/step - loss: 0.0231 - val_loss: 1.1713\n",
      "Epoch 1078/2000\n",
      "103/103 [==============================] - 0s 574us/step - loss: 0.0210 - val_loss: 1.3135\n",
      "Epoch 1079/2000\n",
      "103/103 [==============================] - 0s 618us/step - loss: 0.0208 - val_loss: 1.2434\n",
      "Epoch 1080/2000\n",
      "103/103 [==============================] - 0s 938us/step - loss: 0.0193 - val_loss: 1.3211\n",
      "Epoch 1081/2000\n",
      "103/103 [==============================] - 0s 942us/step - loss: 0.0187 - val_loss: 1.2348\n",
      "Epoch 1082/2000\n",
      "103/103 [==============================] - 0s 861us/step - loss: 0.0187 - val_loss: 1.3191\n",
      "Epoch 1083/2000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0194 - val_loss: 1.2667\n",
      "Epoch 1084/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0205 - val_loss: 1.2031\n",
      "Epoch 1085/2000\n",
      "103/103 [==============================] - 0s 579us/step - loss: 0.0251 - val_loss: 1.3545\n",
      "Epoch 1086/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 543us/step - loss: 0.0250 - val_loss: 1.1567\n",
      "Epoch 1087/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0224 - val_loss: 1.2814\n",
      "Epoch 1088/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0202 - val_loss: 1.2438\n",
      "Epoch 1089/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0181 - val_loss: 1.2822\n",
      "Epoch 1090/2000\n",
      "103/103 [==============================] - 0s 690us/step - loss: 0.0179 - val_loss: 1.2806\n",
      "Epoch 1091/2000\n",
      "103/103 [==============================] - 0s 612us/step - loss: 0.0184 - val_loss: 1.2150\n",
      "Epoch 1092/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0193 - val_loss: 1.3413\n",
      "Epoch 1093/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 1.1400\n",
      "Epoch 1094/2000\n",
      "103/103 [==============================] - 0s 895us/step - loss: 0.0250 - val_loss: 1.3196\n",
      "Epoch 1095/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0236 - val_loss: 1.1882\n",
      "Epoch 1096/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0205 - val_loss: 1.2470\n",
      "Epoch 1097/2000\n",
      "103/103 [==============================] - 0s 857us/step - loss: 0.0201 - val_loss: 1.1899\n",
      "Epoch 1098/2000\n",
      "103/103 [==============================] - 0s 914us/step - loss: 0.0190 - val_loss: 1.2929\n",
      "Epoch 1099/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 1.2550\n",
      "Epoch 1100/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0193 - val_loss: 1.2562\n",
      "Epoch 1101/2000\n",
      "103/103 [==============================] - 0s 623us/step - loss: 0.0191 - val_loss: 1.2434\n",
      "Epoch 1102/2000\n",
      "103/103 [==============================] - 0s 579us/step - loss: 0.0192 - val_loss: 1.3803\n",
      "Epoch 1103/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0200 - val_loss: 1.1658\n",
      "Epoch 1104/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0220 - val_loss: 1.3538\n",
      "Epoch 1105/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0245 - val_loss: 1.1637\n",
      "Epoch 1106/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0210 - val_loss: 1.3086\n",
      "Epoch 1107/2000\n",
      "103/103 [==============================] - 0s 716us/step - loss: 0.0202 - val_loss: 1.2319\n",
      "Epoch 1108/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0181 - val_loss: 1.3120\n",
      "Epoch 1109/2000\n",
      "103/103 [==============================] - 0s 572us/step - loss: 0.0187 - val_loss: 1.1954\n",
      "Epoch 1110/2000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0209 - val_loss: 1.3159\n",
      "Epoch 1111/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0239 - val_loss: 1.1180\n",
      "Epoch 1112/2000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0209 - val_loss: 1.3481\n",
      "Epoch 1113/2000\n",
      "103/103 [==============================] - 0s 532us/step - loss: 0.0213 - val_loss: 1.1689\n",
      "Epoch 1114/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0206 - val_loss: 1.3217\n",
      "Epoch 1115/2000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0209 - val_loss: 1.1610\n",
      "Epoch 1116/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0195 - val_loss: 1.3718\n",
      "Epoch 1117/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0212 - val_loss: 1.2329\n",
      "Epoch 1118/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0212 - val_loss: 1.2965\n",
      "Epoch 1119/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0208 - val_loss: 1.2068\n",
      "Epoch 1120/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0199 - val_loss: 1.3817\n",
      "Epoch 1121/2000\n",
      "103/103 [==============================] - 0s 920us/step - loss: 0.0208 - val_loss: 1.2569\n",
      "Epoch 1122/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0196 - val_loss: 1.3147\n",
      "Epoch 1123/2000\n",
      "103/103 [==============================] - 0s 704us/step - loss: 0.0191 - val_loss: 1.2215\n",
      "Epoch 1124/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0199 - val_loss: 1.3451\n",
      "Epoch 1125/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0199 - val_loss: 1.3013\n",
      "Epoch 1126/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0202 - val_loss: 1.2885\n",
      "Epoch 1127/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0207 - val_loss: 1.1844\n",
      "Epoch 1128/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0208 - val_loss: 1.3946\n",
      "Epoch 1129/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0219 - val_loss: 1.2314\n",
      "Epoch 1130/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0207 - val_loss: 1.3483\n",
      "Epoch 1131/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0190 - val_loss: 1.1871\n",
      "Epoch 1132/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0197 - val_loss: 1.3501\n",
      "Epoch 1133/2000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0216 - val_loss: 1.1901\n",
      "Epoch 1134/2000\n",
      "103/103 [==============================] - 0s 584us/step - loss: 0.0201 - val_loss: 1.3217\n",
      "Epoch 1135/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0199 - val_loss: 1.1529\n",
      "Epoch 1136/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0209 - val_loss: 1.3386\n",
      "Epoch 1137/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 1.2054\n",
      "Epoch 1138/2000\n",
      "103/103 [==============================] - 0s 907us/step - loss: 0.0193 - val_loss: 1.3354\n",
      "Epoch 1139/2000\n",
      "103/103 [==============================] - 0s 872us/step - loss: 0.0190 - val_loss: 1.1527\n",
      "Epoch 1140/2000\n",
      "103/103 [==============================] - 0s 615us/step - loss: 0.0204 - val_loss: 1.4109\n",
      "Epoch 1141/2000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0223 - val_loss: 1.1833\n",
      "Epoch 1142/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0198 - val_loss: 1.2865\n",
      "Epoch 1143/2000\n",
      "103/103 [==============================] - 0s 911us/step - loss: 0.0195 - val_loss: 1.1792\n",
      "Epoch 1144/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 1.3710\n",
      "Epoch 1145/2000\n",
      "103/103 [==============================] - 0s 598us/step - loss: 0.0213 - val_loss: 1.2330\n",
      "Epoch 1146/2000\n",
      "103/103 [==============================] - 0s 602us/step - loss: 0.0211 - val_loss: 1.3114\n",
      "Epoch 1147/2000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0204 - val_loss: 1.1562\n",
      "Epoch 1148/2000\n",
      "103/103 [==============================] - 0s 621us/step - loss: 0.0199 - val_loss: 1.4041\n",
      "Epoch 1149/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0216 - val_loss: 1.2489\n",
      "Epoch 1150/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0202 - val_loss: 1.3259\n",
      "Epoch 1151/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0194 - val_loss: 1.2008\n",
      "Epoch 1152/2000\n",
      "103/103 [==============================] - 0s 672us/step - loss: 0.0195 - val_loss: 1.3676\n",
      "Epoch 1153/2000\n",
      "103/103 [==============================] - 0s 647us/step - loss: 0.0213 - val_loss: 1.2964\n",
      "Epoch 1154/2000\n",
      "103/103 [==============================] - 0s 574us/step - loss: 0.0204 - val_loss: 1.3005\n",
      "Epoch 1155/2000\n",
      "103/103 [==============================] - 0s 570us/step - loss: 0.0194 - val_loss: 1.2667\n",
      "Epoch 1156/2000\n",
      "103/103 [==============================] - 0s 621us/step - loss: 0.0193 - val_loss: 1.4010\n",
      "Epoch 1157/2000\n",
      "103/103 [==============================] - 0s 604us/step - loss: 0.0200 - val_loss: 1.2565\n",
      "Epoch 1158/2000\n",
      "103/103 [==============================] - 0s 696us/step - loss: 0.0205 - val_loss: 1.3979\n",
      "Epoch 1159/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0203 - val_loss: 1.1843\n",
      "Epoch 1160/2000\n",
      "103/103 [==============================] - 0s 621us/step - loss: 0.0203 - val_loss: 1.4155\n",
      "Epoch 1161/2000\n",
      "103/103 [==============================] - 0s 788us/step - loss: 0.0203 - val_loss: 1.2208\n",
      "Epoch 1162/2000\n",
      "103/103 [==============================] - 0s 609us/step - loss: 0.0200 - val_loss: 1.4023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1163/2000\n",
      "103/103 [==============================] - 0s 592us/step - loss: 0.0203 - val_loss: 1.2061\n",
      "Epoch 1164/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0201 - val_loss: 1.3434\n",
      "Epoch 1165/2000\n",
      "103/103 [==============================] - 0s 694us/step - loss: 0.0205 - val_loss: 1.1989\n",
      "Epoch 1166/2000\n",
      "103/103 [==============================] - 0s 697us/step - loss: 0.0198 - val_loss: 1.3671\n",
      "Epoch 1167/2000\n",
      "103/103 [==============================] - 0s 579us/step - loss: 0.0203 - val_loss: 1.2311\n",
      "Epoch 1168/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0210 - val_loss: 1.2990\n",
      "Epoch 1169/2000\n",
      "103/103 [==============================] - 0s 641us/step - loss: 0.0229 - val_loss: 1.2126\n",
      "Epoch 1170/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0198 - val_loss: 1.3362\n",
      "Epoch 1171/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0195 - val_loss: 1.2252\n",
      "Epoch 1172/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0196 - val_loss: 1.3598\n",
      "Epoch 1173/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0201 - val_loss: 1.1851\n",
      "Epoch 1174/2000\n",
      "103/103 [==============================] - 0s 735us/step - loss: 0.0197 - val_loss: 1.3703\n",
      "Epoch 1175/2000\n",
      "103/103 [==============================] - 0s 615us/step - loss: 0.0205 - val_loss: 1.2439\n",
      "Epoch 1176/2000\n",
      "103/103 [==============================] - 0s 830us/step - loss: 0.0210 - val_loss: 1.3401\n",
      "Epoch 1177/2000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0206 - val_loss: 1.2194\n",
      "Epoch 1178/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0193 - val_loss: 1.3421\n",
      "Epoch 1179/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0190 - val_loss: 1.2406\n",
      "Epoch 1180/2000\n",
      "103/103 [==============================] - 0s 758us/step - loss: 0.0189 - val_loss: 1.3391\n",
      "Epoch 1181/2000\n",
      "103/103 [==============================] - 0s 861us/step - loss: 0.0200 - val_loss: 1.2166\n",
      "Epoch 1182/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0196 - val_loss: 1.3770\n",
      "Epoch 1183/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0197 - val_loss: 1.2748\n",
      "Epoch 1184/2000\n",
      "103/103 [==============================] - 0s 851us/step - loss: 0.0201 - val_loss: 1.3341\n",
      "Epoch 1185/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0206 - val_loss: 1.2466\n",
      "Epoch 1186/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0187 - val_loss: 1.4206\n",
      "Epoch 1187/2000\n",
      "103/103 [==============================] - 0s 569us/step - loss: 0.0182 - val_loss: 1.2577\n",
      "Epoch 1188/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0191 - val_loss: 1.3904\n",
      "Epoch 1189/2000\n",
      "103/103 [==============================] - 0s 530us/step - loss: 0.0218 - val_loss: 1.1905\n",
      "Epoch 1190/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0216 - val_loss: 1.4213\n",
      "Epoch 1191/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0191 - val_loss: 1.2419\n",
      "Epoch 1192/2000\n",
      "103/103 [==============================] - 0s 801us/step - loss: 0.0180 - val_loss: 1.3755\n",
      "Epoch 1193/2000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0187 - val_loss: 1.1958\n",
      "Epoch 1194/2000\n",
      "103/103 [==============================] - 0s 681us/step - loss: 0.0202 - val_loss: 1.4072\n",
      "Epoch 1195/2000\n",
      "103/103 [==============================] - 0s 694us/step - loss: 0.0219 - val_loss: 1.2343\n",
      "Epoch 1196/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0210 - val_loss: 1.3228\n",
      "Epoch 1197/2000\n",
      "103/103 [==============================] - 0s 767us/step - loss: 0.0214 - val_loss: 1.2352\n",
      "Epoch 1198/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0190 - val_loss: 1.3353\n",
      "Epoch 1199/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0188 - val_loss: 1.2483\n",
      "Epoch 1200/2000\n",
      "103/103 [==============================] - 0s 641us/step - loss: 0.0188 - val_loss: 1.3743\n",
      "Epoch 1201/2000\n",
      "103/103 [==============================] - 0s 784us/step - loss: 0.0199 - val_loss: 1.2293\n",
      "Epoch 1202/2000\n",
      "103/103 [==============================] - 0s 570us/step - loss: 0.0201 - val_loss: 1.3797\n",
      "Epoch 1203/2000\n",
      "103/103 [==============================] - 0s 881us/step - loss: 0.0197 - val_loss: 1.1912\n",
      "Epoch 1204/2000\n",
      "103/103 [==============================] - 0s 600us/step - loss: 0.0207 - val_loss: 1.3555\n",
      "Epoch 1205/2000\n",
      "103/103 [==============================] - 0s 691us/step - loss: 0.0214 - val_loss: 1.2382\n",
      "Epoch 1206/2000\n",
      "103/103 [==============================] - 0s 751us/step - loss: 0.0190 - val_loss: 1.3219\n",
      "Epoch 1207/2000\n",
      "103/103 [==============================] - 0s 781us/step - loss: 0.0190 - val_loss: 1.2609\n",
      "Epoch 1208/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0188 - val_loss: 1.4057\n",
      "Epoch 1209/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0196 - val_loss: 1.2299\n",
      "Epoch 1210/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0196 - val_loss: 1.4082\n",
      "Epoch 1211/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0208 - val_loss: 1.2401\n",
      "Epoch 1212/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0210 - val_loss: 1.3795\n",
      "Epoch 1213/2000\n",
      "103/103 [==============================] - 0s 970us/step - loss: 0.0201 - val_loss: 1.2443\n",
      "Epoch 1214/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0180 - val_loss: 1.3481\n",
      "Epoch 1215/2000\n",
      "103/103 [==============================] - 0s 695us/step - loss: 0.0179 - val_loss: 1.2592\n",
      "Epoch 1216/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0191 - val_loss: 1.3489\n",
      "Epoch 1217/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0217 - val_loss: 1.2684\n",
      "Epoch 1218/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0203 - val_loss: 1.3210\n",
      "Epoch 1219/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0210 - val_loss: 1.2530\n",
      "Epoch 1220/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0189 - val_loss: 1.4127\n",
      "Epoch 1221/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0192 - val_loss: 1.2602\n",
      "Epoch 1222/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0195 - val_loss: 1.3850\n",
      "Epoch 1223/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0200 - val_loss: 1.2011\n",
      "Epoch 1224/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0203 - val_loss: 1.3536\n",
      "Epoch 1225/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0214 - val_loss: 1.2848\n",
      "Epoch 1226/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0193 - val_loss: 1.3353\n",
      "Epoch 1227/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0178 - val_loss: 1.2085\n",
      "Epoch 1228/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0181 - val_loss: 1.3896\n",
      "Epoch 1229/2000\n",
      "103/103 [==============================] - 0s 579us/step - loss: 0.0209 - val_loss: 1.2448\n",
      "Epoch 1230/2000\n",
      "103/103 [==============================] - 0s 620us/step - loss: 0.0215 - val_loss: 1.3525\n",
      "Epoch 1231/2000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0207 - val_loss: 1.2337\n",
      "Epoch 1232/2000\n",
      "103/103 [==============================] - 0s 618us/step - loss: 0.0188 - val_loss: 1.3575\n",
      "Epoch 1233/2000\n",
      "103/103 [==============================] - 0s 618us/step - loss: 0.0188 - val_loss: 1.2373\n",
      "Epoch 1234/2000\n",
      "103/103 [==============================] - 0s 587us/step - loss: 0.0182 - val_loss: 1.3707\n",
      "Epoch 1235/2000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0190 - val_loss: 1.2127\n",
      "Epoch 1236/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0198 - val_loss: 1.3539\n",
      "Epoch 1237/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0188 - val_loss: 1.2292\n",
      "Epoch 1238/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0187 - val_loss: 1.3827\n",
      "Epoch 1239/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 541us/step - loss: 0.0192 - val_loss: 1.2418\n",
      "Epoch 1240/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0198 - val_loss: 1.3116\n",
      "Epoch 1241/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0194 - val_loss: 1.2278\n",
      "Epoch 1242/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0181 - val_loss: 1.3675\n",
      "Epoch 1243/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0176 - val_loss: 1.2186\n",
      "Epoch 1244/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0181 - val_loss: 1.3813\n",
      "Epoch 1245/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0188 - val_loss: 1.2259\n",
      "Epoch 1246/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0199 - val_loss: 1.4008\n",
      "Epoch 1247/2000\n",
      "103/103 [==============================] - 0s 587us/step - loss: 0.0190 - val_loss: 1.1931\n",
      "Epoch 1248/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0187 - val_loss: 1.4338\n",
      "Epoch 1249/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0196 - val_loss: 1.2328\n",
      "Epoch 1250/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0206 - val_loss: 1.3400\n",
      "Epoch 1251/2000\n",
      "103/103 [==============================] - 0s 696us/step - loss: 0.0201 - val_loss: 1.1947\n",
      "Epoch 1252/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 1.3889\n",
      "Epoch 1253/2000\n",
      "103/103 [==============================] - 0s 600us/step - loss: 0.0197 - val_loss: 1.2325\n",
      "Epoch 1254/2000\n",
      "103/103 [==============================] - 0s 782us/step - loss: 0.0190 - val_loss: 1.3480\n",
      "Epoch 1255/2000\n",
      "103/103 [==============================] - 0s 595us/step - loss: 0.0190 - val_loss: 1.1834\n",
      "Epoch 1256/2000\n",
      "103/103 [==============================] - 0s 603us/step - loss: 0.0186 - val_loss: 1.3501\n",
      "Epoch 1257/2000\n",
      "103/103 [==============================] - 0s 800us/step - loss: 0.0188 - val_loss: 1.2471\n",
      "Epoch 1258/2000\n",
      "103/103 [==============================] - 0s 755us/step - loss: 0.0192 - val_loss: 1.3234\n",
      "Epoch 1259/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0195 - val_loss: 1.2344\n",
      "Epoch 1260/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0183 - val_loss: 1.3952\n",
      "Epoch 1261/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0192 - val_loss: 1.2151\n",
      "Epoch 1262/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0193 - val_loss: 1.3977\n",
      "Epoch 1263/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0191 - val_loss: 1.2196\n",
      "Epoch 1264/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0187 - val_loss: 1.3545\n",
      "Epoch 1265/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0186 - val_loss: 1.2296\n",
      "Epoch 1266/2000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0192 - val_loss: 1.3821\n",
      "Epoch 1267/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0210 - val_loss: 1.2028\n",
      "Epoch 1268/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0191 - val_loss: 1.3708\n",
      "Epoch 1269/2000\n",
      "103/103 [==============================] - 0s 590us/step - loss: 0.0196 - val_loss: 1.2384\n",
      "Epoch 1270/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0189 - val_loss: 1.3460\n",
      "Epoch 1271/2000\n",
      "103/103 [==============================] - 0s 722us/step - loss: 0.0196 - val_loss: 1.2768\n",
      "Epoch 1272/2000\n",
      "103/103 [==============================] - 0s 777us/step - loss: 0.0185 - val_loss: 1.3777\n",
      "Epoch 1273/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0190 - val_loss: 1.2742\n",
      "Epoch 1274/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0188 - val_loss: 1.3511\n",
      "Epoch 1275/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0194 - val_loss: 1.2991\n",
      "Epoch 1276/2000\n",
      "103/103 [==============================] - 0s 637us/step - loss: 0.0180 - val_loss: 1.3876\n",
      "Epoch 1277/2000\n",
      "103/103 [==============================] - 0s 620us/step - loss: 0.0183 - val_loss: 1.2126\n",
      "Epoch 1278/2000\n",
      "103/103 [==============================] - 0s 625us/step - loss: 0.0201 - val_loss: 1.4417\n",
      "Epoch 1279/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0210 - val_loss: 1.2561\n",
      "Epoch 1280/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 1.3715\n",
      "Epoch 1281/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0175 - val_loss: 1.2672\n",
      "Epoch 1282/2000\n",
      "103/103 [==============================] - 0s 572us/step - loss: 0.0176 - val_loss: 1.3956\n",
      "Epoch 1283/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0178 - val_loss: 1.2564\n",
      "Epoch 1284/2000\n",
      "103/103 [==============================] - 0s 680us/step - loss: 0.0190 - val_loss: 1.4008\n",
      "Epoch 1285/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0204 - val_loss: 1.1764\n",
      "Epoch 1286/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0199 - val_loss: 1.3662\n",
      "Epoch 1287/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0180 - val_loss: 1.2568\n",
      "Epoch 1288/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0178 - val_loss: 1.3551\n",
      "Epoch 1289/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0176 - val_loss: 1.2353\n",
      "Epoch 1290/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 1.3534\n",
      "Epoch 1291/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0200 - val_loss: 1.3104\n",
      "Epoch 1292/2000\n",
      "103/103 [==============================] - 0s 649us/step - loss: 0.0181 - val_loss: 1.3980\n",
      "Epoch 1293/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0181 - val_loss: 1.2276\n",
      "Epoch 1294/2000\n",
      "103/103 [==============================] - 0s 595us/step - loss: 0.0197 - val_loss: 1.4028\n",
      "Epoch 1295/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 1.2633\n",
      "Epoch 1296/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0183 - val_loss: 1.3678\n",
      "Epoch 1297/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0173 - val_loss: 1.2243\n",
      "Epoch 1298/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0177 - val_loss: 1.4259\n",
      "Epoch 1299/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0203 - val_loss: 1.2727\n",
      "Epoch 1300/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 1.3662\n",
      "Epoch 1301/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 1.2671\n",
      "Epoch 1302/2000\n",
      "103/103 [==============================] - 0s 742us/step - loss: 0.0181 - val_loss: 1.4159\n",
      "Epoch 1303/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0176 - val_loss: 1.2580\n",
      "Epoch 1304/2000\n",
      "103/103 [==============================] - 0s 523us/step - loss: 0.0181 - val_loss: 1.3706\n",
      "Epoch 1305/2000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0189 - val_loss: 1.1964\n",
      "Epoch 1306/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0192 - val_loss: 1.4040\n",
      "Epoch 1307/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 1.2409\n",
      "Epoch 1308/2000\n",
      "103/103 [==============================] - 0s 943us/step - loss: 0.0179 - val_loss: 1.4281\n",
      "Epoch 1309/2000\n",
      "103/103 [==============================] - 0s 584us/step - loss: 0.0191 - val_loss: 1.2191\n",
      "Epoch 1310/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0192 - val_loss: 1.3590\n",
      "Epoch 1311/2000\n",
      "103/103 [==============================] - 0s 620us/step - loss: 0.0194 - val_loss: 1.2004\n",
      "Epoch 1312/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0179 - val_loss: 1.4105\n",
      "Epoch 1313/2000\n",
      "103/103 [==============================] - 0s 608us/step - loss: 0.0178 - val_loss: 1.2592\n",
      "Epoch 1314/2000\n",
      "103/103 [==============================] - 0s 624us/step - loss: 0.0175 - val_loss: 1.3907\n",
      "Epoch 1315/2000\n",
      "103/103 [==============================] - 0s 592us/step - loss: 0.0184 - val_loss: 1.2585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1316/2000\n",
      "103/103 [==============================] - 0s 818us/step - loss: 0.0198 - val_loss: 1.4032\n",
      "Epoch 1317/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0205 - val_loss: 1.2583\n",
      "Epoch 1318/2000\n",
      "103/103 [==============================] - 0s 767us/step - loss: 0.0195 - val_loss: 1.3632\n",
      "Epoch 1319/2000\n",
      "103/103 [==============================] - 0s 945us/step - loss: 0.0190 - val_loss: 1.2562\n",
      "Epoch 1320/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0172 - val_loss: 1.3714\n",
      "Epoch 1321/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0175 - val_loss: 1.2632\n",
      "Epoch 1322/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0185 - val_loss: 1.3719\n",
      "Epoch 1323/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0195 - val_loss: 1.2878\n",
      "Epoch 1324/2000\n",
      "103/103 [==============================] - 0s 603us/step - loss: 0.0184 - val_loss: 1.3644\n",
      "Epoch 1325/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0187 - val_loss: 1.2883\n",
      "Epoch 1326/2000\n",
      "103/103 [==============================] - 0s 887us/step - loss: 0.0180 - val_loss: 1.4166\n",
      "Epoch 1327/2000\n",
      "103/103 [==============================] - 0s 594us/step - loss: 0.0196 - val_loss: 1.3390\n",
      "Epoch 1328/2000\n",
      "103/103 [==============================] - 0s 591us/step - loss: 0.0188 - val_loss: 1.3715\n",
      "Epoch 1329/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0184 - val_loss: 1.2816\n",
      "Epoch 1330/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 1.4382\n",
      "Epoch 1331/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 1.3066\n",
      "Epoch 1332/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 1.4339\n",
      "Epoch 1333/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 1.2304\n",
      "Epoch 1334/2000\n",
      "103/103 [==============================] - 0s 598us/step - loss: 0.0202 - val_loss: 1.4729\n",
      "Epoch 1335/2000\n",
      "103/103 [==============================] - 0s 831us/step - loss: 0.0192 - val_loss: 1.3174\n",
      "Epoch 1336/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0191 - val_loss: 1.4094\n",
      "Epoch 1337/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 1.2516\n",
      "Epoch 1338/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 1.3970\n",
      "Epoch 1339/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 1.3137\n",
      "Epoch 1340/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 1.3811\n",
      "Epoch 1341/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 1.3280\n",
      "Epoch 1342/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0176 - val_loss: 1.4116\n",
      "Epoch 1343/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0173 - val_loss: 1.3714\n",
      "Epoch 1344/2000\n",
      "103/103 [==============================] - 0s 609us/step - loss: 0.0175 - val_loss: 1.4327\n",
      "Epoch 1345/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0193 - val_loss: 1.2828\n",
      "Epoch 1346/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0196 - val_loss: 1.4741\n",
      "Epoch 1347/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0193 - val_loss: 1.2431\n",
      "Epoch 1348/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0194 - val_loss: 1.3733\n",
      "Epoch 1349/2000\n",
      "103/103 [==============================] - 0s 881us/step - loss: 0.0179 - val_loss: 1.2981\n",
      "Epoch 1350/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0165 - val_loss: 1.3668\n",
      "Epoch 1351/2000\n",
      "103/103 [==============================] - 0s 708us/step - loss: 0.0164 - val_loss: 1.2957\n",
      "Epoch 1352/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0175 - val_loss: 1.4500\n",
      "Epoch 1353/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0193 - val_loss: 1.3159\n",
      "Epoch 1354/2000\n",
      "103/103 [==============================] - 0s 722us/step - loss: 0.0191 - val_loss: 1.3498\n",
      "Epoch 1355/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0184 - val_loss: 1.3203\n",
      "Epoch 1356/2000\n",
      "103/103 [==============================] - 0s 632us/step - loss: 0.0170 - val_loss: 1.4488\n",
      "Epoch 1357/2000\n",
      "103/103 [==============================] - 0s 715us/step - loss: 0.0173 - val_loss: 1.2722\n",
      "Epoch 1358/2000\n",
      "103/103 [==============================] - 0s 842us/step - loss: 0.0185 - val_loss: 1.4009\n",
      "Epoch 1359/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0191 - val_loss: 1.2483\n",
      "Epoch 1360/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0185 - val_loss: 1.4122\n",
      "Epoch 1361/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0178 - val_loss: 1.2932\n",
      "Epoch 1362/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0178 - val_loss: 1.4394\n",
      "Epoch 1363/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0188 - val_loss: 1.3131\n",
      "Epoch 1364/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0188 - val_loss: 1.4063\n",
      "Epoch 1365/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0188 - val_loss: 1.2752\n",
      "Epoch 1366/2000\n",
      "103/103 [==============================] - 0s 608us/step - loss: 0.0174 - val_loss: 1.4442\n",
      "Epoch 1367/2000\n",
      "103/103 [==============================] - 0s 700us/step - loss: 0.0173 - val_loss: 1.2616\n",
      "Epoch 1368/2000\n",
      "103/103 [==============================] - 0s 574us/step - loss: 0.0178 - val_loss: 1.4379\n",
      "Epoch 1369/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0198 - val_loss: 1.2763\n",
      "Epoch 1370/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0179 - val_loss: 1.4029\n",
      "Epoch 1371/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0174 - val_loss: 1.2725\n",
      "Epoch 1372/2000\n",
      "103/103 [==============================] - 0s 684us/step - loss: 0.0178 - val_loss: 1.4433\n",
      "Epoch 1373/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0202 - val_loss: 1.2873\n",
      "Epoch 1374/2000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0183 - val_loss: 1.3921\n",
      "Epoch 1375/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0177 - val_loss: 1.2750\n",
      "Epoch 1376/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0170 - val_loss: 1.4214\n",
      "Epoch 1377/2000\n",
      "103/103 [==============================] - 0s 701us/step - loss: 0.0191 - val_loss: 1.2794\n",
      "Epoch 1378/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0193 - val_loss: 1.3955\n",
      "Epoch 1379/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0183 - val_loss: 1.2858\n",
      "Epoch 1380/2000\n",
      "103/103 [==============================] - 0s 572us/step - loss: 0.0181 - val_loss: 1.3968\n",
      "Epoch 1381/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0185 - val_loss: 1.2912\n",
      "Epoch 1382/2000\n",
      "103/103 [==============================] - 0s 834us/step - loss: 0.0179 - val_loss: 1.4411\n",
      "Epoch 1383/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0171 - val_loss: 1.3098\n",
      "Epoch 1384/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0186 - val_loss: 1.3550\n",
      "Epoch 1385/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0186 - val_loss: 1.3554\n",
      "Epoch 1386/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0175 - val_loss: 1.4766\n",
      "Epoch 1387/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0160 - val_loss: 1.3870\n",
      "Epoch 1388/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 1.4266\n",
      "Epoch 1389/2000\n",
      "103/103 [==============================] - 0s 569us/step - loss: 0.0159 - val_loss: 1.3711\n",
      "Epoch 1390/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0185 - val_loss: 1.4716\n",
      "Epoch 1391/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0202 - val_loss: 1.2848\n",
      "Epoch 1392/2000\n",
      "103/103 [==============================] - 0s 579us/step - loss: 0.0181 - val_loss: 1.4738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1393/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0175 - val_loss: 1.3247\n",
      "Epoch 1394/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0177 - val_loss: 1.4503\n",
      "Epoch 1395/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0181 - val_loss: 1.3031\n",
      "Epoch 1396/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0173 - val_loss: 1.4290\n",
      "Epoch 1397/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0183 - val_loss: 1.3420\n",
      "Epoch 1398/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0170 - val_loss: 1.4858\n",
      "Epoch 1399/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0196 - val_loss: 1.3085\n",
      "Epoch 1400/2000\n",
      "103/103 [==============================] - 0s 687us/step - loss: 0.0194 - val_loss: 1.4084\n",
      "Epoch 1401/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0192 - val_loss: 1.3200\n",
      "Epoch 1402/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0162 - val_loss: 1.4629\n",
      "Epoch 1403/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 1.3308\n",
      "Epoch 1404/2000\n",
      "103/103 [==============================] - 0s 764us/step - loss: 0.0173 - val_loss: 1.4790\n",
      "Epoch 1405/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0188 - val_loss: 1.3119\n",
      "Epoch 1406/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0184 - val_loss: 1.4319\n",
      "Epoch 1407/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0191 - val_loss: 1.3843\n",
      "Epoch 1408/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0171 - val_loss: 1.4678\n",
      "Epoch 1409/2000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0164 - val_loss: 1.3585\n",
      "Epoch 1410/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0177 - val_loss: 1.4184\n",
      "Epoch 1411/2000\n",
      "103/103 [==============================] - 0s 618us/step - loss: 0.0183 - val_loss: 1.2945\n",
      "Epoch 1412/2000\n",
      "103/103 [==============================] - 0s 569us/step - loss: 0.0171 - val_loss: 1.4012\n",
      "Epoch 1413/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0174 - val_loss: 1.3257\n",
      "Epoch 1414/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0168 - val_loss: 1.4490\n",
      "Epoch 1415/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0171 - val_loss: 1.3119\n",
      "Epoch 1416/2000\n",
      "103/103 [==============================] - 0s 582us/step - loss: 0.0179 - val_loss: 1.4434\n",
      "Epoch 1417/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0181 - val_loss: 1.2806\n",
      "Epoch 1418/2000\n",
      "103/103 [==============================] - 0s 726us/step - loss: 0.0180 - val_loss: 1.3877\n",
      "Epoch 1419/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0175 - val_loss: 1.3139\n",
      "Epoch 1420/2000\n",
      "103/103 [==============================] - 0s 638us/step - loss: 0.0167 - val_loss: 1.4415\n",
      "Epoch 1421/2000\n",
      "103/103 [==============================] - 0s 928us/step - loss: 0.0174 - val_loss: 1.2826\n",
      "Epoch 1422/2000\n",
      "103/103 [==============================] - 0s 668us/step - loss: 0.0190 - val_loss: 1.4365\n",
      "Epoch 1423/2000\n",
      "103/103 [==============================] - 0s 601us/step - loss: 0.0183 - val_loss: 1.2465\n",
      "Epoch 1424/2000\n",
      "103/103 [==============================] - 0s 729us/step - loss: 0.0172 - val_loss: 1.4032\n",
      "Epoch 1425/2000\n",
      "103/103 [==============================] - 0s 819us/step - loss: 0.0167 - val_loss: 1.2685\n",
      "Epoch 1426/2000\n",
      "103/103 [==============================] - 0s 644us/step - loss: 0.0164 - val_loss: 1.4567\n",
      "Epoch 1427/2000\n",
      "103/103 [==============================] - 0s 620us/step - loss: 0.0190 - val_loss: 1.2875\n",
      "Epoch 1428/2000\n",
      "103/103 [==============================] - 0s 616us/step - loss: 0.0189 - val_loss: 1.3920\n",
      "Epoch 1429/2000\n",
      "103/103 [==============================] - 0s 608us/step - loss: 0.0195 - val_loss: 1.3261\n",
      "Epoch 1430/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 1.4491\n",
      "Epoch 1431/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 1.3306\n",
      "Epoch 1432/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 1.4578\n",
      "Epoch 1433/2000\n",
      "103/103 [==============================] - 0s 664us/step - loss: 0.0179 - val_loss: 1.2753\n",
      "Epoch 1434/2000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0188 - val_loss: 1.4485\n",
      "Epoch 1435/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 1.3577\n",
      "Epoch 1436/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 1.4735\n",
      "Epoch 1437/2000\n",
      "103/103 [==============================] - 0s 893us/step - loss: 0.0167 - val_loss: 1.3274\n",
      "Epoch 1438/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0169 - val_loss: 1.4512\n",
      "Epoch 1439/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0175 - val_loss: 1.3185\n",
      "Epoch 1440/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0169 - val_loss: 1.4729\n",
      "Epoch 1441/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0168 - val_loss: 1.3218\n",
      "Epoch 1442/2000\n",
      "103/103 [==============================] - 0s 532us/step - loss: 0.0166 - val_loss: 1.5011\n",
      "Epoch 1443/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0175 - val_loss: 1.2696\n",
      "Epoch 1444/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0186 - val_loss: 1.4486\n",
      "Epoch 1445/2000\n",
      "103/103 [==============================] - 0s 699us/step - loss: 0.0189 - val_loss: 1.3401\n",
      "Epoch 1446/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0159 - val_loss: 1.4729\n",
      "Epoch 1447/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0159 - val_loss: 1.3384\n",
      "Epoch 1448/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0167 - val_loss: 1.5187\n",
      "Epoch 1449/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0191 - val_loss: 1.3021\n",
      "Epoch 1450/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0179 - val_loss: 1.4294\n",
      "Epoch 1451/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0165 - val_loss: 1.3803\n",
      "Epoch 1452/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0148 - val_loss: 1.4868\n",
      "Epoch 1453/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0156 - val_loss: 1.3616\n",
      "Epoch 1454/2000\n",
      "103/103 [==============================] - 0s 664us/step - loss: 0.0174 - val_loss: 1.4849\n",
      "Epoch 1455/2000\n",
      "103/103 [==============================] - 0s 582us/step - loss: 0.0226 - val_loss: 1.3160\n",
      "Epoch 1456/2000\n",
      "103/103 [==============================] - 0s 596us/step - loss: 0.0180 - val_loss: 1.4735\n",
      "Epoch 1457/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0156 - val_loss: 1.3713\n",
      "Epoch 1458/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0145 - val_loss: 1.4733\n",
      "Epoch 1459/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0157 - val_loss: 1.3602\n",
      "Epoch 1460/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0180 - val_loss: 1.5108\n",
      "Epoch 1461/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 1.3088\n",
      "Epoch 1462/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0169 - val_loss: 1.4717\n",
      "Epoch 1463/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0156 - val_loss: 1.3604\n",
      "Epoch 1464/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0155 - val_loss: 1.5058\n",
      "Epoch 1465/2000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.018 - 0s 4ms/step - loss: 0.0172 - val_loss: 1.3069\n",
      "Epoch 1466/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 1.4493\n",
      "Epoch 1467/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 1.3191\n",
      "Epoch 1468/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 1.4840\n",
      "Epoch 1469/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 1.3116\n",
      "Epoch 1470/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0164 - val_loss: 1.5114\n",
      "Epoch 1471/2000\n",
      "103/103 [==============================] - 0s 570us/step - loss: 0.0188 - val_loss: 1.3351\n",
      "Epoch 1472/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 1.4340\n",
      "Epoch 1473/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 1.3351\n",
      "Epoch 1474/2000\n",
      "103/103 [==============================] - 0s 727us/step - loss: 0.0152 - val_loss: 1.4541\n",
      "Epoch 1475/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 1.3484\n",
      "Epoch 1476/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 1.4525\n",
      "Epoch 1477/2000\n",
      "103/103 [==============================] - 0s 744us/step - loss: 0.0191 - val_loss: 1.3524\n",
      "Epoch 1478/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 1.4571\n",
      "Epoch 1479/2000\n",
      "103/103 [==============================] - 0s 958us/step - loss: 0.0157 - val_loss: 1.3055\n",
      "Epoch 1480/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0156 - val_loss: 1.4571\n",
      "Epoch 1481/2000\n",
      "103/103 [==============================] - 0s 745us/step - loss: 0.0163 - val_loss: 1.2779\n",
      "Epoch 1482/2000\n",
      "103/103 [==============================] - 0s 574us/step - loss: 0.0165 - val_loss: 1.4380\n",
      "Epoch 1483/2000\n",
      "103/103 [==============================] - 0s 602us/step - loss: 0.0174 - val_loss: 1.3051\n",
      "Epoch 1484/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0163 - val_loss: 1.4840\n",
      "Epoch 1485/2000\n",
      "103/103 [==============================] - 0s 688us/step - loss: 0.0171 - val_loss: 1.3009\n",
      "Epoch 1486/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0174 - val_loss: 1.4333\n",
      "Epoch 1487/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0173 - val_loss: 1.3231\n",
      "Epoch 1488/2000\n",
      "103/103 [==============================] - 0s 785us/step - loss: 0.0156 - val_loss: 1.4790\n",
      "Epoch 1489/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 1.2856\n",
      "Epoch 1490/2000\n",
      "103/103 [==============================] - 0s 636us/step - loss: 0.0178 - val_loss: 1.4905\n",
      "Epoch 1491/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0197 - val_loss: 1.2713\n",
      "Epoch 1492/2000\n",
      "103/103 [==============================] - 0s 599us/step - loss: 0.0166 - val_loss: 1.4401\n",
      "Epoch 1493/2000\n",
      "103/103 [==============================] - 0s 583us/step - loss: 0.0150 - val_loss: 1.3451\n",
      "Epoch 1494/2000\n",
      "103/103 [==============================] - 0s 645us/step - loss: 0.0146 - val_loss: 1.4525\n",
      "Epoch 1495/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0154 - val_loss: 1.3302\n",
      "Epoch 1496/2000\n",
      "103/103 [==============================] - 0s 587us/step - loss: 0.0164 - val_loss: 1.4745\n",
      "Epoch 1497/2000\n",
      "103/103 [==============================] - 0s 988us/step - loss: 0.0185 - val_loss: 1.2924\n",
      "Epoch 1498/2000\n",
      "103/103 [==============================] - 0s 594us/step - loss: 0.0166 - val_loss: 1.4483\n",
      "Epoch 1499/2000\n",
      "103/103 [==============================] - 0s 599us/step - loss: 0.0162 - val_loss: 1.3578\n",
      "Epoch 1500/2000\n",
      "103/103 [==============================] - 0s 596us/step - loss: 0.0166 - val_loss: 1.5178\n",
      "Epoch 1501/2000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0181 - val_loss: 1.3176\n",
      "Epoch 1502/2000\n",
      "103/103 [==============================] - 0s 618us/step - loss: 0.0178 - val_loss: 1.3838\n",
      "Epoch 1503/2000\n",
      "103/103 [==============================] - 0s 647us/step - loss: 0.0161 - val_loss: 1.3548\n",
      "Epoch 1504/2000\n",
      "103/103 [==============================] - 0s 614us/step - loss: 0.0169 - val_loss: 1.4139\n",
      "Epoch 1505/2000\n",
      "103/103 [==============================] - 0s 693us/step - loss: 0.0163 - val_loss: 1.3119\n",
      "Epoch 1506/2000\n",
      "103/103 [==============================] - 0s 609us/step - loss: 0.0158 - val_loss: 1.4329\n",
      "Epoch 1507/2000\n",
      "103/103 [==============================] - 0s 584us/step - loss: 0.0155 - val_loss: 1.2801\n",
      "Epoch 1508/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0159 - val_loss: 1.4540\n",
      "Epoch 1509/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0173 - val_loss: 1.2450\n",
      "Epoch 1510/2000\n",
      "103/103 [==============================] - 0s 530us/step - loss: 0.0169 - val_loss: 1.4177\n",
      "Epoch 1511/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0176 - val_loss: 1.3080\n",
      "Epoch 1512/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0173 - val_loss: 1.4217\n",
      "Epoch 1513/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 1.2846\n",
      "Epoch 1514/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 1.4454\n",
      "Epoch 1515/2000\n",
      "103/103 [==============================] - 0s 779us/step - loss: 0.0154 - val_loss: 1.3060\n",
      "Epoch 1516/2000\n",
      "103/103 [==============================] - 0s 697us/step - loss: 0.0167 - val_loss: 1.4915\n",
      "Epoch 1517/2000\n",
      "103/103 [==============================] - 0s 606us/step - loss: 0.0169 - val_loss: 1.2464\n",
      "Epoch 1518/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0165 - val_loss: 1.4292\n",
      "Epoch 1519/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0157 - val_loss: 1.3099\n",
      "Epoch 1520/2000\n",
      "103/103 [==============================] - 0s 589us/step - loss: 0.0149 - val_loss: 1.4315\n",
      "Epoch 1521/2000\n",
      "103/103 [==============================] - 0s 591us/step - loss: 0.0149 - val_loss: 1.3134\n",
      "Epoch 1522/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0171 - val_loss: 1.4619\n",
      "Epoch 1523/2000\n",
      "103/103 [==============================] - 0s 585us/step - loss: 0.0189 - val_loss: 1.2679\n",
      "Epoch 1524/2000\n",
      "103/103 [==============================] - 0s 584us/step - loss: 0.0179 - val_loss: 1.4044\n",
      "Epoch 1525/2000\n",
      "103/103 [==============================] - 0s 579us/step - loss: 0.0153 - val_loss: 1.3227\n",
      "Epoch 1526/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0145 - val_loss: 1.4343\n",
      "Epoch 1527/2000\n",
      "103/103 [==============================] - 0s 865us/step - loss: 0.0144 - val_loss: 1.3085\n",
      "Epoch 1528/2000\n",
      "103/103 [==============================] - 0s 696us/step - loss: 0.0152 - val_loss: 1.4744\n",
      "Epoch 1529/2000\n",
      "103/103 [==============================] - 0s 975us/step - loss: 0.0167 - val_loss: 1.2192\n",
      "Epoch 1530/2000\n",
      "103/103 [==============================] - 0s 912us/step - loss: 0.0175 - val_loss: 1.4429\n",
      "Epoch 1531/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0181 - val_loss: 1.3065\n",
      "Epoch 1532/2000\n",
      "103/103 [==============================] - 0s 615us/step - loss: 0.0155 - val_loss: 1.4318\n",
      "Epoch 1533/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0158 - val_loss: 1.3393\n",
      "Epoch 1534/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0138 - val_loss: 1.4679\n",
      "Epoch 1535/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0145 - val_loss: 1.3589\n",
      "Epoch 1536/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0161 - val_loss: 1.5089\n",
      "Epoch 1537/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0175 - val_loss: 1.2747\n",
      "Epoch 1538/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0171 - val_loss: 1.3756\n",
      "Epoch 1539/2000\n",
      "103/103 [==============================] - 0s 900us/step - loss: 0.0164 - val_loss: 1.3710\n",
      "Epoch 1540/2000\n",
      "103/103 [==============================] - 0s 623us/step - loss: 0.0151 - val_loss: 1.4269\n",
      "Epoch 1541/2000\n",
      "103/103 [==============================] - 0s 594us/step - loss: 0.0141 - val_loss: 1.3055\n",
      "Epoch 1542/2000\n",
      "103/103 [==============================] - 0s 587us/step - loss: 0.0147 - val_loss: 1.4554\n",
      "Epoch 1543/2000\n",
      "103/103 [==============================] - 0s 630us/step - loss: 0.0165 - val_loss: 1.2704\n",
      "Epoch 1544/2000\n",
      "103/103 [==============================] - 0s 611us/step - loss: 0.0169 - val_loss: 1.3793\n",
      "Epoch 1545/2000\n",
      "103/103 [==============================] - 0s 618us/step - loss: 0.0161 - val_loss: 1.2874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1546/2000\n",
      "103/103 [==============================] - 0s 609us/step - loss: 0.0162 - val_loss: 1.4651\n",
      "Epoch 1547/2000\n",
      "103/103 [==============================] - 0s 585us/step - loss: 0.0147 - val_loss: 1.2501\n",
      "Epoch 1548/2000\n",
      "103/103 [==============================] - 0s 641us/step - loss: 0.0153 - val_loss: 1.4565\n",
      "Epoch 1549/2000\n",
      "103/103 [==============================] - 0s 859us/step - loss: 0.0161 - val_loss: 1.2645\n",
      "Epoch 1550/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 1.4067\n",
      "Epoch 1551/2000\n",
      "103/103 [==============================] - 0s 967us/step - loss: 0.0164 - val_loss: 1.3029\n",
      "Epoch 1552/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0143 - val_loss: 1.4147\n",
      "Epoch 1553/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0146 - val_loss: 1.3003\n",
      "Epoch 1554/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0152 - val_loss: 1.4753\n",
      "Epoch 1555/2000\n",
      "103/103 [==============================] - 0s 739us/step - loss: 0.0158 - val_loss: 1.1931\n",
      "Epoch 1556/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0171 - val_loss: 1.4475\n",
      "Epoch 1557/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0174 - val_loss: 1.2552\n",
      "Epoch 1558/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0143 - val_loss: 1.4121\n",
      "Epoch 1559/2000\n",
      "103/103 [==============================] - 0s 912us/step - loss: 0.0142 - val_loss: 1.3194\n",
      "Epoch 1560/2000\n",
      "103/103 [==============================] - 0s 647us/step - loss: 0.0135 - val_loss: 1.4409\n",
      "Epoch 1561/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0147 - val_loss: 1.3617\n",
      "Epoch 1562/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 1.4520\n",
      "Epoch 1563/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0181 - val_loss: 1.2488\n",
      "Epoch 1564/2000\n",
      "103/103 [==============================] - 0s 961us/step - loss: 0.0180 - val_loss: 1.4351\n",
      "Epoch 1565/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 1.3295\n",
      "Epoch 1566/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 1.4441\n",
      "Epoch 1567/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0139 - val_loss: 1.2866\n",
      "Epoch 1568/2000\n",
      "103/103 [==============================] - 0s 606us/step - loss: 0.0143 - val_loss: 1.4353\n",
      "Epoch 1569/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0150 - val_loss: 1.2925\n",
      "Epoch 1570/2000\n",
      "103/103 [==============================] - 0s 923us/step - loss: 0.0154 - val_loss: 1.4672\n",
      "Epoch 1571/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0171 - val_loss: 1.2675\n",
      "Epoch 1572/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0168 - val_loss: 1.4287\n",
      "Epoch 1573/2000\n",
      "103/103 [==============================] - 0s 624us/step - loss: 0.0159 - val_loss: 1.2937\n",
      "Epoch 1574/2000\n",
      "103/103 [==============================] - 0s 602us/step - loss: 0.0145 - val_loss: 1.4445\n",
      "Epoch 1575/2000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0135 - val_loss: 1.3525\n",
      "Epoch 1576/2000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0142 - val_loss: 1.4855\n",
      "Epoch 1577/2000\n",
      "103/103 [==============================] - 0s 580us/step - loss: 0.0154 - val_loss: 1.2835\n",
      "Epoch 1578/2000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0155 - val_loss: 1.5017\n",
      "Epoch 1579/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0168 - val_loss: 1.2613\n",
      "Epoch 1580/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 1.4737\n",
      "Epoch 1581/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 1.3418\n",
      "Epoch 1582/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 1.4597\n",
      "Epoch 1583/2000\n",
      "103/103 [==============================] - 0s 572us/step - loss: 0.0162 - val_loss: 1.3103\n",
      "Epoch 1584/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0144 - val_loss: 1.4679\n",
      "Epoch 1585/2000\n",
      "103/103 [==============================] - 0s 932us/step - loss: 0.0139 - val_loss: 1.3470\n",
      "Epoch 1586/2000\n",
      "103/103 [==============================] - 0s 616us/step - loss: 0.0134 - val_loss: 1.5096\n",
      "Epoch 1587/2000\n",
      "103/103 [==============================] - 0s 993us/step - loss: 0.0144 - val_loss: 1.3207\n",
      "Epoch 1588/2000\n",
      "103/103 [==============================] - 0s 636us/step - loss: 0.0158 - val_loss: 1.4892\n",
      "Epoch 1589/2000\n",
      "103/103 [==============================] - 0s 765us/step - loss: 0.0161 - val_loss: 1.2514\n",
      "Epoch 1590/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0150 - val_loss: 1.4603\n",
      "Epoch 1591/2000\n",
      "103/103 [==============================] - 0s 692us/step - loss: 0.0156 - val_loss: 1.3559\n",
      "Epoch 1592/2000\n",
      "103/103 [==============================] - 0s 605us/step - loss: 0.0143 - val_loss: 1.4045\n",
      "Epoch 1593/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0151 - val_loss: 1.3477\n",
      "Epoch 1594/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0166 - val_loss: 1.4298\n",
      "Epoch 1595/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0153 - val_loss: 1.3591\n",
      "Epoch 1596/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0148 - val_loss: 1.4335\n",
      "Epoch 1597/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0134 - val_loss: 1.3064\n",
      "Epoch 1598/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0143 - val_loss: 1.4780\n",
      "Epoch 1599/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0145 - val_loss: 1.2234\n",
      "Epoch 1600/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0144 - val_loss: 1.4318\n",
      "Epoch 1601/2000\n",
      "103/103 [==============================] - 0s 574us/step - loss: 0.0153 - val_loss: 1.2412\n",
      "Epoch 1602/2000\n",
      "103/103 [==============================] - 0s 844us/step - loss: 0.0153 - val_loss: 1.4498\n",
      "Epoch 1603/2000\n",
      "103/103 [==============================] - 0s 597us/step - loss: 0.0149 - val_loss: 1.2728\n",
      "Epoch 1604/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0149 - val_loss: 1.4142\n",
      "Epoch 1605/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0154 - val_loss: 1.3333\n",
      "Epoch 1606/2000\n",
      "103/103 [==============================] - 0s 578us/step - loss: 0.0130 - val_loss: 1.4253\n",
      "Epoch 1607/2000\n",
      "103/103 [==============================] - 0s 606us/step - loss: 0.0127 - val_loss: 1.3753\n",
      "Epoch 1608/2000\n",
      "103/103 [==============================] - 0s 644us/step - loss: 0.0145 - val_loss: 1.4548\n",
      "Epoch 1609/2000\n",
      "103/103 [==============================] - 0s 696us/step - loss: 0.0161 - val_loss: 1.2188\n",
      "Epoch 1610/2000\n",
      "103/103 [==============================] - 0s 836us/step - loss: 0.0156 - val_loss: 1.4298\n",
      "Epoch 1611/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0142 - val_loss: 1.3032\n",
      "Epoch 1612/2000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0136 - val_loss: 1.4532\n",
      "Epoch 1613/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0139 - val_loss: 1.3062\n",
      "Epoch 1614/2000\n",
      "103/103 [==============================] - 0s 602us/step - loss: 0.0153 - val_loss: 1.4517\n",
      "Epoch 1615/2000\n",
      "103/103 [==============================] - 0s 620us/step - loss: 0.0159 - val_loss: 1.2550\n",
      "Epoch 1616/2000\n",
      "103/103 [==============================] - 0s 604us/step - loss: 0.0164 - val_loss: 1.3621\n",
      "Epoch 1617/2000\n",
      "103/103 [==============================] - 0s 604us/step - loss: 0.0147 - val_loss: 1.3279\n",
      "Epoch 1618/2000\n",
      "103/103 [==============================] - 0s 780us/step - loss: 0.0136 - val_loss: 1.3788\n",
      "Epoch 1619/2000\n",
      "103/103 [==============================] - 0s 663us/step - loss: 0.0132 - val_loss: 1.3530\n",
      "Epoch 1620/2000\n",
      "103/103 [==============================] - 0s 625us/step - loss: 0.0132 - val_loss: 1.3933\n",
      "Epoch 1621/2000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0131 - val_loss: 1.2503\n",
      "Epoch 1622/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 613us/step - loss: 0.0142 - val_loss: 1.4499\n",
      "Epoch 1623/2000\n",
      "103/103 [==============================] - 0s 594us/step - loss: 0.0155 - val_loss: 1.1446\n",
      "Epoch 1624/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0165 - val_loss: 1.3789\n",
      "Epoch 1625/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 1.2123\n",
      "Epoch 1626/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 1.3762\n",
      "Epoch 1627/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 1.2527\n",
      "Epoch 1628/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0137 - val_loss: 1.4526\n",
      "Epoch 1629/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0139 - val_loss: 1.2314\n",
      "Epoch 1630/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0151 - val_loss: 1.4355\n",
      "Epoch 1631/2000\n",
      "103/103 [==============================] - 0s 569us/step - loss: 0.0146 - val_loss: 1.2932\n",
      "Epoch 1632/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0136 - val_loss: 1.3953\n",
      "Epoch 1633/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0140 - val_loss: 1.2874\n",
      "Epoch 1634/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0146 - val_loss: 1.4454\n",
      "Epoch 1635/2000\n",
      "103/103 [==============================] - 0s 924us/step - loss: 0.0142 - val_loss: 1.2896\n",
      "Epoch 1636/2000\n",
      "103/103 [==============================] - 0s 647us/step - loss: 0.0143 - val_loss: 1.4036\n",
      "Epoch 1637/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 1.2757\n",
      "Epoch 1638/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 1.4019\n",
      "Epoch 1639/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0136 - val_loss: 1.2815\n",
      "Epoch 1640/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0143 - val_loss: 1.4842\n",
      "Epoch 1641/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 1.2712\n",
      "Epoch 1642/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0142 - val_loss: 1.4163\n",
      "Epoch 1643/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 1.2814\n",
      "Epoch 1644/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 1.4664\n",
      "Epoch 1645/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 1.2753\n",
      "Epoch 1646/2000\n",
      "103/103 [==============================] - 0s 570us/step - loss: 0.0130 - val_loss: 1.4714\n",
      "Epoch 1647/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0141 - val_loss: 1.2972\n",
      "Epoch 1648/2000\n",
      "103/103 [==============================] - 0s 588us/step - loss: 0.0144 - val_loss: 1.4465\n",
      "Epoch 1649/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0146 - val_loss: 1.2791\n",
      "Epoch 1650/2000\n",
      "103/103 [==============================] - 0s 584us/step - loss: 0.0131 - val_loss: 1.4895\n",
      "Epoch 1651/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0150 - val_loss: 1.3078\n",
      "Epoch 1652/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0162 - val_loss: 1.4104\n",
      "Epoch 1653/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0150 - val_loss: 1.3194\n",
      "Epoch 1654/2000\n",
      "103/103 [==============================] - 0s 597us/step - loss: 0.0132 - val_loss: 1.3969\n",
      "Epoch 1655/2000\n",
      "103/103 [==============================] - 0s 572us/step - loss: 0.0130 - val_loss: 1.3082\n",
      "Epoch 1656/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0141 - val_loss: 1.4576\n",
      "Epoch 1657/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 1.2674\n",
      "Epoch 1658/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 1.4218\n",
      "Epoch 1659/2000\n",
      "103/103 [==============================] - 0s 900us/step - loss: 0.0135 - val_loss: 1.2878\n",
      "Epoch 1660/2000\n",
      "103/103 [==============================] - 0s 612us/step - loss: 0.0136 - val_loss: 1.4167\n",
      "Epoch 1661/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0143 - val_loss: 1.2281\n",
      "Epoch 1662/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0144 - val_loss: 1.4132\n",
      "Epoch 1663/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0148 - val_loss: 1.2371\n",
      "Epoch 1664/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0147 - val_loss: 1.4035\n",
      "Epoch 1665/2000\n",
      "103/103 [==============================] - 0s 589us/step - loss: 0.0142 - val_loss: 1.2609\n",
      "Epoch 1666/2000\n",
      "103/103 [==============================] - 0s 619us/step - loss: 0.0140 - val_loss: 1.4410\n",
      "Epoch 1667/2000\n",
      "103/103 [==============================] - 0s 669us/step - loss: 0.0138 - val_loss: 1.2759\n",
      "Epoch 1668/2000\n",
      "103/103 [==============================] - 0s 831us/step - loss: 0.0134 - val_loss: 1.4449\n",
      "Epoch 1669/2000\n",
      "103/103 [==============================] - 0s 635us/step - loss: 0.0142 - val_loss: 1.2830\n",
      "Epoch 1670/2000\n",
      "103/103 [==============================] - 0s 605us/step - loss: 0.0137 - val_loss: 1.3817\n",
      "Epoch 1671/2000\n",
      "103/103 [==============================] - 0s 628us/step - loss: 0.0134 - val_loss: 1.2832\n",
      "Epoch 1672/2000\n",
      "103/103 [==============================] - 0s 596us/step - loss: 0.0128 - val_loss: 1.3987\n",
      "Epoch 1673/2000\n",
      "103/103 [==============================] - 0s 601us/step - loss: 0.0129 - val_loss: 1.2511\n",
      "Epoch 1674/2000\n",
      "103/103 [==============================] - 0s 608us/step - loss: 0.0147 - val_loss: 1.4072\n",
      "Epoch 1675/2000\n",
      "103/103 [==============================] - 0s 641us/step - loss: 0.0154 - val_loss: 1.2071\n",
      "Epoch 1676/2000\n",
      "103/103 [==============================] - 0s 819us/step - loss: 0.0137 - val_loss: 1.3827\n",
      "Epoch 1677/2000\n",
      "103/103 [==============================] - 0s 609us/step - loss: 0.0133 - val_loss: 1.2372\n",
      "Epoch 1678/2000\n",
      "103/103 [==============================] - 0s 599us/step - loss: 0.0129 - val_loss: 1.4451\n",
      "Epoch 1679/2000\n",
      "103/103 [==============================] - 0s 581us/step - loss: 0.0126 - val_loss: 1.2225\n",
      "Epoch 1680/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0138 - val_loss: 1.4181\n",
      "Epoch 1681/2000\n",
      "103/103 [==============================] - 0s 592us/step - loss: 0.0149 - val_loss: 1.2364\n",
      "Epoch 1682/2000\n",
      "103/103 [==============================] - 0s 605us/step - loss: 0.0136 - val_loss: 1.3601\n",
      "Epoch 1683/2000\n",
      "103/103 [==============================] - 0s 591us/step - loss: 0.0140 - val_loss: 1.2499\n",
      "Epoch 1684/2000\n",
      "103/103 [==============================] - 0s 803us/step - loss: 0.0137 - val_loss: 1.3897\n",
      "Epoch 1685/2000\n",
      "103/103 [==============================] - 0s 779us/step - loss: 0.0134 - val_loss: 1.2906\n",
      "Epoch 1686/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 1.3834\n",
      "Epoch 1687/2000\n",
      "103/103 [==============================] - 0s 777us/step - loss: 0.0129 - val_loss: 1.2249\n",
      "Epoch 1688/2000\n",
      "103/103 [==============================] - 0s 826us/step - loss: 0.0126 - val_loss: 1.3757\n",
      "Epoch 1689/2000\n",
      "103/103 [==============================] - 0s 585us/step - loss: 0.0143 - val_loss: 1.2183\n",
      "Epoch 1690/2000\n",
      "103/103 [==============================] - 0s 590us/step - loss: 0.0134 - val_loss: 1.3806\n",
      "Epoch 1691/2000\n",
      "103/103 [==============================] - 0s 592us/step - loss: 0.0144 - val_loss: 1.2597\n",
      "Epoch 1692/2000\n",
      "103/103 [==============================] - 0s 639us/step - loss: 0.0142 - val_loss: 1.3640\n",
      "Epoch 1693/2000\n",
      "103/103 [==============================] - 0s 653us/step - loss: 0.0137 - val_loss: 1.1794\n",
      "Epoch 1694/2000\n",
      "103/103 [==============================] - 0s 604us/step - loss: 0.0133 - val_loss: 1.3876\n",
      "Epoch 1695/2000\n",
      "103/103 [==============================] - 0s 628us/step - loss: 0.0125 - val_loss: 1.2088\n",
      "Epoch 1696/2000\n",
      "103/103 [==============================] - 0s 870us/step - loss: 0.0129 - val_loss: 1.3966\n",
      "Epoch 1697/2000\n",
      "103/103 [==============================] - 0s 611us/step - loss: 0.0128 - val_loss: 1.2221\n",
      "Epoch 1698/2000\n",
      "103/103 [==============================] - 0s 603us/step - loss: 0.0132 - val_loss: 1.3225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1699/2000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0129 - val_loss: 1.2360\n",
      "Epoch 1700/2000\n",
      "103/103 [==============================] - 0s 583us/step - loss: 0.0142 - val_loss: 1.3662\n",
      "Epoch 1701/2000\n",
      "103/103 [==============================] - 0s 610us/step - loss: 0.0136 - val_loss: 1.2354\n",
      "Epoch 1702/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0126 - val_loss: 1.3824\n",
      "Epoch 1703/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0130 - val_loss: 1.2340\n",
      "Epoch 1704/2000\n",
      "103/103 [==============================] - 0s 525us/step - loss: 0.0139 - val_loss: 1.3720\n",
      "Epoch 1705/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0139 - val_loss: 1.1722\n",
      "Epoch 1706/2000\n",
      "103/103 [==============================] - 0s 570us/step - loss: 0.0132 - val_loss: 1.4128\n",
      "Epoch 1707/2000\n",
      "103/103 [==============================] - 0s 585us/step - loss: 0.0123 - val_loss: 1.2194\n",
      "Epoch 1708/2000\n",
      "103/103 [==============================] - 0s 603us/step - loss: 0.0130 - val_loss: 1.3617\n",
      "Epoch 1709/2000\n",
      "103/103 [==============================] - 0s 578us/step - loss: 0.0127 - val_loss: 1.2533\n",
      "Epoch 1710/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0132 - val_loss: 1.3382\n",
      "Epoch 1711/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0147 - val_loss: 1.2192\n",
      "Epoch 1712/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0128 - val_loss: 1.3582\n",
      "Epoch 1713/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0120 - val_loss: 1.2829\n",
      "Epoch 1714/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0118 - val_loss: 1.3535\n",
      "Epoch 1715/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0127 - val_loss: 1.1647\n",
      "Epoch 1716/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0127 - val_loss: 1.4090\n",
      "Epoch 1717/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0140 - val_loss: 1.1412\n",
      "Epoch 1718/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0139 - val_loss: 1.3310\n",
      "Epoch 1719/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0134 - val_loss: 1.2143\n",
      "Epoch 1720/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0127 - val_loss: 1.3283\n",
      "Epoch 1721/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0119 - val_loss: 1.1735\n",
      "Epoch 1722/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0120 - val_loss: 1.3711\n",
      "Epoch 1723/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0130 - val_loss: 1.1926\n",
      "Epoch 1724/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0124 - val_loss: 1.3683\n",
      "Epoch 1725/2000\n",
      "103/103 [==============================] - 0s 530us/step - loss: 0.0124 - val_loss: 1.2174\n",
      "Epoch 1726/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0125 - val_loss: 1.3208\n",
      "Epoch 1727/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0125 - val_loss: 1.1734\n",
      "Epoch 1728/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0125 - val_loss: 1.4025\n",
      "Epoch 1729/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0123 - val_loss: 1.1734\n",
      "Epoch 1730/2000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0126 - val_loss: 1.3388\n",
      "Epoch 1731/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0137 - val_loss: 1.2251\n",
      "Epoch 1732/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0124 - val_loss: 1.2701\n",
      "Epoch 1733/2000\n",
      "103/103 [==============================] - 0s 627us/step - loss: 0.0131 - val_loss: 1.1717\n",
      "Epoch 1734/2000\n",
      "103/103 [==============================] - 0s 756us/step - loss: 0.0132 - val_loss: 1.2957\n",
      "Epoch 1735/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0118 - val_loss: 1.2413\n",
      "Epoch 1736/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0106 - val_loss: 1.2553\n",
      "Epoch 1737/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0110 - val_loss: 1.2695\n",
      "Epoch 1738/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0120 - val_loss: 1.2147\n",
      "Epoch 1739/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0113 - val_loss: 1.2300\n",
      "Epoch 1740/2000\n",
      "103/103 [==============================] - 0s 595us/step - loss: 0.0106 - val_loss: 1.2006\n",
      "Epoch 1741/2000\n",
      "103/103 [==============================] - 0s 585us/step - loss: 0.0122 - val_loss: 1.3151\n",
      "Epoch 1742/2000\n",
      "103/103 [==============================] - 0s 784us/step - loss: 0.0142 - val_loss: 1.0332\n",
      "Epoch 1743/2000\n",
      "103/103 [==============================] - 0s 582us/step - loss: 0.0139 - val_loss: 1.2727\n",
      "Epoch 1744/2000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0125 - val_loss: 1.1738\n",
      "Epoch 1745/2000\n",
      "103/103 [==============================] - 0s 605us/step - loss: 0.0113 - val_loss: 1.2531\n",
      "Epoch 1746/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0111 - val_loss: 1.1516\n",
      "Epoch 1747/2000\n",
      "103/103 [==============================] - 0s 605us/step - loss: 0.0116 - val_loss: 1.3253\n",
      "Epoch 1748/2000\n",
      "103/103 [==============================] - 0s 617us/step - loss: 0.0128 - val_loss: 1.1148\n",
      "Epoch 1749/2000\n",
      "103/103 [==============================] - 0s 611us/step - loss: 0.0124 - val_loss: 1.2602\n",
      "Epoch 1750/2000\n",
      "103/103 [==============================] - 0s 675us/step - loss: 0.0122 - val_loss: 1.1129\n",
      "Epoch 1751/2000\n",
      "103/103 [==============================] - 0s 731us/step - loss: 0.0113 - val_loss: 1.2665\n",
      "Epoch 1752/2000\n",
      "103/103 [==============================] - 0s 616us/step - loss: 0.0109 - val_loss: 1.0808\n",
      "Epoch 1753/2000\n",
      "103/103 [==============================] - 0s 616us/step - loss: 0.0106 - val_loss: 1.2948\n",
      "Epoch 1754/2000\n",
      "103/103 [==============================] - 0s 573us/step - loss: 0.0121 - val_loss: 1.1143\n",
      "Epoch 1755/2000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0127 - val_loss: 1.2450\n",
      "Epoch 1756/2000\n",
      "103/103 [==============================] - 0s 594us/step - loss: 0.0119 - val_loss: 1.1015\n",
      "Epoch 1757/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0110 - val_loss: 1.2783\n",
      "Epoch 1758/2000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0108 - val_loss: 1.0766\n",
      "Epoch 1759/2000\n",
      "103/103 [==============================] - 0s 738us/step - loss: 0.0125 - val_loss: 1.2643\n",
      "Epoch 1760/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0134 - val_loss: 1.0935\n",
      "Epoch 1761/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0109 - val_loss: 1.2528\n",
      "Epoch 1762/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0108 - val_loss: 1.0871\n",
      "Epoch 1763/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0108 - val_loss: 1.2865\n",
      "Epoch 1764/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 1.1255\n",
      "Epoch 1765/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 1.2213\n",
      "Epoch 1766/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 1.0745\n",
      "Epoch 1767/2000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0107 - val_loss: 1.2709\n",
      "Epoch 1768/2000\n",
      "103/103 [==============================] - 0s 991us/step - loss: 0.0113 - val_loss: 1.0585\n",
      "Epoch 1769/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0119 - val_loss: 1.2496\n",
      "Epoch 1770/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0108 - val_loss: 1.1076\n",
      "Epoch 1771/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0104 - val_loss: 1.2378\n",
      "Epoch 1772/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0122 - val_loss: 1.0373\n",
      "Epoch 1773/2000\n",
      "103/103 [==============================] - 0s 915us/step - loss: 0.0120 - val_loss: 1.2510\n",
      "Epoch 1774/2000\n",
      "103/103 [==============================] - 0s 764us/step - loss: 0.0117 - val_loss: 1.1101\n",
      "Epoch 1775/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 564us/step - loss: 0.0117 - val_loss: 1.2103\n",
      "Epoch 1776/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0114 - val_loss: 1.0622\n",
      "Epoch 1777/2000\n",
      "103/103 [==============================] - 0s 574us/step - loss: 0.0103 - val_loss: 1.2663\n",
      "Epoch 1778/2000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0098 - val_loss: 1.0477\n",
      "Epoch 1779/2000\n",
      "103/103 [==============================] - 0s 776us/step - loss: 0.0107 - val_loss: 1.2367\n",
      "Epoch 1780/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0115 - val_loss: 1.1120\n",
      "Epoch 1781/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0106 - val_loss: 1.2413\n",
      "Epoch 1782/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0111 - val_loss: 1.0044\n",
      "Epoch 1783/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0111 - val_loss: 1.2460\n",
      "Epoch 1784/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0116 - val_loss: 1.0901\n",
      "Epoch 1785/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0111 - val_loss: 1.1794\n",
      "Epoch 1786/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0108 - val_loss: 1.0666\n",
      "Epoch 1787/2000\n",
      "103/103 [==============================] - 0s 564us/step - loss: 0.0117 - val_loss: 1.2492\n",
      "Epoch 1788/2000\n",
      "103/103 [==============================] - 0s 642us/step - loss: 0.0101 - val_loss: 1.0951\n",
      "Epoch 1789/2000\n",
      "103/103 [==============================] - 0s 628us/step - loss: 0.0103 - val_loss: 1.1606\n",
      "Epoch 1790/2000\n",
      "103/103 [==============================] - 0s 761us/step - loss: 0.0101 - val_loss: 1.1459\n",
      "Epoch 1791/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0090 - val_loss: 1.2017\n",
      "Epoch 1792/2000\n",
      "103/103 [==============================] - 0s 641us/step - loss: 0.0103 - val_loss: 0.9996\n",
      "Epoch 1793/2000\n",
      "103/103 [==============================] - 0s 770us/step - loss: 0.0123 - val_loss: 1.2309\n",
      "Epoch 1794/2000\n",
      "103/103 [==============================] - 0s 728us/step - loss: 0.0121 - val_loss: 1.0036\n",
      "Epoch 1795/2000\n",
      "103/103 [==============================] - 0s 661us/step - loss: 0.0115 - val_loss: 1.1804\n",
      "Epoch 1796/2000\n",
      "103/103 [==============================] - 0s 605us/step - loss: 0.0111 - val_loss: 1.0586\n",
      "Epoch 1797/2000\n",
      "103/103 [==============================] - 0s 596us/step - loss: 0.0101 - val_loss: 1.1965\n",
      "Epoch 1798/2000\n",
      "103/103 [==============================] - 0s 595us/step - loss: 0.0094 - val_loss: 1.0233\n",
      "Epoch 1799/2000\n",
      "103/103 [==============================] - 0s 623us/step - loss: 0.0089 - val_loss: 1.2081\n",
      "Epoch 1800/2000\n",
      "103/103 [==============================] - 0s 612us/step - loss: 0.0098 - val_loss: 1.0237\n",
      "Epoch 1801/2000\n",
      "103/103 [==============================] - 0s 633us/step - loss: 0.0118 - val_loss: 1.1879\n",
      "Epoch 1802/2000\n",
      "103/103 [==============================] - 0s 619us/step - loss: 0.0107 - val_loss: 1.0450\n",
      "Epoch 1803/2000\n",
      "103/103 [==============================] - 0s 646us/step - loss: 0.0102 - val_loss: 1.2063\n",
      "Epoch 1804/2000\n",
      "103/103 [==============================] - 0s 630us/step - loss: 0.0104 - val_loss: 0.9817\n",
      "Epoch 1805/2000\n",
      "103/103 [==============================] - 0s 608us/step - loss: 0.0104 - val_loss: 1.1883\n",
      "Epoch 1806/2000\n",
      "103/103 [==============================] - 0s 642us/step - loss: 0.0106 - val_loss: 1.0303\n",
      "Epoch 1807/2000\n",
      "103/103 [==============================] - 0s 758us/step - loss: 0.0102 - val_loss: 1.1706\n",
      "Epoch 1808/2000\n",
      "103/103 [==============================] - 0s 975us/step - loss: 0.0109 - val_loss: 0.9976\n",
      "Epoch 1809/2000\n",
      "103/103 [==============================] - 0s 815us/step - loss: 0.0096 - val_loss: 1.1834\n",
      "Epoch 1810/2000\n",
      "103/103 [==============================] - 0s 862us/step - loss: 0.0088 - val_loss: 1.0299\n",
      "Epoch 1811/2000\n",
      "103/103 [==============================] - 0s 595us/step - loss: 0.0095 - val_loss: 1.1610\n",
      "Epoch 1812/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0116 - val_loss: 0.9874\n",
      "Epoch 1813/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0104 - val_loss: 1.1764\n",
      "Epoch 1814/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0095 - val_loss: 1.0181\n",
      "Epoch 1815/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0094 - val_loss: 1.1688\n",
      "Epoch 1816/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0112 - val_loss: 0.9496\n",
      "Epoch 1817/2000\n",
      "103/103 [==============================] - 0s 663us/step - loss: 0.0100 - val_loss: 1.1771\n",
      "Epoch 1818/2000\n",
      "103/103 [==============================] - 0s 749us/step - loss: 0.0095 - val_loss: 1.0392\n",
      "Epoch 1819/2000\n",
      "103/103 [==============================] - 0s 652us/step - loss: 0.0095 - val_loss: 1.1432\n",
      "Epoch 1820/2000\n",
      "103/103 [==============================] - 0s 569us/step - loss: 0.0093 - val_loss: 0.9794\n",
      "Epoch 1821/2000\n",
      "103/103 [==============================] - 0s 876us/step - loss: 0.0096 - val_loss: 1.1820\n",
      "Epoch 1822/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0089 - val_loss: 0.9954\n",
      "Epoch 1823/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0096 - val_loss: 1.1480\n",
      "Epoch 1824/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0095 - val_loss: 0.9643\n",
      "Epoch 1825/2000\n",
      "103/103 [==============================] - 0s 642us/step - loss: 0.0095 - val_loss: 1.1906\n",
      "Epoch 1826/2000\n",
      "103/103 [==============================] - 0s 753us/step - loss: 0.0101 - val_loss: 0.9863\n",
      "Epoch 1827/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0086 - val_loss: 1.1410\n",
      "Epoch 1828/2000\n",
      "103/103 [==============================] - 0s 580us/step - loss: 0.0091 - val_loss: 1.0354\n",
      "Epoch 1829/2000\n",
      "103/103 [==============================] - 0s 607us/step - loss: 0.0095 - val_loss: 1.1769\n",
      "Epoch 1830/2000\n",
      "103/103 [==============================] - 0s 588us/step - loss: 0.0084 - val_loss: 0.9372\n",
      "Epoch 1831/2000\n",
      "103/103 [==============================] - 0s 603us/step - loss: 0.0093 - val_loss: 1.1598\n",
      "Epoch 1832/2000\n",
      "103/103 [==============================] - 0s 608us/step - loss: 0.0089 - val_loss: 1.0108\n",
      "Epoch 1833/2000\n",
      "103/103 [==============================] - 0s 652us/step - loss: 0.0085 - val_loss: 1.1500\n",
      "Epoch 1834/2000\n",
      "103/103 [==============================] - 0s 725us/step - loss: 0.0096 - val_loss: 0.9438\n",
      "Epoch 1835/2000\n",
      "103/103 [==============================] - 0s 611us/step - loss: 0.0093 - val_loss: 1.1551\n",
      "Epoch 1836/2000\n",
      "103/103 [==============================] - 0s 597us/step - loss: 0.0084 - val_loss: 1.0070\n",
      "Epoch 1837/2000\n",
      "103/103 [==============================] - 0s 638us/step - loss: 0.0082 - val_loss: 1.1289\n",
      "Epoch 1838/2000\n",
      "103/103 [==============================] - 0s 645us/step - loss: 0.0093 - val_loss: 0.9602\n",
      "Epoch 1839/2000\n",
      "103/103 [==============================] - 0s 616us/step - loss: 0.0101 - val_loss: 1.1750\n",
      "Epoch 1840/2000\n",
      "103/103 [==============================] - 0s 624us/step - loss: 0.0088 - val_loss: 1.0271\n",
      "Epoch 1841/2000\n",
      "103/103 [==============================] - 0s 650us/step - loss: 0.0084 - val_loss: 1.1237\n",
      "Epoch 1842/2000\n",
      "103/103 [==============================] - 0s 763us/step - loss: 0.0074 - val_loss: 0.9719\n",
      "Epoch 1843/2000\n",
      "103/103 [==============================] - 0s 616us/step - loss: 0.0084 - val_loss: 1.1926\n",
      "Epoch 1844/2000\n",
      "103/103 [==============================] - 0s 606us/step - loss: 0.0099 - val_loss: 0.9344\n",
      "Epoch 1845/2000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0091 - val_loss: 1.1246\n",
      "Epoch 1846/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 1.0099\n",
      "Epoch 1847/2000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 1.1301\n",
      "Epoch 1848/2000\n",
      "103/103 [==============================] - 0s 594us/step - loss: 0.0094 - val_loss: 0.9372\n",
      "Epoch 1849/2000\n",
      "103/103 [==============================] - 0s 619us/step - loss: 0.0090 - val_loss: 1.1432\n",
      "Epoch 1850/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0070 - val_loss: 1.0243\n",
      "Epoch 1851/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 1.1235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1852/2000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.9359\n",
      "Epoch 1853/2000\n",
      "103/103 [==============================] - 0s 594us/step - loss: 0.0087 - val_loss: 1.1627\n",
      "Epoch 1854/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0087 - val_loss: 0.9749\n",
      "Epoch 1855/2000\n",
      "103/103 [==============================] - 0s 592us/step - loss: 0.0081 - val_loss: 1.1103\n",
      "Epoch 1856/2000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0084 - val_loss: 0.9609\n",
      "Epoch 1857/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0076 - val_loss: 1.1284\n",
      "Epoch 1858/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0090 - val_loss: 0.9680\n",
      "Epoch 1859/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0076 - val_loss: 1.1172\n",
      "Epoch 1860/2000\n",
      "103/103 [==============================] - 0s 526us/step - loss: 0.0063 - val_loss: 1.0420\n",
      "Epoch 1861/2000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0069 - val_loss: 1.1095\n",
      "Epoch 1862/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0095 - val_loss: 0.9189\n",
      "Epoch 1863/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0096 - val_loss: 1.1604\n",
      "Epoch 1864/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0074 - val_loss: 0.9701\n",
      "Epoch 1865/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0075 - val_loss: 1.1061\n",
      "Epoch 1866/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0072 - val_loss: 0.9587\n",
      "Epoch 1867/2000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0077 - val_loss: 1.1304\n",
      "Epoch 1868/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0075 - val_loss: 0.9018\n",
      "Epoch 1869/2000\n",
      "103/103 [==============================] - 0s 587us/step - loss: 0.0064 - val_loss: 1.1264\n",
      "Epoch 1870/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0079 - val_loss: 0.9772\n",
      "Epoch 1871/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0086 - val_loss: 1.1058\n",
      "Epoch 1872/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0082 - val_loss: 0.9064\n",
      "Epoch 1873/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0089 - val_loss: 1.1218\n",
      "Epoch 1874/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0061 - val_loss: 0.9872\n",
      "Epoch 1875/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0071 - val_loss: 1.0829\n",
      "Epoch 1876/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0068 - val_loss: 0.9444\n",
      "Epoch 1877/2000\n",
      "103/103 [==============================] - 0s 524us/step - loss: 0.0073 - val_loss: 1.1592\n",
      "Epoch 1878/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0084 - val_loss: 0.9054\n",
      "Epoch 1879/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0077 - val_loss: 1.0889\n",
      "Epoch 1880/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0067 - val_loss: 0.9295\n",
      "Epoch 1881/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0066 - val_loss: 1.0816\n",
      "Epoch 1882/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0082 - val_loss: 0.9122\n",
      "Epoch 1883/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0074 - val_loss: 1.0960\n",
      "Epoch 1884/2000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0073 - val_loss: 0.9673\n",
      "Epoch 1885/2000\n",
      "103/103 [==============================] - 0s 679us/step - loss: 0.0068 - val_loss: 1.0779\n",
      "Epoch 1886/2000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0075 - val_loss: 0.9134\n",
      "Epoch 1887/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0073 - val_loss: 1.1335\n",
      "Epoch 1888/2000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0066 - val_loss: 0.9178\n",
      "Epoch 1889/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0077 - val_loss: 1.0646\n",
      "Epoch 1890/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0075 - val_loss: 0.9198\n",
      "Epoch 1891/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0066 - val_loss: 1.0828\n",
      "Epoch 1892/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0071 - val_loss: 0.9293\n",
      "Epoch 1893/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0064 - val_loss: 1.0551\n",
      "Epoch 1894/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0054 - val_loss: 0.9794\n",
      "Epoch 1895/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0059 - val_loss: 1.0669\n",
      "Epoch 1896/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0084 - val_loss: 0.8811\n",
      "Epoch 1897/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0084 - val_loss: 1.0916\n",
      "Epoch 1898/2000\n",
      "103/103 [==============================] - 0s 557us/step - loss: 0.0080 - val_loss: 0.9424\n",
      "Epoch 1899/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0063 - val_loss: 1.0485\n",
      "Epoch 1900/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0061 - val_loss: 0.9506\n",
      "Epoch 1901/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0059 - val_loss: 1.0981\n",
      "Epoch 1902/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0058 - val_loss: 0.8898\n",
      "Epoch 1903/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0076 - val_loss: 1.0766\n",
      "Epoch 1904/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0088 - val_loss: 0.9535\n",
      "Epoch 1905/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0067 - val_loss: 1.1137\n",
      "Epoch 1906/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0056 - val_loss: 0.8931\n",
      "Epoch 1907/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0053 - val_loss: 1.1006\n",
      "Epoch 1908/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0073 - val_loss: 0.9766\n",
      "Epoch 1909/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0074 - val_loss: 1.1129\n",
      "Epoch 1910/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0071 - val_loss: 0.8901\n",
      "Epoch 1911/2000\n",
      "103/103 [==============================] - 0s 569us/step - loss: 0.0062 - val_loss: 1.1176\n",
      "Epoch 1912/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0058 - val_loss: 0.9617\n",
      "Epoch 1913/2000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0072 - val_loss: 1.0749\n",
      "Epoch 1914/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0070 - val_loss: 0.9268\n",
      "Epoch 1915/2000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0059 - val_loss: 1.1046\n",
      "Epoch 1916/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0054 - val_loss: 0.9193\n",
      "Epoch 1917/2000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0058 - val_loss: 1.0863\n",
      "Epoch 1918/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0064 - val_loss: 0.9481\n",
      "Epoch 1919/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0066 - val_loss: 1.0822\n",
      "Epoch 1920/2000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0079 - val_loss: 0.9130\n",
      "Epoch 1921/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0060 - val_loss: 1.1022\n",
      "Epoch 1922/2000\n",
      "103/103 [==============================] - 0s 530us/step - loss: 0.0054 - val_loss: 0.9789\n",
      "Epoch 1923/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0062 - val_loss: 1.0683\n",
      "Epoch 1924/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0064 - val_loss: 0.9495\n",
      "Epoch 1925/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0062 - val_loss: 1.1556\n",
      "Epoch 1926/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0057 - val_loss: 0.9482\n",
      "Epoch 1927/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0067 - val_loss: 1.0852\n",
      "Epoch 1928/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 535us/step - loss: 0.0062 - val_loss: 0.9297\n",
      "Epoch 1929/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0061 - val_loss: 1.1400\n",
      "Epoch 1930/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0060 - val_loss: 0.9566\n",
      "Epoch 1931/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0053 - val_loss: 1.1068\n",
      "Epoch 1932/2000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0061 - val_loss: 0.9780\n",
      "Epoch 1933/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0062 - val_loss: 1.1142\n",
      "Epoch 1934/2000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0068 - val_loss: 0.9204\n",
      "Epoch 1935/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0061 - val_loss: 1.1560\n",
      "Epoch 1936/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0048 - val_loss: 0.9437\n",
      "Epoch 1937/2000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0063 - val_loss: 1.0778\n",
      "Epoch 1938/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0081 - val_loss: 0.9825\n",
      "Epoch 1939/2000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0064 - val_loss: 1.1528\n",
      "Epoch 1940/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0048 - val_loss: 0.9482\n",
      "Epoch 1941/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0045 - val_loss: 1.1048\n",
      "Epoch 1942/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0065 - val_loss: 1.0148\n",
      "Epoch 1943/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0070 - val_loss: 1.1603\n",
      "Epoch 1944/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0064 - val_loss: 0.9181\n",
      "Epoch 1945/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0051 - val_loss: 1.1362\n",
      "Epoch 1946/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0066 - val_loss: 1.0117\n",
      "Epoch 1947/2000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0058 - val_loss: 1.1288\n",
      "Epoch 1948/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0056 - val_loss: 0.9547\n",
      "Epoch 1949/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0057 - val_loss: 1.1559\n",
      "Epoch 1950/2000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0049 - val_loss: 0.9780\n",
      "Epoch 1951/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0071 - val_loss: 1.0790\n",
      "Epoch 1952/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0073 - val_loss: 1.0037\n",
      "Epoch 1953/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0052 - val_loss: 1.1533\n",
      "Epoch 1954/2000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0044 - val_loss: 0.9893\n",
      "Epoch 1955/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0055 - val_loss: 1.0810\n",
      "Epoch 1956/2000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0055 - val_loss: 1.0350\n",
      "Epoch 1957/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0058 - val_loss: 1.1204\n",
      "Epoch 1958/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0067 - val_loss: 0.9673\n",
      "Epoch 1959/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0057 - val_loss: 1.1410\n",
      "Epoch 1960/2000\n",
      "103/103 [==============================] - 0s 545us/step - loss: 0.0057 - val_loss: 0.9828\n",
      "Epoch 1961/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0064 - val_loss: 1.1168\n",
      "Epoch 1962/2000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0064 - val_loss: 0.9753\n",
      "Epoch 1963/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0052 - val_loss: 1.1603\n",
      "Epoch 1964/2000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0046 - val_loss: 0.9692\n",
      "Epoch 1965/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0059 - val_loss: 1.1174\n",
      "Epoch 1966/2000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0062 - val_loss: 0.9690\n",
      "Epoch 1967/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0058 - val_loss: 1.1433\n",
      "Epoch 1968/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0056 - val_loss: 0.9562\n",
      "Epoch 1969/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0044 - val_loss: 1.1430\n",
      "Epoch 1970/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0058 - val_loss: 1.0193\n",
      "Epoch 1971/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0054 - val_loss: 1.1240\n",
      "Epoch 1972/2000\n",
      "103/103 [==============================] - 0s 597us/step - loss: 0.0056 - val_loss: 0.9375\n",
      "Epoch 1973/2000\n",
      "103/103 [==============================] - 0s 562us/step - loss: 0.0054 - val_loss: 1.1550\n",
      "Epoch 1974/2000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0050 - val_loss: 0.9812\n",
      "Epoch 1975/2000\n",
      "103/103 [==============================] - 0s 532us/step - loss: 0.0057 - val_loss: 1.0925\n",
      "Epoch 1976/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0051 - val_loss: 0.9685\n",
      "Epoch 1977/2000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0064 - val_loss: 1.1612\n",
      "Epoch 1978/2000\n",
      "103/103 [==============================] - 0s 528us/step - loss: 0.0063 - val_loss: 0.9644\n",
      "Epoch 1979/2000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0048 - val_loss: 1.1182\n",
      "Epoch 1980/2000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0059 - val_loss: 0.9971\n",
      "Epoch 1981/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0051 - val_loss: 1.1302\n",
      "Epoch 1982/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0051 - val_loss: 0.9250\n",
      "Epoch 1983/2000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0054 - val_loss: 1.1432\n",
      "Epoch 1984/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0061 - val_loss: 1.0084\n",
      "Epoch 1985/2000\n",
      "103/103 [==============================] - 0s 546us/step - loss: 0.0048 - val_loss: 1.1127\n",
      "Epoch 1986/2000\n",
      "103/103 [==============================] - 0s 536us/step - loss: 0.0054 - val_loss: 0.9670\n",
      "Epoch 1987/2000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0054 - val_loss: 1.1378\n",
      "Epoch 1988/2000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0054 - val_loss: 0.9955\n",
      "Epoch 1989/2000\n",
      "103/103 [==============================] - 0s 572us/step - loss: 0.0062 - val_loss: 1.1002\n",
      "Epoch 1990/2000\n",
      "103/103 [==============================] - 0s 842us/step - loss: 0.0050 - val_loss: 0.9887\n",
      "Epoch 1991/2000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0043 - val_loss: 1.1358\n",
      "Epoch 1992/2000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0042 - val_loss: 0.9765\n",
      "Epoch 1993/2000\n",
      "103/103 [==============================] - 0s 542us/step - loss: 0.0057 - val_loss: 1.0850\n",
      "Epoch 1994/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0062 - val_loss: 1.0358\n",
      "Epoch 1995/2000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0047 - val_loss: 1.0941\n",
      "Epoch 1996/2000\n",
      "103/103 [==============================] - 0s 541us/step - loss: 0.0047 - val_loss: 0.9498\n",
      "Epoch 1997/2000\n",
      "103/103 [==============================] - 0s 553us/step - loss: 0.0045 - val_loss: 1.1409\n",
      "Epoch 1998/2000\n",
      "103/103 [==============================] - 0s 549us/step - loss: 0.0054 - val_loss: 0.9546\n",
      "Epoch 1999/2000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0057 - val_loss: 1.0877\n",
      "Epoch 2000/2000\n",
      "103/103 [==============================] - 0s 548us/step - loss: 0.0064 - val_loss: 0.9619\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_inputs['X'],\n",
    "          train_inputs['target'],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(valid_inputs['X'], valid_inputs['target']),\n",
    "          callbacks=[earlystop ,best_val],\n",
    "          verbose=1 , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(np.array(history.history['val_loss']))+1\n",
    "model.load_weights(str(sat_var) +'_' +  var_name + '_{:02d}.h5'.format(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1187\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0911\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2057\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0426\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0388\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0350\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0343\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0338\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0367\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0405\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0538\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0550\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0678\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0466\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0465\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0372\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0373\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0346\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0367\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0364\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0418\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0417\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0497\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0440\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0488\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0401\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0420\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0366\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0387\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0359\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0393\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0374\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0423\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0394\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0443\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0391\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0426\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0373\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0400\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0360\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0390\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0360\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0396\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0367\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0408\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0371\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0408\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0365\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0397\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0357\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0387\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0352\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0385\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0353\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0388\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0354\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0389\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0353\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0386\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0349\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0379\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0344\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0375\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0342\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0374\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0342\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0374\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0341\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0372\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0338\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0369\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0336\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0365\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0333\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0363\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0332\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0362\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0330\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0360\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0329\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0358\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0327\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0355\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0325\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0353\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0323\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0351\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0321\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0349\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0320\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0347\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0318\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0345\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0316\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0343\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0341\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0313\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0339\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f733aad3208>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.fit(valid_inputs['X'],\n",
    "          valid_inputs['target'],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=100,\n",
    "          callbacks=[earlystop ,best_val],\n",
    "          verbose=1 , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"15\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"4\" halign=\"left\">e</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">OMEGA</th>\n",
       "      <th colspan=\"6\" halign=\"left\">omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>...</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-25 22:00:00</th>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>1.39860886580713073002</td>\n",
       "      <td>1.48803276544208196164</td>\n",
       "      <td>1.43731065555048820315</td>\n",
       "      <td>1.48695822226151652679</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.82770481597636758053</td>\n",
       "      <td>-1.82823045905599612659</td>\n",
       "      <td>-1.82876275685983169517</td>\n",
       "      <td>-1.82929449229750584749</td>\n",
       "      <td>-0.31144368107991765582</td>\n",
       "      <td>-0.40523282738911287071</td>\n",
       "      <td>-0.34739770242227863140</td>\n",
       "      <td>-0.27168534997949750354</td>\n",
       "      <td>-0.36199099883295593472</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 12:00:00</th>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>1.48803276544208196164</td>\n",
       "      <td>1.43731065555048820315</td>\n",
       "      <td>1.48695822226151652679</td>\n",
       "      <td>1.64713809870233096611</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.82823045905599612659</td>\n",
       "      <td>-1.82876275685983169517</td>\n",
       "      <td>-1.82929449229750584749</td>\n",
       "      <td>-2.93515827620206515292</td>\n",
       "      <td>-0.40523282738911287071</td>\n",
       "      <td>-0.34739770242227863140</td>\n",
       "      <td>-0.27168534997949750354</td>\n",
       "      <td>-0.36199099883295593472</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 14:00:00</th>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>1.43731065555048820315</td>\n",
       "      <td>1.48695822226151652679</td>\n",
       "      <td>1.64713809870233096611</td>\n",
       "      <td>1.53003103651788108230</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.82876275685983169517</td>\n",
       "      <td>-1.82929449229750584749</td>\n",
       "      <td>-2.93515827620206515292</td>\n",
       "      <td>-2.93569044011878377276</td>\n",
       "      <td>-0.34739770242227863140</td>\n",
       "      <td>-0.27168534997949750354</td>\n",
       "      <td>-0.36199099883295593472</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 16:00:00</th>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>1.48695822226151652679</td>\n",
       "      <td>1.64713809870233096611</td>\n",
       "      <td>1.53003103651788108230</td>\n",
       "      <td>1.58814746625392766433</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.82929449229750584749</td>\n",
       "      <td>-2.93515827620206515292</td>\n",
       "      <td>-2.93569044011878377276</td>\n",
       "      <td>-2.93622470622248954442</td>\n",
       "      <td>-0.27168534997949750354</td>\n",
       "      <td>-0.36199099883295593472</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 18:00:00</th>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>1.64713809870233096611</td>\n",
       "      <td>1.53003103651788108230</td>\n",
       "      <td>1.58814746625392766433</td>\n",
       "      <td>1.69594788624974257552</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.93515827620206515292</td>\n",
       "      <td>-2.93569044011878377276</td>\n",
       "      <td>-2.93622470622248954442</td>\n",
       "      <td>-2.93675819572051111095</td>\n",
       "      <td>-0.36199099883295593472</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                               target                          \\\n",
       "feature                                   y                           \n",
       "time step                               t+1                     t+2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -0.71488756279922882619 -0.76880470046970927900   \n",
       "2017-11-26 12:00:00 -0.76880470046970927900 -0.82350493069577734850   \n",
       "2017-11-26 14:00:00 -0.82350493069577734850 -0.76519237318943711390   \n",
       "2017-11-26 16:00:00 -0.76519237318943711390 -0.81408762043275528786   \n",
       "2017-11-26 18:00:00 -0.81408762043275528786 -1.10550894519194264909   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+3                     t+4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -0.82350493069577734850 -0.76519237318943711390   \n",
       "2017-11-26 12:00:00 -0.76519237318943711390 -0.81408762043275528786   \n",
       "2017-11-26 14:00:00 -0.81408762043275528786 -1.10550894519194264909   \n",
       "2017-11-26 16:00:00 -1.10550894519194264909 -1.58193698922062453427   \n",
       "2017-11-26 18:00:00 -1.58193698922062453427 -1.59611347777377043933   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+5                     t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -0.81408762043275528786 -1.10550894519194264909   \n",
       "2017-11-26 12:00:00 -1.10550894519194264909 -1.58193698922062453427   \n",
       "2017-11-26 14:00:00 -1.58193698922062453427 -1.59611347777377043933   \n",
       "2017-11-26 16:00:00 -1.59611347777377043933 -1.71266533515592245251   \n",
       "2017-11-26 18:00:00 -1.71266533515592245251 -1.71362020196184428045   \n",
       "\n",
       "tensor                                   X                         \\\n",
       "feature                                  e                          \n",
       "time step                              t-5                    t-4   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-25 22:00:00 1.39860886580713073002 1.48803276544208196164   \n",
       "2017-11-26 12:00:00 1.48803276544208196164 1.43731065555048820315   \n",
       "2017-11-26 14:00:00 1.43731065555048820315 1.48695822226151652679   \n",
       "2017-11-26 16:00:00 1.48695822226151652679 1.64713809870233096611   \n",
       "2017-11-26 18:00:00 1.64713809870233096611 1.53003103651788108230   \n",
       "\n",
       "tensor                                                             ...  \\\n",
       "feature                                                            ...   \n",
       "time step                              t-3                    t-2  ...   \n",
       "Epoch_Time_of_Clock                                                ...   \n",
       "2017-11-25 22:00:00 1.43731065555048820315 1.48695822226151652679  ...   \n",
       "2017-11-26 12:00:00 1.48695822226151652679 1.64713809870233096611  ...   \n",
       "2017-11-26 14:00:00 1.64713809870233096611 1.53003103651788108230  ...   \n",
       "2017-11-26 16:00:00 1.53003103651788108230 1.58814746625392766433  ...   \n",
       "2017-11-26 18:00:00 1.58814746625392766433 1.69594788624974257552  ...   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                               OMEGA                           \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -1.82770481597636758053 -1.82823045905599612659   \n",
       "2017-11-26 12:00:00 -1.82823045905599612659 -1.82876275685983169517   \n",
       "2017-11-26 14:00:00 -1.82876275685983169517 -1.82929449229750584749   \n",
       "2017-11-26 16:00:00 -1.82929449229750584749 -2.93515827620206515292   \n",
       "2017-11-26 18:00:00 -2.93515827620206515292 -2.93569044011878377276   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-1                       t   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -1.82876275685983169517 -1.82929449229750584749   \n",
       "2017-11-26 12:00:00 -1.82929449229750584749 -2.93515827620206515292   \n",
       "2017-11-26 14:00:00 -2.93515827620206515292 -2.93569044011878377276   \n",
       "2017-11-26 16:00:00 -2.93569044011878377276 -2.93622470622248954442   \n",
       "2017-11-26 18:00:00 -2.93622470622248954442 -2.93675819572051111095   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                               omega                           \n",
       "time step                               t-5                     t-4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -0.31144368107991765582 -0.40523282738911287071   \n",
       "2017-11-26 12:00:00 -0.40523282738911287071 -0.34739770242227863140   \n",
       "2017-11-26 14:00:00 -0.34739770242227863140 -0.27168534997949750354   \n",
       "2017-11-26 16:00:00 -0.27168534997949750354 -0.36199099883295593472   \n",
       "2017-11-26 18:00:00 -0.36199099883295593472 -0.57484294773798028100   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -0.34739770242227863140 -0.27168534997949750354   \n",
       "2017-11-26 12:00:00 -0.27168534997949750354 -0.36199099883295593472   \n",
       "2017-11-26 14:00:00 -0.36199099883295593472 -0.57484294773798028100   \n",
       "2017-11-26 16:00:00 -0.57484294773798028100 -0.71488756279922882619   \n",
       "2017-11-26 18:00:00 -0.71488756279922882619 -0.76880470046970927900   \n",
       "\n",
       "tensor                                                               \n",
       "feature                                                              \n",
       "time step                               t-1                       t  \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-25 22:00:00 -0.36199099883295593472 -0.57484294773798028100  \n",
       "2017-11-26 12:00:00 -0.57484294773798028100 -0.71488756279922882619  \n",
       "2017-11-26 14:00:00 -0.71488756279922882619 -0.76880470046970927900  \n",
       "2017-11-26 16:00:00 -0.76880470046970927900 -0.82350493069577734850  \n",
       "2017-11-26 18:00:00 -0.82350493069577734850 -0.76519237318943711390  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back_dt = dt.datetime.strptime(test_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1)\n",
    "test = df.copy()[test_start_dt:][['e', 'OMEGA', 'omega']]\n",
    "test[['e', 'OMEGA', 'omega']] = X_scaler.transform(test)\n",
    "test_inputs = TimeSeriesTensor(test, var_name, HORIZON, tensor_structure,freq =None)\n",
    "test_inputs.dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"15\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"4\" halign=\"left\">e</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">OMEGA</th>\n",
       "      <th colspan=\"6\" halign=\"left\">omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>...</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-25 22:00:00</th>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>1.39860886580713073002</td>\n",
       "      <td>1.48803276544208196164</td>\n",
       "      <td>1.43731065555048820315</td>\n",
       "      <td>1.48695822226151652679</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.82770481597636758053</td>\n",
       "      <td>-1.82823045905599612659</td>\n",
       "      <td>-1.82876275685983169517</td>\n",
       "      <td>-1.82929449229750584749</td>\n",
       "      <td>-0.31144368107991765582</td>\n",
       "      <td>-0.40523282738911287071</td>\n",
       "      <td>-0.34739770242227863140</td>\n",
       "      <td>-0.27168534997949750354</td>\n",
       "      <td>-0.36199099883295593472</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 12:00:00</th>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>1.48803276544208196164</td>\n",
       "      <td>1.43731065555048820315</td>\n",
       "      <td>1.48695822226151652679</td>\n",
       "      <td>1.64713809870233096611</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.82823045905599612659</td>\n",
       "      <td>-1.82876275685983169517</td>\n",
       "      <td>-1.82929449229750584749</td>\n",
       "      <td>-2.93515827620206515292</td>\n",
       "      <td>-0.40523282738911287071</td>\n",
       "      <td>-0.34739770242227863140</td>\n",
       "      <td>-0.27168534997949750354</td>\n",
       "      <td>-0.36199099883295593472</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 14:00:00</th>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>1.43731065555048820315</td>\n",
       "      <td>1.48695822226151652679</td>\n",
       "      <td>1.64713809870233096611</td>\n",
       "      <td>1.53003103651788108230</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.82876275685983169517</td>\n",
       "      <td>-1.82929449229750584749</td>\n",
       "      <td>-2.93515827620206515292</td>\n",
       "      <td>-2.93569044011878377276</td>\n",
       "      <td>-0.34739770242227863140</td>\n",
       "      <td>-0.27168534997949750354</td>\n",
       "      <td>-0.36199099883295593472</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 16:00:00</th>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>1.48695822226151652679</td>\n",
       "      <td>1.64713809870233096611</td>\n",
       "      <td>1.53003103651788108230</td>\n",
       "      <td>1.58814746625392766433</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.82929449229750584749</td>\n",
       "      <td>-2.93515827620206515292</td>\n",
       "      <td>-2.93569044011878377276</td>\n",
       "      <td>-2.93622470622248954442</td>\n",
       "      <td>-0.27168534997949750354</td>\n",
       "      <td>-0.36199099883295593472</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 18:00:00</th>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>1.64713809870233096611</td>\n",
       "      <td>1.53003103651788108230</td>\n",
       "      <td>1.58814746625392766433</td>\n",
       "      <td>1.69594788624974257552</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.93515827620206515292</td>\n",
       "      <td>-2.93569044011878377276</td>\n",
       "      <td>-2.93622470622248954442</td>\n",
       "      <td>-2.93675819572051111095</td>\n",
       "      <td>-0.36199099883295593472</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 20:00:00</th>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>1.53003103651788108230</td>\n",
       "      <td>1.58814746625392766433</td>\n",
       "      <td>1.69594788624974257552</td>\n",
       "      <td>1.63152989475315779444</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.93569044011878377276</td>\n",
       "      <td>-2.93622470622248954442</td>\n",
       "      <td>-2.93675819572051111095</td>\n",
       "      <td>-2.93729673316920392168</td>\n",
       "      <td>-0.57484294773798028100</td>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 22:00:00</th>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>1.58814746625392766433</td>\n",
       "      <td>1.69594788624974257552</td>\n",
       "      <td>1.63152989475315779444</td>\n",
       "      <td>1.66001441291780427179</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.93622470622248954442</td>\n",
       "      <td>-2.93675819572051111095</td>\n",
       "      <td>-2.93729673316920392168</td>\n",
       "      <td>-2.93783636857828733824</td>\n",
       "      <td>-0.71488756279922882619</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 12:00:00</th>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>1.69594788624974257552</td>\n",
       "      <td>1.63152989475315779444</td>\n",
       "      <td>1.66001441291780427179</td>\n",
       "      <td>1.86368599476121343805</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.93675819572051111095</td>\n",
       "      <td>-2.93729673316920392168</td>\n",
       "      <td>-2.93783636857828733824</td>\n",
       "      <td>-2.94160871496564446659</td>\n",
       "      <td>-0.76880470046970927900</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 14:00:00</th>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "      <td>1.63152989475315779444</td>\n",
       "      <td>1.66001441291780427179</td>\n",
       "      <td>1.86368599476121343805</td>\n",
       "      <td>1.75745185931851755079</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.93729673316920392168</td>\n",
       "      <td>-2.93783636857828733824</td>\n",
       "      <td>-2.94160871496564446659</td>\n",
       "      <td>-2.94214694445017288515</td>\n",
       "      <td>-0.82350493069577734850</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 16:00:00</th>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "      <td>-2.82275869154103453695</td>\n",
       "      <td>1.66001441291780427179</td>\n",
       "      <td>1.86368599476121343805</td>\n",
       "      <td>1.75745185931851755079</td>\n",
       "      <td>1.89914594005952541700</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.93783636857828733824</td>\n",
       "      <td>-2.94160871496564446659</td>\n",
       "      <td>-2.94214694445017288515</td>\n",
       "      <td>-2.94268616477990674341</td>\n",
       "      <td>-0.76519237318943711390</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 18:00:00</th>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "      <td>-2.82275869154103453695</td>\n",
       "      <td>-2.90708758143469081503</td>\n",
       "      <td>1.86368599476121343805</td>\n",
       "      <td>1.75745185931851755079</td>\n",
       "      <td>1.89914594005952541700</td>\n",
       "      <td>2.00425089386997345997</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94160871496564446659</td>\n",
       "      <td>-2.94214694445017288515</td>\n",
       "      <td>-2.94268616477990674341</td>\n",
       "      <td>-2.94322594747580179586</td>\n",
       "      <td>-0.81408762043275528786</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 20:00:00</th>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "      <td>-2.82275869154103453695</td>\n",
       "      <td>-2.90708758143469081503</td>\n",
       "      <td>-2.86510375345636170152</td>\n",
       "      <td>1.75745185931851755079</td>\n",
       "      <td>1.89914594005952541700</td>\n",
       "      <td>2.00425089386997345997</td>\n",
       "      <td>1.95305525594434659098</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94214694445017288515</td>\n",
       "      <td>-2.94268616477990674341</td>\n",
       "      <td>-2.94322594747580179586</td>\n",
       "      <td>-2.94376955965625075606</td>\n",
       "      <td>-1.10550894519194264909</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 22:00:00</th>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "      <td>-2.82275869154103453695</td>\n",
       "      <td>-2.90708758143469081503</td>\n",
       "      <td>-2.86510375345636170152</td>\n",
       "      <td>-3.04369669595454395150</td>\n",
       "      <td>1.89914594005952541700</td>\n",
       "      <td>2.00425089386997345997</td>\n",
       "      <td>1.95305525594434659098</td>\n",
       "      <td>1.97385405336060215653</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94268616477990674341</td>\n",
       "      <td>-2.94322594747580179586</td>\n",
       "      <td>-2.94376955965625075606</td>\n",
       "      <td>-2.94431590336086257409</td>\n",
       "      <td>-1.58193698922062453427</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 12:00:00</th>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "      <td>-2.82275869154103453695</td>\n",
       "      <td>-2.90708758143469081503</td>\n",
       "      <td>-2.86510375345636170152</td>\n",
       "      <td>-3.04369669595454395150</td>\n",
       "      <td>-3.84123282457471049156</td>\n",
       "      <td>2.00425089386997345997</td>\n",
       "      <td>1.95305525594434659098</td>\n",
       "      <td>1.97385405336060215653</td>\n",
       "      <td>2.19888902421887300065</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94322594747580179586</td>\n",
       "      <td>-2.94376955965625075606</td>\n",
       "      <td>-2.94431590336086257409</td>\n",
       "      <td>-2.94811737253914740720</td>\n",
       "      <td>-1.59611347777377043933</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 14:00:00</th>\n",
       "      <td>-2.82275869154103453695</td>\n",
       "      <td>-2.90708758143469081503</td>\n",
       "      <td>-2.86510375345636170152</td>\n",
       "      <td>-3.04369669595454395150</td>\n",
       "      <td>-3.84123282457471049156</td>\n",
       "      <td>-3.82513144494409296215</td>\n",
       "      <td>1.95305525594434659098</td>\n",
       "      <td>1.97385405336060215653</td>\n",
       "      <td>2.19888902421887300065</td>\n",
       "      <td>2.13252228526480847037</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94376955965625075606</td>\n",
       "      <td>-2.94431590336086257409</td>\n",
       "      <td>-2.94811737253914740720</td>\n",
       "      <td>-2.94865849421604053759</td>\n",
       "      <td>-1.71266533515592245251</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 16:00:00</th>\n",
       "      <td>-2.90708758143469081503</td>\n",
       "      <td>-2.86510375345636170152</td>\n",
       "      <td>-3.04369669595454395150</td>\n",
       "      <td>-3.84123282457471049156</td>\n",
       "      <td>-3.82513144494409296215</td>\n",
       "      <td>-3.85745040453474530295</td>\n",
       "      <td>1.97385405336060215653</td>\n",
       "      <td>2.19888902421887300065</td>\n",
       "      <td>2.13252228526480847037</td>\n",
       "      <td>2.42927850077002238649</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94431590336086257409</td>\n",
       "      <td>-2.94811737253914740720</td>\n",
       "      <td>-2.94865849421604053759</td>\n",
       "      <td>-2.94919944183418980543</td>\n",
       "      <td>-1.71362020196184428045</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "      <td>-2.82275869154103453695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 18:00:00</th>\n",
       "      <td>-2.86510375345636170152</td>\n",
       "      <td>-3.04369669595454395150</td>\n",
       "      <td>-3.84123282457471049156</td>\n",
       "      <td>-3.82513144494409296215</td>\n",
       "      <td>-3.85745040453474530295</td>\n",
       "      <td>-4.01956557945350301253</td>\n",
       "      <td>2.19888902421887300065</td>\n",
       "      <td>2.13252228526480847037</td>\n",
       "      <td>2.42927850077002238649</td>\n",
       "      <td>2.51345717005090207863</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94811737253914740720</td>\n",
       "      <td>-2.94865849421604053759</td>\n",
       "      <td>-2.94919944183418980543</td>\n",
       "      <td>-2.94974255859203537966</td>\n",
       "      <td>-1.71602757806717276523</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "      <td>-2.82275869154103453695</td>\n",
       "      <td>-2.90708758143469081503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 20:00:00</th>\n",
       "      <td>-3.04369669595454395150</td>\n",
       "      <td>-3.84123282457471049156</td>\n",
       "      <td>-3.82513144494409296215</td>\n",
       "      <td>-3.85745040453474530295</td>\n",
       "      <td>-4.01956557945350301253</td>\n",
       "      <td>-3.95024173961241187314</td>\n",
       "      <td>2.13252228526480847037</td>\n",
       "      <td>2.42927850077002238649</td>\n",
       "      <td>2.51345717005090207863</td>\n",
       "      <td>2.49559060283696210192</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94865849421604053759</td>\n",
       "      <td>-2.94919944183418980543</td>\n",
       "      <td>-2.94974255859203537966</td>\n",
       "      <td>-2.95028819262537034263</td>\n",
       "      <td>-2.00273266896203505638</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "      <td>-2.82275869154103453695</td>\n",
       "      <td>-2.90708758143469081503</td>\n",
       "      <td>-2.86510375345636170152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:00:00</th>\n",
       "      <td>-3.84123282457471049156</td>\n",
       "      <td>-3.82513144494409296215</td>\n",
       "      <td>-3.85745040453474530295</td>\n",
       "      <td>-4.01956557945350301253</td>\n",
       "      <td>-3.95024173961241187314</td>\n",
       "      <td>-3.93129596951839133112</td>\n",
       "      <td>2.42927850077002238649</td>\n",
       "      <td>2.51345717005090207863</td>\n",
       "      <td>2.49559060283696210192</td>\n",
       "      <td>2.52606029357211436803</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94919944183418980543</td>\n",
       "      <td>-2.94974255859203537966</td>\n",
       "      <td>-2.95028819262537034263</td>\n",
       "      <td>-2.95083836580538427796</td>\n",
       "      <td>-2.72925499529375015229</td>\n",
       "      <td>-2.71693266643172304242</td>\n",
       "      <td>-2.82275869154103453695</td>\n",
       "      <td>-2.90708758143469081503</td>\n",
       "      <td>-2.86510375345636170152</td>\n",
       "      <td>-3.04369669595454395150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                               target                          \\\n",
       "feature                                   y                           \n",
       "time step                               t+1                     t+2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -0.71488756279922882619 -0.76880470046970927900   \n",
       "2017-11-26 12:00:00 -0.76880470046970927900 -0.82350493069577734850   \n",
       "2017-11-26 14:00:00 -0.82350493069577734850 -0.76519237318943711390   \n",
       "2017-11-26 16:00:00 -0.76519237318943711390 -0.81408762043275528786   \n",
       "2017-11-26 18:00:00 -0.81408762043275528786 -1.10550894519194264909   \n",
       "2017-11-26 20:00:00 -1.10550894519194264909 -1.58193698922062453427   \n",
       "2017-11-26 22:00:00 -1.58193698922062453427 -1.59611347777377043933   \n",
       "2017-11-27 12:00:00 -1.59611347777377043933 -1.71266533515592245251   \n",
       "2017-11-27 14:00:00 -1.71266533515592245251 -1.71362020196184428045   \n",
       "2017-11-27 16:00:00 -1.71362020196184428045 -1.71602757806717276523   \n",
       "2017-11-27 18:00:00 -1.71602757806717276523 -2.00273266896203505638   \n",
       "2017-11-27 20:00:00 -2.00273266896203505638 -2.72925499529375015229   \n",
       "2017-11-27 22:00:00 -2.72925499529375015229 -2.71693266643172304242   \n",
       "2017-11-28 12:00:00 -2.71693266643172304242 -2.82275869154103453695   \n",
       "2017-11-28 14:00:00 -2.82275869154103453695 -2.90708758143469081503   \n",
       "2017-11-28 16:00:00 -2.90708758143469081503 -2.86510375345636170152   \n",
       "2017-11-28 18:00:00 -2.86510375345636170152 -3.04369669595454395150   \n",
       "2017-11-28 20:00:00 -3.04369669595454395150 -3.84123282457471049156   \n",
       "2017-11-28 22:00:00 -3.84123282457471049156 -3.82513144494409296215   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+3                     t+4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -0.82350493069577734850 -0.76519237318943711390   \n",
       "2017-11-26 12:00:00 -0.76519237318943711390 -0.81408762043275528786   \n",
       "2017-11-26 14:00:00 -0.81408762043275528786 -1.10550894519194264909   \n",
       "2017-11-26 16:00:00 -1.10550894519194264909 -1.58193698922062453427   \n",
       "2017-11-26 18:00:00 -1.58193698922062453427 -1.59611347777377043933   \n",
       "2017-11-26 20:00:00 -1.59611347777377043933 -1.71266533515592245251   \n",
       "2017-11-26 22:00:00 -1.71266533515592245251 -1.71362020196184428045   \n",
       "2017-11-27 12:00:00 -1.71362020196184428045 -1.71602757806717276523   \n",
       "2017-11-27 14:00:00 -1.71602757806717276523 -2.00273266896203505638   \n",
       "2017-11-27 16:00:00 -2.00273266896203505638 -2.72925499529375015229   \n",
       "2017-11-27 18:00:00 -2.72925499529375015229 -2.71693266643172304242   \n",
       "2017-11-27 20:00:00 -2.71693266643172304242 -2.82275869154103453695   \n",
       "2017-11-27 22:00:00 -2.82275869154103453695 -2.90708758143469081503   \n",
       "2017-11-28 12:00:00 -2.90708758143469081503 -2.86510375345636170152   \n",
       "2017-11-28 14:00:00 -2.86510375345636170152 -3.04369669595454395150   \n",
       "2017-11-28 16:00:00 -3.04369669595454395150 -3.84123282457471049156   \n",
       "2017-11-28 18:00:00 -3.84123282457471049156 -3.82513144494409296215   \n",
       "2017-11-28 20:00:00 -3.82513144494409296215 -3.85745040453474530295   \n",
       "2017-11-28 22:00:00 -3.85745040453474530295 -4.01956557945350301253   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+5                     t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -0.81408762043275528786 -1.10550894519194264909   \n",
       "2017-11-26 12:00:00 -1.10550894519194264909 -1.58193698922062453427   \n",
       "2017-11-26 14:00:00 -1.58193698922062453427 -1.59611347777377043933   \n",
       "2017-11-26 16:00:00 -1.59611347777377043933 -1.71266533515592245251   \n",
       "2017-11-26 18:00:00 -1.71266533515592245251 -1.71362020196184428045   \n",
       "2017-11-26 20:00:00 -1.71362020196184428045 -1.71602757806717276523   \n",
       "2017-11-26 22:00:00 -1.71602757806717276523 -2.00273266896203505638   \n",
       "2017-11-27 12:00:00 -2.00273266896203505638 -2.72925499529375015229   \n",
       "2017-11-27 14:00:00 -2.72925499529375015229 -2.71693266643172304242   \n",
       "2017-11-27 16:00:00 -2.71693266643172304242 -2.82275869154103453695   \n",
       "2017-11-27 18:00:00 -2.82275869154103453695 -2.90708758143469081503   \n",
       "2017-11-27 20:00:00 -2.90708758143469081503 -2.86510375345636170152   \n",
       "2017-11-27 22:00:00 -2.86510375345636170152 -3.04369669595454395150   \n",
       "2017-11-28 12:00:00 -3.04369669595454395150 -3.84123282457471049156   \n",
       "2017-11-28 14:00:00 -3.84123282457471049156 -3.82513144494409296215   \n",
       "2017-11-28 16:00:00 -3.82513144494409296215 -3.85745040453474530295   \n",
       "2017-11-28 18:00:00 -3.85745040453474530295 -4.01956557945350301253   \n",
       "2017-11-28 20:00:00 -4.01956557945350301253 -3.95024173961241187314   \n",
       "2017-11-28 22:00:00 -3.95024173961241187314 -3.93129596951839133112   \n",
       "\n",
       "tensor                                   X                         \\\n",
       "feature                                  e                          \n",
       "time step                              t-5                    t-4   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-25 22:00:00 1.39860886580713073002 1.48803276544208196164   \n",
       "2017-11-26 12:00:00 1.48803276544208196164 1.43731065555048820315   \n",
       "2017-11-26 14:00:00 1.43731065555048820315 1.48695822226151652679   \n",
       "2017-11-26 16:00:00 1.48695822226151652679 1.64713809870233096611   \n",
       "2017-11-26 18:00:00 1.64713809870233096611 1.53003103651788108230   \n",
       "2017-11-26 20:00:00 1.53003103651788108230 1.58814746625392766433   \n",
       "2017-11-26 22:00:00 1.58814746625392766433 1.69594788624974257552   \n",
       "2017-11-27 12:00:00 1.69594788624974257552 1.63152989475315779444   \n",
       "2017-11-27 14:00:00 1.63152989475315779444 1.66001441291780427179   \n",
       "2017-11-27 16:00:00 1.66001441291780427179 1.86368599476121343805   \n",
       "2017-11-27 18:00:00 1.86368599476121343805 1.75745185931851755079   \n",
       "2017-11-27 20:00:00 1.75745185931851755079 1.89914594005952541700   \n",
       "2017-11-27 22:00:00 1.89914594005952541700 2.00425089386997345997   \n",
       "2017-11-28 12:00:00 2.00425089386997345997 1.95305525594434659098   \n",
       "2017-11-28 14:00:00 1.95305525594434659098 1.97385405336060215653   \n",
       "2017-11-28 16:00:00 1.97385405336060215653 2.19888902421887300065   \n",
       "2017-11-28 18:00:00 2.19888902421887300065 2.13252228526480847037   \n",
       "2017-11-28 20:00:00 2.13252228526480847037 2.42927850077002238649   \n",
       "2017-11-28 22:00:00 2.42927850077002238649 2.51345717005090207863   \n",
       "\n",
       "tensor                                                             ...  \\\n",
       "feature                                                            ...   \n",
       "time step                              t-3                    t-2  ...   \n",
       "Epoch_Time_of_Clock                                                ...   \n",
       "2017-11-25 22:00:00 1.43731065555048820315 1.48695822226151652679  ...   \n",
       "2017-11-26 12:00:00 1.48695822226151652679 1.64713809870233096611  ...   \n",
       "2017-11-26 14:00:00 1.64713809870233096611 1.53003103651788108230  ...   \n",
       "2017-11-26 16:00:00 1.53003103651788108230 1.58814746625392766433  ...   \n",
       "2017-11-26 18:00:00 1.58814746625392766433 1.69594788624974257552  ...   \n",
       "2017-11-26 20:00:00 1.69594788624974257552 1.63152989475315779444  ...   \n",
       "2017-11-26 22:00:00 1.63152989475315779444 1.66001441291780427179  ...   \n",
       "2017-11-27 12:00:00 1.66001441291780427179 1.86368599476121343805  ...   \n",
       "2017-11-27 14:00:00 1.86368599476121343805 1.75745185931851755079  ...   \n",
       "2017-11-27 16:00:00 1.75745185931851755079 1.89914594005952541700  ...   \n",
       "2017-11-27 18:00:00 1.89914594005952541700 2.00425089386997345997  ...   \n",
       "2017-11-27 20:00:00 2.00425089386997345997 1.95305525594434659098  ...   \n",
       "2017-11-27 22:00:00 1.95305525594434659098 1.97385405336060215653  ...   \n",
       "2017-11-28 12:00:00 1.97385405336060215653 2.19888902421887300065  ...   \n",
       "2017-11-28 14:00:00 2.19888902421887300065 2.13252228526480847037  ...   \n",
       "2017-11-28 16:00:00 2.13252228526480847037 2.42927850077002238649  ...   \n",
       "2017-11-28 18:00:00 2.42927850077002238649 2.51345717005090207863  ...   \n",
       "2017-11-28 20:00:00 2.51345717005090207863 2.49559060283696210192  ...   \n",
       "2017-11-28 22:00:00 2.49559060283696210192 2.52606029357211436803  ...   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                               OMEGA                           \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -1.82770481597636758053 -1.82823045905599612659   \n",
       "2017-11-26 12:00:00 -1.82823045905599612659 -1.82876275685983169517   \n",
       "2017-11-26 14:00:00 -1.82876275685983169517 -1.82929449229750584749   \n",
       "2017-11-26 16:00:00 -1.82929449229750584749 -2.93515827620206515292   \n",
       "2017-11-26 18:00:00 -2.93515827620206515292 -2.93569044011878377276   \n",
       "2017-11-26 20:00:00 -2.93569044011878377276 -2.93622470622248954442   \n",
       "2017-11-26 22:00:00 -2.93622470622248954442 -2.93675819572051111095   \n",
       "2017-11-27 12:00:00 -2.93675819572051111095 -2.93729673316920392168   \n",
       "2017-11-27 14:00:00 -2.93729673316920392168 -2.93783636857828733824   \n",
       "2017-11-27 16:00:00 -2.93783636857828733824 -2.94160871496564446659   \n",
       "2017-11-27 18:00:00 -2.94160871496564446659 -2.94214694445017288515   \n",
       "2017-11-27 20:00:00 -2.94214694445017288515 -2.94268616477990674341   \n",
       "2017-11-27 22:00:00 -2.94268616477990674341 -2.94322594747580179586   \n",
       "2017-11-28 12:00:00 -2.94322594747580179586 -2.94376955965625075606   \n",
       "2017-11-28 14:00:00 -2.94376955965625075606 -2.94431590336086257409   \n",
       "2017-11-28 16:00:00 -2.94431590336086257409 -2.94811737253914740720   \n",
       "2017-11-28 18:00:00 -2.94811737253914740720 -2.94865849421604053759   \n",
       "2017-11-28 20:00:00 -2.94865849421604053759 -2.94919944183418980543   \n",
       "2017-11-28 22:00:00 -2.94919944183418980543 -2.94974255859203537966   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-1                       t   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -1.82876275685983169517 -1.82929449229750584749   \n",
       "2017-11-26 12:00:00 -1.82929449229750584749 -2.93515827620206515292   \n",
       "2017-11-26 14:00:00 -2.93515827620206515292 -2.93569044011878377276   \n",
       "2017-11-26 16:00:00 -2.93569044011878377276 -2.93622470622248954442   \n",
       "2017-11-26 18:00:00 -2.93622470622248954442 -2.93675819572051111095   \n",
       "2017-11-26 20:00:00 -2.93675819572051111095 -2.93729673316920392168   \n",
       "2017-11-26 22:00:00 -2.93729673316920392168 -2.93783636857828733824   \n",
       "2017-11-27 12:00:00 -2.93783636857828733824 -2.94160871496564446659   \n",
       "2017-11-27 14:00:00 -2.94160871496564446659 -2.94214694445017288515   \n",
       "2017-11-27 16:00:00 -2.94214694445017288515 -2.94268616477990674341   \n",
       "2017-11-27 18:00:00 -2.94268616477990674341 -2.94322594747580179586   \n",
       "2017-11-27 20:00:00 -2.94322594747580179586 -2.94376955965625075606   \n",
       "2017-11-27 22:00:00 -2.94376955965625075606 -2.94431590336086257409   \n",
       "2017-11-28 12:00:00 -2.94431590336086257409 -2.94811737253914740720   \n",
       "2017-11-28 14:00:00 -2.94811737253914740720 -2.94865849421604053759   \n",
       "2017-11-28 16:00:00 -2.94865849421604053759 -2.94919944183418980543   \n",
       "2017-11-28 18:00:00 -2.94919944183418980543 -2.94974255859203537966   \n",
       "2017-11-28 20:00:00 -2.94974255859203537966 -2.95028819262537034263   \n",
       "2017-11-28 22:00:00 -2.95028819262537034263 -2.95083836580538427796   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                               omega                           \n",
       "time step                               t-5                     t-4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -0.31144368107991765582 -0.40523282738911287071   \n",
       "2017-11-26 12:00:00 -0.40523282738911287071 -0.34739770242227863140   \n",
       "2017-11-26 14:00:00 -0.34739770242227863140 -0.27168534997949750354   \n",
       "2017-11-26 16:00:00 -0.27168534997949750354 -0.36199099883295593472   \n",
       "2017-11-26 18:00:00 -0.36199099883295593472 -0.57484294773798028100   \n",
       "2017-11-26 20:00:00 -0.57484294773798028100 -0.71488756279922882619   \n",
       "2017-11-26 22:00:00 -0.71488756279922882619 -0.76880470046970927900   \n",
       "2017-11-27 12:00:00 -0.76880470046970927900 -0.82350493069577734850   \n",
       "2017-11-27 14:00:00 -0.82350493069577734850 -0.76519237318943711390   \n",
       "2017-11-27 16:00:00 -0.76519237318943711390 -0.81408762043275528786   \n",
       "2017-11-27 18:00:00 -0.81408762043275528786 -1.10550894519194264909   \n",
       "2017-11-27 20:00:00 -1.10550894519194264909 -1.58193698922062453427   \n",
       "2017-11-27 22:00:00 -1.58193698922062453427 -1.59611347777377043933   \n",
       "2017-11-28 12:00:00 -1.59611347777377043933 -1.71266533515592245251   \n",
       "2017-11-28 14:00:00 -1.71266533515592245251 -1.71362020196184428045   \n",
       "2017-11-28 16:00:00 -1.71362020196184428045 -1.71602757806717276523   \n",
       "2017-11-28 18:00:00 -1.71602757806717276523 -2.00273266896203505638   \n",
       "2017-11-28 20:00:00 -2.00273266896203505638 -2.72925499529375015229   \n",
       "2017-11-28 22:00:00 -2.72925499529375015229 -2.71693266643172304242   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -0.34739770242227863140 -0.27168534997949750354   \n",
       "2017-11-26 12:00:00 -0.27168534997949750354 -0.36199099883295593472   \n",
       "2017-11-26 14:00:00 -0.36199099883295593472 -0.57484294773798028100   \n",
       "2017-11-26 16:00:00 -0.57484294773798028100 -0.71488756279922882619   \n",
       "2017-11-26 18:00:00 -0.71488756279922882619 -0.76880470046970927900   \n",
       "2017-11-26 20:00:00 -0.76880470046970927900 -0.82350493069577734850   \n",
       "2017-11-26 22:00:00 -0.82350493069577734850 -0.76519237318943711390   \n",
       "2017-11-27 12:00:00 -0.76519237318943711390 -0.81408762043275528786   \n",
       "2017-11-27 14:00:00 -0.81408762043275528786 -1.10550894519194264909   \n",
       "2017-11-27 16:00:00 -1.10550894519194264909 -1.58193698922062453427   \n",
       "2017-11-27 18:00:00 -1.58193698922062453427 -1.59611347777377043933   \n",
       "2017-11-27 20:00:00 -1.59611347777377043933 -1.71266533515592245251   \n",
       "2017-11-27 22:00:00 -1.71266533515592245251 -1.71362020196184428045   \n",
       "2017-11-28 12:00:00 -1.71362020196184428045 -1.71602757806717276523   \n",
       "2017-11-28 14:00:00 -1.71602757806717276523 -2.00273266896203505638   \n",
       "2017-11-28 16:00:00 -2.00273266896203505638 -2.72925499529375015229   \n",
       "2017-11-28 18:00:00 -2.72925499529375015229 -2.71693266643172304242   \n",
       "2017-11-28 20:00:00 -2.71693266643172304242 -2.82275869154103453695   \n",
       "2017-11-28 22:00:00 -2.82275869154103453695 -2.90708758143469081503   \n",
       "\n",
       "tensor                                                               \n",
       "feature                                                              \n",
       "time step                               t-1                       t  \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-25 22:00:00 -0.36199099883295593472 -0.57484294773798028100  \n",
       "2017-11-26 12:00:00 -0.57484294773798028100 -0.71488756279922882619  \n",
       "2017-11-26 14:00:00 -0.71488756279922882619 -0.76880470046970927900  \n",
       "2017-11-26 16:00:00 -0.76880470046970927900 -0.82350493069577734850  \n",
       "2017-11-26 18:00:00 -0.82350493069577734850 -0.76519237318943711390  \n",
       "2017-11-26 20:00:00 -0.76519237318943711390 -0.81408762043275528786  \n",
       "2017-11-26 22:00:00 -0.81408762043275528786 -1.10550894519194264909  \n",
       "2017-11-27 12:00:00 -1.10550894519194264909 -1.58193698922062453427  \n",
       "2017-11-27 14:00:00 -1.58193698922062453427 -1.59611347777377043933  \n",
       "2017-11-27 16:00:00 -1.59611347777377043933 -1.71266533515592245251  \n",
       "2017-11-27 18:00:00 -1.71266533515592245251 -1.71362020196184428045  \n",
       "2017-11-27 20:00:00 -1.71362020196184428045 -1.71602757806717276523  \n",
       "2017-11-27 22:00:00 -1.71602757806717276523 -2.00273266896203505638  \n",
       "2017-11-28 12:00:00 -2.00273266896203505638 -2.72925499529375015229  \n",
       "2017-11-28 14:00:00 -2.72925499529375015229 -2.71693266643172304242  \n",
       "2017-11-28 16:00:00 -2.71693266643172304242 -2.82275869154103453695  \n",
       "2017-11-28 18:00:00 -2.82275869154103453695 -2.90708758143469081503  \n",
       "2017-11-28 20:00:00 -2.90708758143469081503 -2.86510375345636170152  \n",
       "2017-11-28 22:00:00 -2.86510375345636170152 -3.04369669595454395150  \n",
       "\n",
       "[19 rows x 24 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs.dataframe.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 24)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs.dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11387476 , -0.14296739 , -0.14844142 , -0.14873727 ,\n",
       "        -0.14980416 , -0.15279259 ],\n",
       "       [-0.11437218 , -0.1433374  , -0.1487305  , -0.14909984 ,\n",
       "        -0.1503532  , -0.15357907 ],\n",
       "       [-0.114183545, -0.14306624 , -0.1486865  , -0.14949988 ,\n",
       "        -0.15128548 , -0.15503813 ],\n",
       "       [-0.11979404 , -0.14931478 , -0.15435492 , -0.15450512 ,\n",
       "        -0.15582748 , -0.15932478 ],\n",
       "       [-0.1374583  , -0.16858353 , -0.17085248 , -0.16761638 ,\n",
       "        -0.16607492 , -0.1674358  ],\n",
       "       [-0.15899576 , -0.19137426 , -0.18973257 , -0.18201415 ,\n",
       "        -0.1767425  , -0.17532654 ],\n",
       "       [-0.1644035  , -0.19677003 , -0.19414477 , -0.18557031 ,\n",
       "        -0.17975755 , -0.17805573 ],\n",
       "       [-0.17434503 , -0.20784548 , -0.20400758 , -0.19381712 ,\n",
       "        -0.18658403 , -0.18378429 ],\n",
       "       [-0.19461358 , -0.23002894 , -0.22337331 , -0.20960364 ,\n",
       "        -0.19924073 , -0.19401091 ],\n",
       "       [-0.2153198  , -0.25255033 , -0.24306214 , -0.22576025 ,\n",
       "        -0.21233498 , -0.20473757 ],\n",
       "       [-0.2539741  , -0.29299322 , -0.27716613 , -0.25267527 ,\n",
       "        -0.23320986 , -0.22100867 ],\n",
       "       [-0.31281254 , -0.3519633  , -0.32544187 , -0.28983012 ,\n",
       "        -0.2613489  , -0.24239068 ],\n",
       "       [-0.34584376 , -0.3840527  , -0.35142842 , -0.3098764  ,\n",
       "        -0.27674383 , -0.2543655  ],\n",
       "       [-0.35430026 , -0.39271027 , -0.35902363 , -0.31637844 ,\n",
       "        -0.2823927  , -0.25939626 ],\n",
       "       [-0.38214105 , -0.42049462 , -0.3827059  , -0.3359323  ,\n",
       "        -0.29867417 , -0.27324346 ],\n",
       "       [-0.40470904 , -0.4431231  , -0.40229565 , -0.35244042 ,\n",
       "        -0.31273353 , -0.28547293 ],\n",
       "       [-0.44383904 , -0.48106223 , -0.4343945  , -0.37898502 ,\n",
       "        -0.33498496 , -0.30455732 ],\n",
       "       [-0.5183116  , -0.5502717  , -0.49193448 , -0.42624423 ,\n",
       "        -0.37460726 , -0.33865303 ],\n",
       "       [-0.5616027  , -0.5891439  , -0.5241486  , -0.4529779  ,\n",
       "        -0.39744487 , -0.35873795 ]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp    h             prediction                 actual\n",
      "0 2017-11-25 22:00:00  t+1 0.63179460014647403909 0.63144654127700006185\n",
      "1 2017-11-26 12:00:00  t+1 0.63179431208283698407 0.63141531675399997781\n",
      "2 2017-11-26 14:00:00  t+1 0.63179442132454877168 0.63138363872600000715\n",
      "3 2017-11-26 16:00:00  t+1 0.63179117217107183535 0.63141740872700002907\n",
      "4 2017-11-26 18:00:00  t+1 0.63178094243855764667 0.63138909248399999186\n",
      "              timestamp    h             prediction                 actual\n",
      "109 2017-11-28 14:00:00  t+6 0.63170230645914293710 0.62964533509599995842\n",
      "110 2017-11-28 16:00:00  t+6 0.63169522412023015878 0.62962661852199997004\n",
      "111 2017-11-28 18:00:00  t+6 0.63168417195528847596 0.62953273429099998459\n",
      "112 2017-11-28 20:00:00  t+6 0.63166442643174824934 0.62957288115199994127\n",
      "113 2017-11-28 22:00:00  t+6 0.63165279484303737956 0.62958385303700004076\n",
      "(114, 4)\n"
     ]
    }
   ],
   "source": [
    "eval_df = create_evaluation_df(predictions, test_inputs, HORIZON, y_scalar)\n",
    "print(eval_df.head())\n",
    "print(eval_df.tail())\n",
    "print(eval_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h\n",
       "t+1   0.00150618939977695114\n",
       "t+2   0.00162554642406295496\n",
       "t+3   0.00179243934710066792\n",
       "t+4   0.00197108176675403132\n",
       "t+5   0.00214381451336661264\n",
       "t+6   0.00230634315307437108\n",
       "Name: APE, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual']\n",
    "eval_df.groupby('h')['APE'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011917391567827717"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(eval_df['prediction'], eval_df['actual'])\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "a = mean_absolute_error(eval_df['prediction'], eval_df['actual'])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot actuals vs predictions at each horizon for first week of the test period. As is to be expected, predictions for one step ahead (*t+1*) are more accurate than those for 2 or 3 steps ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df = eval_df[(eval_df.h=='t+1')][['timestamp', 'actual']]\n",
    "for t in range(1, HORIZON+1):\n",
    "    plot_df['t+'+str(t)] = eval_df[ (eval_df.h=='t+'+str(t))]['prediction'].values\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.plot(plot_df['timestamp'], plot_df['actual'], color='red', linewidth=4.0)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(plot_df['timestamp'], plot_df['t+1'], color='blue', linewidth=4.0, alpha=0.75)\n",
    "ax.plot(plot_df['timestamp'], plot_df['t+2'], color='blue', linewidth=3.0, alpha=0.5)\n",
    "ax.plot(plot_df['timestamp'], plot_df['t+3'], color='blue', linewidth=2.0, alpha=0.25)\n",
    "plt.xlabel('timestamp', fontsize=12)\n",
    "plt.ylabel('load', fontsize=12)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take input here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>OMEGA</th>\n",
       "      <th>omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-28 12:00:00</th>\n",
       "      <td>0.00713293417356999915</td>\n",
       "      <td>0.50910438027500004576</td>\n",
       "      <td>0.63027997968000004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 14:00:00</th>\n",
       "      <td>0.00713347224518999946</td>\n",
       "      <td>0.50904525936699995814</td>\n",
       "      <td>0.63028711579399998577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 16:00:00</th>\n",
       "      <td>0.00713335804176000021</td>\n",
       "      <td>0.50898615747599995629</td>\n",
       "      <td>0.63022582976700003421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 18:00:00</th>\n",
       "      <td>0.00713355280459000013</td>\n",
       "      <td>0.50892681859300004099</td>\n",
       "      <td>0.63017699317300002182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 20:00:00</th>\n",
       "      <td>0.00713495200033999969</td>\n",
       "      <td>0.50886720468199997391</td>\n",
       "      <td>0.63020130687099995548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:00:00</th>\n",
       "      <td>0.00713491823989999968</td>\n",
       "      <td>0.50880709484099995166</td>\n",
       "      <td>0.63009788002599997903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         e                  OMEGA  \\\n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-28 12:00:00 0.00713293417356999915 0.50910438027500004576   \n",
       "2017-11-28 14:00:00 0.00713347224518999946 0.50904525936699995814   \n",
       "2017-11-28 16:00:00 0.00713335804176000021 0.50898615747599995629   \n",
       "2017-11-28 18:00:00 0.00713355280459000013 0.50892681859300004099   \n",
       "2017-11-28 20:00:00 0.00713495200033999969 0.50886720468199997391   \n",
       "2017-11-28 22:00:00 0.00713491823989999968 0.50880709484099995166   \n",
       "\n",
       "                                     omega  \n",
       "Epoch_Time_of_Clock                         \n",
       "2017-11-28 12:00:00 0.63027997968000004647  \n",
       "2017-11-28 14:00:00 0.63028711579399998577  \n",
       "2017-11-28 16:00:00 0.63022582976700003421  \n",
       "2017-11-28 18:00:00 0.63017699317300002182  \n",
       "2017-11-28 20:00:00 0.63020130687099995548  \n",
       "2017-11-28 22:00:00 0.63009788002599997903  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df.iloc[156:162  , :]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['e', 'OMEGA', 'omega'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key , value in enumerate(columns):\n",
    "    new_df[value] = a[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.dropna( how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>OMEGA</th>\n",
       "      <th>omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-28 12:00:00</th>\n",
       "      <td>0.00713293417356999915</td>\n",
       "      <td>0.50910438027500004576</td>\n",
       "      <td>0.63027997968000004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 14:00:00</th>\n",
       "      <td>0.00713347224518999946</td>\n",
       "      <td>0.50904525936699995814</td>\n",
       "      <td>0.63028711579399998577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 16:00:00</th>\n",
       "      <td>0.00713335804176000021</td>\n",
       "      <td>0.50898615747599995629</td>\n",
       "      <td>0.63022582976700003421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 18:00:00</th>\n",
       "      <td>0.00713355280459000013</td>\n",
       "      <td>0.50892681859300004099</td>\n",
       "      <td>0.63017699317300002182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 20:00:00</th>\n",
       "      <td>0.00713495200033999969</td>\n",
       "      <td>0.50886720468199997391</td>\n",
       "      <td>0.63020130687099995548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:00:00</th>\n",
       "      <td>0.00713491823989999968</td>\n",
       "      <td>0.50880709484099995166</td>\n",
       "      <td>0.63009788002599997903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         e                  OMEGA  \\\n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-28 12:00:00 0.00713293417356999915 0.50910438027500004576   \n",
       "2017-11-28 14:00:00 0.00713347224518999946 0.50904525936699995814   \n",
       "2017-11-28 16:00:00 0.00713335804176000021 0.50898615747599995629   \n",
       "2017-11-28 18:00:00 0.00713355280459000013 0.50892681859300004099   \n",
       "2017-11-28 20:00:00 0.00713495200033999969 0.50886720468199997391   \n",
       "2017-11-28 22:00:00 0.00713491823989999968 0.50880709484099995166   \n",
       "\n",
       "                                     omega  \n",
       "Epoch_Time_of_Clock                         \n",
       "2017-11-28 12:00:00 0.63027997968000004647  \n",
       "2017-11-28 14:00:00 0.63028711579399998577  \n",
       "2017-11-28 16:00:00 0.63022582976700003421  \n",
       "2017-11-28 18:00:00 0.63017699317300002182  \n",
       "2017-11-28 20:00:00 0.63020130687099995548  \n",
       "2017-11-28 22:00:00 0.63009788002599997903  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 11, 29)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating index for output\n",
    "import datetime\n",
    "date = new_df.index.date[0]\n",
    "date + datetime.timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = new_df.index + datetime.timedelta(days =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2017-11-29 12:00:00', '2017-11-29 14:00:00',\n",
       "               '2017-11-29 16:00:00', '2017-11-29 18:00:00',\n",
       "               '2017-11-29 20:00:00', '2017-11-29 22:00:00'],\n",
       "              dtype='datetime64[ns]', name='Epoch_Time_of_Clock', freq='2H')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.index= date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['e', 'OMEGA', 'omega'], dtype='object')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         e                  OMEGA  \\\n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-29 12:00:00 0.00713293417356999915 0.50910438027500004576   \n",
      "2017-11-29 14:00:00 0.00713347224518999946 0.50904525936699995814   \n",
      "2017-11-29 16:00:00 0.00713335804176000021 0.50898615747599995629   \n",
      "2017-11-29 18:00:00 0.00713355280459000013 0.50892681859300004099   \n",
      "2017-11-29 20:00:00 0.00713495200033999969 0.50886720468199997391   \n",
      "2017-11-29 22:00:00 0.00713491823989999968 0.50880709484099995166   \n",
      "\n",
      "                                     omega  \n",
      "Epoch_Time_of_Clock                         \n",
      "2017-11-29 12:00:00 0.63027997968000004647  \n",
      "2017-11-29 14:00:00 0.63028711579399998577  \n",
      "2017-11-29 16:00:00 0.63022582976700003421  \n",
      "2017-11-29 18:00:00 0.63017699317300002182  \n",
      "2017-11-29 20:00:00 0.63020130687099995548  \n",
      "2017-11-29 22:00:00 0.63009788002599997903  \n",
      "Index(['e', 'OMEGA', 'omega'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(new_df)\n",
    "print(new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         e                   OMEGA  \\\n",
      "Epoch_Time_of_Clock                                                  \n",
      "2017-11-29 12:00:00 2.42927850077002238649 -2.94811737253914740720   \n",
      "2017-11-29 14:00:00 2.51345717005090207863 -2.94865849421604053759   \n",
      "2017-11-29 16:00:00 2.49559060283696210192 -2.94919944183418980543   \n",
      "2017-11-29 18:00:00 2.52606029357211436803 -2.94974255859203537966   \n",
      "2017-11-29 20:00:00 2.74495761489591449944 -2.95028819262537034263   \n",
      "\n",
      "                                      omega  \n",
      "Epoch_Time_of_Clock                          \n",
      "2017-11-29 12:00:00 -2.72925499529375015229  \n",
      "2017-11-29 14:00:00 -2.71693266643172304242  \n",
      "2017-11-29 16:00:00 -2.82275869154103453695  \n",
      "2017-11-29 18:00:00 -2.90708758143469081503  \n",
      "2017-11-29 20:00:00 -2.86510375345636170152  \n"
     ]
    }
   ],
   "source": [
    "freq = None\n",
    "idx_tuples = []\n",
    "drop_incomplete  = True\n",
    "new_df[['e', 'OMEGA', 'omega']] = X_scaler.transform(new_df)\n",
    "new_new_df = new_df.copy()\n",
    "tensor_structure={'X':(range(-T+1, 1), ['e', 'OMEGA', 'omega'])}\n",
    "for name, structure in tensor_structure.items():\n",
    "        rng = structure[0]\n",
    "        dataset_cols = structure[1]\n",
    "        for col in dataset_cols:\n",
    "        # do not shift non-sequential 'static' features\n",
    "            if rng is None:\n",
    "                new_df['context_'+col] = new_df[col]\n",
    "                idx_tuples.append((name, col, 'static'))\n",
    "            else:\n",
    "                for t in rng:\n",
    "                    sign = '+' if t > 0 else ''\n",
    "                    shift = str(t) if t != 0 else ''\n",
    "                    period = 't'+sign+shift\n",
    "                    shifted_col = name+'_'+col+'_'+ period\n",
    "                    new_new_df[shifted_col] = new_new_df[col].shift(t*-1, freq=freq)\n",
    "                    idx_tuples.append((name, col, period))\n",
    "        new_new_df = new_new_df.drop(new_df.columns, axis=1)\n",
    "        idx = pd.MultiIndex.from_tuples(idx_tuples, names=['tensor', 'feature', 'time step'])\n",
    "        print(new_df.head())\n",
    "        new_new_df.columns = idx\n",
    "        if drop_incomplete:\n",
    "            new_new_df = new_new_df.dropna(how='any')\n",
    "            \n",
    "inputs = {}           \n",
    "for name, structure in tensor_structure.items():\n",
    "    rng = structure[0]\n",
    "    cols = structure[1]\n",
    "    tensor = new_new_df[name][cols].as_matrix()\n",
    "    if rng is None:\n",
    "        tensor = tensor.reshape(tensor.shape[0], len(cols))\n",
    "    else:\n",
    "        tensor = tensor.reshape(tensor.shape[0], len(cols), len(rng))\n",
    "        tensor = np.transpose(tensor, axes=[0, 2, 1])\n",
    "    inputs[name] = tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor                                   X                         \\\n",
      "feature                                  e                          \n",
      "time step                              t-5                    t-4   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-29 22:00:00 2.42927850077002238649 2.51345717005090207863   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                              t-3                    t-2   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-29 22:00:00 2.49559060283696210192 2.52606029357211436803   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                              t-1                      t   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-29 22:00:00 2.74495761489591449944 2.73967595942868102910   \n",
      "\n",
      "tensor                                                               \\\n",
      "feature                               OMEGA                           \n",
      "time step                               t-5                     t-4   \n",
      "Epoch_Time_of_Clock                                                   \n",
      "2017-11-29 22:00:00 -2.94811737253914740720 -2.94865849421604053759   \n",
      "\n",
      "tensor                                                               \\\n",
      "feature                                                               \n",
      "time step                               t-3                     t-2   \n",
      "Epoch_Time_of_Clock                                                   \n",
      "2017-11-29 22:00:00 -2.94919944183418980543 -2.94974255859203537966   \n",
      "\n",
      "tensor                                                               \\\n",
      "feature                                                               \n",
      "time step                               t-1                       t   \n",
      "Epoch_Time_of_Clock                                                   \n",
      "2017-11-29 22:00:00 -2.95028819262537034263 -2.95083836580538427796   \n",
      "\n",
      "tensor                                                               \\\n",
      "feature                               omega                           \n",
      "time step                               t-5                     t-4   \n",
      "Epoch_Time_of_Clock                                                   \n",
      "2017-11-29 22:00:00 -2.72925499529375015229 -2.71693266643172304242   \n",
      "\n",
      "tensor                                                               \\\n",
      "feature                                                               \n",
      "time step                               t-3                     t-2   \n",
      "Epoch_Time_of_Clock                                                   \n",
      "2017-11-29 22:00:00 -2.82275869154103453695 -2.90708758143469081503   \n",
      "\n",
      "tensor                                                               \n",
      "feature                                                              \n",
      "time step                               t-1                       t  \n",
      "Epoch_Time_of_Clock                                                  \n",
      "2017-11-29 22:00:00 -2.86510375345636170152 -3.04369669595454395150  \n",
      "[[[ 2.4292785007700224 -2.9481173725391474 -2.72925499529375  ]\n",
      "  [ 2.513457170050902  -2.9486584942160405 -2.716932666431723 ]\n",
      "  [ 2.495590602836962  -2.94919944183419   -2.8227586915410345]\n",
      "  [ 2.5260602935721144 -2.9497425585920354 -2.907087581434691 ]\n",
      "  [ 2.7449576148959145 -2.9502881926253703 -2.8651037534563617]\n",
      "  [ 2.739675959428681  -2.9508383658053843 -3.043696695954544 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(new_new_df)\n",
    "print(inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.63877517, -0.5581424 , -0.49259844, -0.44567704, -0.42374152,\n",
       "        -0.417297  ]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictions[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63877517, -0.5581424 , -0.49259844, -0.44567704, -0.42374152,\n",
       "       -0.417297  ], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>omega</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.63877516984939575195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.55814242362976074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.49259844422340393066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.44567704200744628906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.42374151945114135742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.41729700565338134766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    omega\n",
       "0 -0.63877516984939575195\n",
       "1 -0.55814242362976074219\n",
       "2 -0.49259844422340393066\n",
       "3 -0.44567704200744628906\n",
       "4 -0.42374151945114135742\n",
       "5 -0.41729700565338134766"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(results , columns = [var_name])\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-29 12:00:00</th>\n",
       "      <td>-0.63877516984939575195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 14:00:00</th>\n",
       "      <td>-0.55814242362976074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 16:00:00</th>\n",
       "      <td>-0.49259844422340393066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 18:00:00</th>\n",
       "      <td>-0.44567704200744628906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 20:00:00</th>\n",
       "      <td>-0.42374151945114135742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 22:00:00</th>\n",
       "      <td>-0.41729700565338134766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      omega\n",
       "Epoch_Time_of_Clock                        \n",
       "2017-11-29 12:00:00 -0.63877516984939575195\n",
       "2017-11-29 14:00:00 -0.55814242362976074219\n",
       "2017-11-29 16:00:00 -0.49259844422340393066\n",
       "2017-11-29 18:00:00 -0.44567704200744628906\n",
       "2017-11-29 20:00:00 -0.42374151945114135742\n",
       "2017-11-29 22:00:00 -0.41729700565338134766"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.index = date\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[var_name] = y_scalar.inverse_transform(res_df[[var_name]])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final generated output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>omega</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-29 12:00:00</th>\n",
       "      <td>0.63149064779281616211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 14:00:00</th>\n",
       "      <td>0.63153731822967529297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 16:00:00</th>\n",
       "      <td>0.63157528638839721680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 18:00:00</th>\n",
       "      <td>0.63160246610641479492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 20:00:00</th>\n",
       "      <td>0.63161516189575195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 22:00:00</th>\n",
       "      <td>0.63161885738372802734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     omega\n",
       "Epoch_Time_of_Clock                       \n",
       "2017-11-29 12:00:00 0.63149064779281616211\n",
       "2017-11-29 14:00:00 0.63153731822967529297\n",
       "2017-11-29 16:00:00 0.63157528638839721680\n",
       "2017-11-29 18:00:00 0.63160246610641479492\n",
       "2017-11-29 20:00:00 0.63161516189575195312\n",
       "2017-11-29 22:00:00 0.63161885738372802734"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final generated ouput\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('SA1omega.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
