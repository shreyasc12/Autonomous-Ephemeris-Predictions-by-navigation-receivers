{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi step model (simple encoder-decoder)\n",
    "\n",
    "In this notebook, we demonstrate how to:\n",
    "- prepare time series data for training a RNN forecasting model\n",
    "- get data in the required shape for the keras API\n",
    "- implement a RNN model in keras to predict the next 3 steps ahead (time *t+1* to *t+3*) in the time series. This model uses a simple encoder decoder approach in which the final hidden state of the encoder is replicated across each time step of the decoder. \n",
    "- enable early stopping to reduce the likelihood of model overfitting\n",
    "- evaluate the model on a test dataset\n",
    "\n",
    "The data in this example is taken from the GEFCom2014 forecasting competition<sup>1</sup>. It consists of 3 years of hourly electricity load and temperature values between 2012 and 2014. The task is to forecast future values of electricity load.\n",
    "\n",
    "<sup>1</sup>Tao Hong, Pierre Pinson, Shu Fan, Hamidreza Zareipour, Alberto Troccoli and Rob J. Hyndman, \"Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond\", International Journal of Forecasting, vol.32, no.3, pp 896-913, July-September, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from collections import UserDict\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "from common.utils import load_data, mape, TimeSeriesTensor, create_evaluation_df\n",
    "\n",
    "pd.options.display.float_format = '{:,.12f}'.format\n",
    "np.set_printoptions(precision=12)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-01 14:00:00</th>\n",
       "      <td>5,153.670196530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 16:00:00</th>\n",
       "      <td>5,153.669187550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 18:00:00</th>\n",
       "      <td>5,153.669887540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 20:00:00</th>\n",
       "      <td>5,153.670305250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 22:00:00</th>\n",
       "      <td>5,153.673240660000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                sqrt_A\n",
       "Epoch_Time_of_Clock                   \n",
       "2017-11-01 14:00:00 5,153.670196530000\n",
       "2017-11-01 16:00:00 5,153.669187550000\n",
       "2017-11-01 18:00:00 5,153.669887540000\n",
       "2017-11-01 20:00:00 5,153.670305250000\n",
       "2017-11-01 22:00:00 5,153.673240660000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Cleaned1.csv\" , parse_dates = True)\n",
    "a = pd.to_datetime(df['Epoch_Time_of_Clock'])\n",
    "print(type(a[0]))\n",
    "df = df.drop(['Unnamed: 0', 'Unnamed: 0.1' , 'PRN','SV_Clock_Bias', 'SV_Clock_Drift', 'SV_Clock_Drift_Rate', 'IODE', 'M0','Cus','Toe', 'Cic', 'OMEGA', 'e',\n",
    "       'Cis', 'i0', 'omega', 'OMEGA_dot', 'I_dot', 'Codes', 'GPS_week',\n",
    "       'L2_P_Data_flag', 'SV_accuracy', 'SV_health', 'Tgd', 'IODC', 'T_Tx', 'Crs', 'Del_n', 'Cuc', 'Crc',\n",
    "       'Fit_Interval' ,'Epoch_Time_of_Clock' ],axis =1 )\n",
    "df.head()\n",
    "#df = df.set_index(['Epoch_Time_of_Clock'])\n",
    "df = df.set_index(a)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sqrt_A'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter parameters and Satellite PRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = 'sqrt_A'\n",
    "sat_var = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[5 : , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 12:00:00</th>\n",
       "      <td>5,153.672128680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 14:00:00</th>\n",
       "      <td>5,153.669368740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 16:00:00</th>\n",
       "      <td>5,153.668636320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 18:00:00</th>\n",
       "      <td>5,153.668607710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 20:00:00</th>\n",
       "      <td>5,153.669593810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 22:00:00</th>\n",
       "      <td>5,153.672065730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 12:00:00</th>\n",
       "      <td>5,153.673458100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 14:00:00</th>\n",
       "      <td>5,153.670349120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 16:00:00</th>\n",
       "      <td>5,153.669727330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 18:00:00</th>\n",
       "      <td>5,153.669181820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 20:00:00</th>\n",
       "      <td>5,153.670465470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 22:00:00</th>\n",
       "      <td>5,153.672172550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 12:00:00</th>\n",
       "      <td>5,153.675607680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 14:00:00</th>\n",
       "      <td>5,153.673034670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 16:00:00</th>\n",
       "      <td>5,153.672115330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 18:00:00</th>\n",
       "      <td>5,153.671932220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 20:00:00</th>\n",
       "      <td>5,153.672990800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 22:00:00</th>\n",
       "      <td>5,153.674003600001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 12:00:00</th>\n",
       "      <td>5,153.677953720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 14:00:00</th>\n",
       "      <td>5,153.676698680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 16:00:00</th>\n",
       "      <td>5,153.675025940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 18:00:00</th>\n",
       "      <td>5,153.676235200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 20:00:00</th>\n",
       "      <td>5,153.676540370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 22:00:00</th>\n",
       "      <td>5,153.677331920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 12:00:00</th>\n",
       "      <td>5,153.679876330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 14:00:00</th>\n",
       "      <td>5,153.680284500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 16:00:00</th>\n",
       "      <td>5,153.677629470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 18:00:00</th>\n",
       "      <td>5,153.680631640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 20:00:00</th>\n",
       "      <td>5,153.679979320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 22:00:00</th>\n",
       "      <td>5,153.681163790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25 12:00:00</th>\n",
       "      <td>5,153.680116650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25 14:00:00</th>\n",
       "      <td>5,153.679018020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25 16:00:00</th>\n",
       "      <td>5,153.678968430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25 18:00:00</th>\n",
       "      <td>5,153.678714750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25 20:00:00</th>\n",
       "      <td>5,153.680696490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25 22:00:00</th>\n",
       "      <td>5,153.678533550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 12:00:00</th>\n",
       "      <td>5,153.677621840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 14:00:00</th>\n",
       "      <td>5,153.676332470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 16:00:00</th>\n",
       "      <td>5,153.677089690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 18:00:00</th>\n",
       "      <td>5,153.676860810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 20:00:00</th>\n",
       "      <td>5,153.679340360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 22:00:00</th>\n",
       "      <td>5,153.676721570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 12:00:00</th>\n",
       "      <td>5,153.674901960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 14:00:00</th>\n",
       "      <td>5,153.673654560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 16:00:00</th>\n",
       "      <td>5,153.674737930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 18:00:00</th>\n",
       "      <td>5,153.674879070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 20:00:00</th>\n",
       "      <td>5,153.677614210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 22:00:00</th>\n",
       "      <td>5,153.675252910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 12:00:00</th>\n",
       "      <td>5,153.672513960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 14:00:00</th>\n",
       "      <td>5,153.671504970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 16:00:00</th>\n",
       "      <td>5,153.672332760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 18:00:00</th>\n",
       "      <td>5,153.673091890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 20:00:00</th>\n",
       "      <td>5,153.675785060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:00:00</th>\n",
       "      <td>5,153.674497600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 12:00:00</th>\n",
       "      <td>5,153.671113970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 14:00:00</th>\n",
       "      <td>5,153.670415880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 16:00:00</th>\n",
       "      <td>5,153.670558930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 18:00:00</th>\n",
       "      <td>5,153.671985630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 20:00:00</th>\n",
       "      <td>5,153.674263000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 22:00:00</th>\n",
       "      <td>5,153.674684520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                sqrt_A\n",
       "Epoch_Time_of_Clock                   \n",
       "2017-11-02 12:00:00 5,153.672128680000\n",
       "2017-11-02 14:00:00 5,153.669368740000\n",
       "2017-11-02 16:00:00 5,153.668636320000\n",
       "2017-11-02 18:00:00 5,153.668607710000\n",
       "2017-11-02 20:00:00 5,153.669593810000\n",
       "2017-11-02 22:00:00 5,153.672065730000\n",
       "2017-11-03 12:00:00 5,153.673458100000\n",
       "2017-11-03 14:00:00 5,153.670349120000\n",
       "2017-11-03 16:00:00 5,153.669727330000\n",
       "2017-11-03 18:00:00 5,153.669181820000\n",
       "2017-11-03 20:00:00 5,153.670465470000\n",
       "2017-11-03 22:00:00 5,153.672172550000\n",
       "2017-11-04 12:00:00 5,153.675607680000\n",
       "2017-11-04 14:00:00 5,153.673034670000\n",
       "2017-11-04 16:00:00 5,153.672115330000\n",
       "2017-11-04 18:00:00 5,153.671932220000\n",
       "2017-11-04 20:00:00 5,153.672990800000\n",
       "2017-11-04 22:00:00 5,153.674003600001\n",
       "2017-11-05 12:00:00 5,153.677953720000\n",
       "2017-11-05 14:00:00 5,153.676698680000\n",
       "2017-11-05 16:00:00 5,153.675025940000\n",
       "2017-11-05 18:00:00 5,153.676235200000\n",
       "2017-11-05 20:00:00 5,153.676540370000\n",
       "2017-11-05 22:00:00 5,153.677331920000\n",
       "2017-11-06 12:00:00 5,153.679876330000\n",
       "2017-11-06 14:00:00 5,153.680284500000\n",
       "2017-11-06 16:00:00 5,153.677629470000\n",
       "2017-11-06 18:00:00 5,153.680631640000\n",
       "2017-11-06 20:00:00 5,153.679979320000\n",
       "2017-11-06 22:00:00 5,153.681163790000\n",
       "...                                ...\n",
       "2017-11-25 12:00:00 5,153.680116650000\n",
       "2017-11-25 14:00:00 5,153.679018020000\n",
       "2017-11-25 16:00:00 5,153.678968430000\n",
       "2017-11-25 18:00:00 5,153.678714750000\n",
       "2017-11-25 20:00:00 5,153.680696490000\n",
       "2017-11-25 22:00:00 5,153.678533550000\n",
       "2017-11-26 12:00:00 5,153.677621840000\n",
       "2017-11-26 14:00:00 5,153.676332470000\n",
       "2017-11-26 16:00:00 5,153.677089690000\n",
       "2017-11-26 18:00:00 5,153.676860810000\n",
       "2017-11-26 20:00:00 5,153.679340360000\n",
       "2017-11-26 22:00:00 5,153.676721570000\n",
       "2017-11-27 12:00:00 5,153.674901960000\n",
       "2017-11-27 14:00:00 5,153.673654560000\n",
       "2017-11-27 16:00:00 5,153.674737930000\n",
       "2017-11-27 18:00:00 5,153.674879070000\n",
       "2017-11-27 20:00:00 5,153.677614210000\n",
       "2017-11-27 22:00:00 5,153.675252910000\n",
       "2017-11-28 12:00:00 5,153.672513960000\n",
       "2017-11-28 14:00:00 5,153.671504970000\n",
       "2017-11-28 16:00:00 5,153.672332760000\n",
       "2017-11-28 18:00:00 5,153.673091890000\n",
       "2017-11-28 20:00:00 5,153.675785060000\n",
       "2017-11-28 22:00:00 5,153.674497600000\n",
       "2017-11-29 12:00:00 5,153.671113970000\n",
       "2017-11-29 14:00:00 5,153.670415880000\n",
       "2017-11-29 16:00:00 5,153.670558930000\n",
       "2017-11-29 18:00:00 5,153.671985630000\n",
       "2017-11-29 20:00:00 5,153.674263000000\n",
       "2017-11-29 22:00:00 5,153.674684520000\n",
       "\n",
       "[168 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter number of entries per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "2017-11-25 12:00:00 2017-11-21 12:00:00\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "entry = 6\n",
    "print(df.shape[0])\n",
    "no_of_entries = df.shape[0]//entry\n",
    "valid = (no_of_entries * 70)//100\n",
    "test = (no_of_entries * 85)//100\n",
    "indexes = df.index\n",
    "#print(valid , test , indexes)\n",
    "valid_start_dt = indexes[int(valid)*int(entry)] \n",
    "test_start_dt = indexes [int(test)*int(entry)] \n",
    "test_start_dt = str(test_start_dt)\n",
    "valid_start_dt = str(valid_start_dt)\n",
    "print(test_start_dt,valid_start_dt)\n",
    "print(type(test_start_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load data into Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter lag and no. of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"total = len(df)\n",
    "t = total*70/100\n",
    "t = round(t)\n",
    "indexes = df.index\n",
    "valid_start_dt = str(indexes[t])\n",
    "t = total*85/100\n",
    "t = round(t)\n",
    "test_start_dt = str(indexes[t])\n",
    "print(valid_start_dt , test_start_dt)\n",
    "\"\"\"\n",
    "T = 6\n",
    "HORIZON = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training set containing only the model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 12:00:00</th>\n",
       "      <td>5,153.672128680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 14:00:00</th>\n",
       "      <td>5,153.669368740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 16:00:00</th>\n",
       "      <td>5,153.668636320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 18:00:00</th>\n",
       "      <td>5,153.668607710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 20:00:00</th>\n",
       "      <td>5,153.669593810000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                sqrt_A\n",
       "Epoch_Time_of_Clock                   \n",
       "2017-11-02 12:00:00 5,153.672128680000\n",
       "2017-11-02 14:00:00 5,153.669368740000\n",
       "2017-11-02 16:00:00 5,153.668636320000\n",
       "2017-11-02 18:00:00 5,153.668607710000\n",
       "2017-11-02 20:00:00 5,153.669593810000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.copy()[df.index < valid_start_dt][['sqrt_A' ]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-20 12:00:00</th>\n",
       "      <td>5,153.679801940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 14:00:00</th>\n",
       "      <td>5,153.680685040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 16:00:00</th>\n",
       "      <td>5,153.677953720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 18:00:00</th>\n",
       "      <td>5,153.681100850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 20:00:00</th>\n",
       "      <td>5,153.679851530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 22:00:00</th>\n",
       "      <td>5,153.682205200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                sqrt_A\n",
       "Epoch_Time_of_Clock                   \n",
       "2017-11-20 12:00:00 5,153.679801940000\n",
       "2017-11-20 14:00:00 5,153.680685040000\n",
       "2017-11-20 16:00:00 5,153.677953720000\n",
       "2017-11-20 18:00:00 5,153.681100850000\n",
       "2017-11-20 20:00:00 5,153.679851530000\n",
       "2017-11-20 22:00:00 5,153.682205200000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data to be in range (0, 1). This transformation should be calibrated on the training set only. This is to prevent information from the validation or test sets leaking into the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter variable to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_scalar = StandardScaler()\n",
    "y_scalar.fit(train[[var_name]])\n",
    "\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "train[[ 'sqrt_A' ]] = X_scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the TimeSeriesTensor convenience class to:\n",
    "1. Shift the values of the time series to create a Pandas dataframe containing all the data for a single training example\n",
    "2. Discard any samples with missing values\n",
    "3. Transform this Pandas dataframe into a numpy array of shape (samples, time steps, features) for input into Keras\n",
    "\n",
    "The class takes the following parameters:\n",
    "\n",
    "- **dataset**: original time series\n",
    "- **H**: the forecast horizon\n",
    "- **tensor_structure**: a dictionary discribing the tensor structure in the form { 'tensor_name' : (range(max_backward_shift, max_forward_shift), [feature, feature, ...] ) }\n",
    "- **freq**: time series frequency\n",
    "- **drop_incomplete**: (Boolean) whether to drop incomplete samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_structure = {'X':(range(-T+1, 1), ['sqrt_A'])}\n",
    "train_inputs = TimeSeriesTensor(train, var_name, HORIZON, {'X':(range(-T+1, 1), [ 'sqrt_A', ])} ,freq = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"6\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"6\" halign=\"left\">sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 22:00:00</th>\n",
       "      <td>-0.675754723574</td>\n",
       "      <td>-1.426387417839</td>\n",
       "      <td>-1.576512508022</td>\n",
       "      <td>-1.708220541186</td>\n",
       "      <td>-1.398295852747</td>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.996730129056</td>\n",
       "      <td>-1.663090542422</td>\n",
       "      <td>-1.839926159973</td>\n",
       "      <td>-1.846833763407</td>\n",
       "      <td>-1.608749600881</td>\n",
       "      <td>-1.011928788461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 12:00:00</th>\n",
       "      <td>-1.426387417839</td>\n",
       "      <td>-1.576512508022</td>\n",
       "      <td>-1.708220541186</td>\n",
       "      <td>-1.398295852747</td>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-1.663090542422</td>\n",
       "      <td>-1.839926159973</td>\n",
       "      <td>-1.846833763407</td>\n",
       "      <td>-1.608749600881</td>\n",
       "      <td>-1.011928788461</td>\n",
       "      <td>-0.675754723574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 14:00:00</th>\n",
       "      <td>-1.576512508022</td>\n",
       "      <td>-1.708220541186</td>\n",
       "      <td>-1.398295852747</td>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-1.839926159973</td>\n",
       "      <td>-1.846833763407</td>\n",
       "      <td>-1.608749600881</td>\n",
       "      <td>-1.011928788461</td>\n",
       "      <td>-0.675754723574</td>\n",
       "      <td>-1.426387417839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 16:00:00</th>\n",
       "      <td>-1.708220541186</td>\n",
       "      <td>-1.398295852747</td>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.846833763407</td>\n",
       "      <td>-1.608749600881</td>\n",
       "      <td>-1.011928788461</td>\n",
       "      <td>-0.675754723574</td>\n",
       "      <td>-1.426387417839</td>\n",
       "      <td>-1.576512508022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 18:00:00</th>\n",
       "      <td>-1.398295852747</td>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-1.608749600881</td>\n",
       "      <td>-1.011928788461</td>\n",
       "      <td>-0.675754723574</td>\n",
       "      <td>-1.426387417839</td>\n",
       "      <td>-1.576512508022</td>\n",
       "      <td>-1.708220541186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 20:00:00</th>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-1.011928788461</td>\n",
       "      <td>-0.675754723574</td>\n",
       "      <td>-1.426387417839</td>\n",
       "      <td>-1.576512508022</td>\n",
       "      <td>-1.708220541186</td>\n",
       "      <td>-1.398295852747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 22:00:00</th>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>-0.675754723574</td>\n",
       "      <td>-1.426387417839</td>\n",
       "      <td>-1.576512508022</td>\n",
       "      <td>-1.708220541186</td>\n",
       "      <td>-1.398295852747</td>\n",
       "      <td>-0.986138148336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 12:00:00</th>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>-1.426387417839</td>\n",
       "      <td>-1.576512508022</td>\n",
       "      <td>-1.708220541186</td>\n",
       "      <td>-1.398295852747</td>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.156759739101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 14:00:00</th>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-1.576512508022</td>\n",
       "      <td>-1.708220541186</td>\n",
       "      <td>-1.398295852747</td>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-0.777987739546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 16:00:00</th>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-1.708220541186</td>\n",
       "      <td>-1.398295852747</td>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-0.999953355422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 18:00:00</th>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>-1.398295852747</td>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.044163467163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 20:00:00</th>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>-0.986138148336</td>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-0.788579720265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 22:00:00</th>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>-0.156759739101</td>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-0.544049104567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 12:00:00</th>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>-0.777987739546</td>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>0.409668583260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 14:00:00</th>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>-0.999953355422</td>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.106651498474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 16:00:00</th>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>-1.044163467163</td>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-0.297215150184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 18:00:00</th>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>-0.788579720265</td>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-0.005251196818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 20:00:00</th>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>-0.544049104567</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>0.068429103362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 22:00:00</th>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>0.259541078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 12:00:00</th>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>1.140039617486</td>\n",
       "      <td>0.106651498474</td>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>0.873863889868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 14:00:00</th>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>1.140039617486</td>\n",
       "      <td>1.594562830398</td>\n",
       "      <td>-0.297215150184</td>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>0.972412528554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 16:00:00</th>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>1.140039617486</td>\n",
       "      <td>1.594562830398</td>\n",
       "      <td>0.752289101100</td>\n",
       "      <td>-0.005251196818</td>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>0.331381604613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 18:00:00</th>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>1.140039617486</td>\n",
       "      <td>1.594562830398</td>\n",
       "      <td>0.752289101100</td>\n",
       "      <td>1.762648655654</td>\n",
       "      <td>0.068429103362</td>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>1.056226072910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 20:00:00</th>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>1.140039617486</td>\n",
       "      <td>1.594562830398</td>\n",
       "      <td>0.752289101100</td>\n",
       "      <td>1.762648655654</td>\n",
       "      <td>1.434304495819</td>\n",
       "      <td>0.259541078700</td>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>0.898729813998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 22:00:00</th>\n",
       "      <td>1.140039617486</td>\n",
       "      <td>1.594562830398</td>\n",
       "      <td>0.752289101100</td>\n",
       "      <td>1.762648655654</td>\n",
       "      <td>1.434304495819</td>\n",
       "      <td>1.900341990851</td>\n",
       "      <td>0.873863889868</td>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>1.184708465331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 12:00:00</th>\n",
       "      <td>1.594562830398</td>\n",
       "      <td>0.752289101100</td>\n",
       "      <td>1.762648655654</td>\n",
       "      <td>1.434304495819</td>\n",
       "      <td>1.900341990851</td>\n",
       "      <td>1.169512220301</td>\n",
       "      <td>0.972412528554</td>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>1.140039617486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 14:00:00</th>\n",
       "      <td>0.752289101100</td>\n",
       "      <td>1.762648655654</td>\n",
       "      <td>1.434304495819</td>\n",
       "      <td>1.900341990851</td>\n",
       "      <td>1.169512220301</td>\n",
       "      <td>1.838632294958</td>\n",
       "      <td>0.331381604613</td>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>1.140039617486</td>\n",
       "      <td>1.594562830398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 16:00:00</th>\n",
       "      <td>1.762648655654</td>\n",
       "      <td>1.434304495819</td>\n",
       "      <td>1.900341990851</td>\n",
       "      <td>1.169512220301</td>\n",
       "      <td>1.838632294958</td>\n",
       "      <td>0.914848360427</td>\n",
       "      <td>1.056226072910</td>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>1.140039617486</td>\n",
       "      <td>1.594562830398</td>\n",
       "      <td>0.752289101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 18:00:00</th>\n",
       "      <td>1.434304495819</td>\n",
       "      <td>1.900341990851</td>\n",
       "      <td>1.169512220301</td>\n",
       "      <td>1.838632294958</td>\n",
       "      <td>0.914848360427</td>\n",
       "      <td>1.901723028575</td>\n",
       "      <td>0.898729813998</td>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>1.140039617486</td>\n",
       "      <td>1.594562830398</td>\n",
       "      <td>0.752289101100</td>\n",
       "      <td>1.762648655654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 20:00:00</th>\n",
       "      <td>1.900341990851</td>\n",
       "      <td>1.169512220301</td>\n",
       "      <td>1.838632294958</td>\n",
       "      <td>0.914848360427</td>\n",
       "      <td>1.901723028575</td>\n",
       "      <td>1.530091058838</td>\n",
       "      <td>1.184708465331</td>\n",
       "      <td>1.140039617486</td>\n",
       "      <td>1.594562830398</td>\n",
       "      <td>0.752289101100</td>\n",
       "      <td>1.762648655654</td>\n",
       "      <td>1.434304495819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 12:00:00</th>\n",
       "      <td>-1.223763574538</td>\n",
       "      <td>-0.862262434602</td>\n",
       "      <td>-0.758645966530</td>\n",
       "      <td>-1.034952524167</td>\n",
       "      <td>-0.944231376158</td>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.097120956385</td>\n",
       "      <td>-0.748515136510</td>\n",
       "      <td>-0.954823356878</td>\n",
       "      <td>-1.123370332833</td>\n",
       "      <td>-1.092516692075</td>\n",
       "      <td>-1.294681799052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 14:00:00</th>\n",
       "      <td>-0.862262434602</td>\n",
       "      <td>-0.758645966530</td>\n",
       "      <td>-1.034952524167</td>\n",
       "      <td>-0.944231376158</td>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.748515136510</td>\n",
       "      <td>-0.954823356878</td>\n",
       "      <td>-1.123370332833</td>\n",
       "      <td>-1.092516692075</td>\n",
       "      <td>-1.294681799052</td>\n",
       "      <td>-1.223763574538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 16:00:00</th>\n",
       "      <td>-0.758645966530</td>\n",
       "      <td>-1.034952524167</td>\n",
       "      <td>-0.944231376158</td>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.954823356878</td>\n",
       "      <td>-1.123370332833</td>\n",
       "      <td>-1.092516692075</td>\n",
       "      <td>-1.294681799052</td>\n",
       "      <td>-1.223763574538</td>\n",
       "      <td>-0.862262434602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 18:00:00</th>\n",
       "      <td>-1.034952524167</td>\n",
       "      <td>-0.944231376158</td>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-1.123370332833</td>\n",
       "      <td>-1.092516692075</td>\n",
       "      <td>-1.294681799052</td>\n",
       "      <td>-1.223763574538</td>\n",
       "      <td>-0.862262434602</td>\n",
       "      <td>-0.758645966530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 20:00:00</th>\n",
       "      <td>-0.944231376158</td>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-1.092516692075</td>\n",
       "      <td>-1.294681799052</td>\n",
       "      <td>-1.223763574538</td>\n",
       "      <td>-0.862262434602</td>\n",
       "      <td>-0.758645966530</td>\n",
       "      <td>-1.034952524167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 22:00:00</th>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-1.294681799052</td>\n",
       "      <td>-1.223763574538</td>\n",
       "      <td>-0.862262434602</td>\n",
       "      <td>-0.758645966530</td>\n",
       "      <td>-1.034952524167</td>\n",
       "      <td>-0.944231376158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 12:00:00</th>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-1.223763574538</td>\n",
       "      <td>-0.862262434602</td>\n",
       "      <td>-0.758645966530</td>\n",
       "      <td>-1.034952524167</td>\n",
       "      <td>-0.944231376158</td>\n",
       "      <td>-1.171264821641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 14:00:00</th>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.862262434602</td>\n",
       "      <td>-0.758645966530</td>\n",
       "      <td>-1.034952524167</td>\n",
       "      <td>-0.944231376158</td>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.072716182955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 16:00:00</th>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>-0.758645966530</td>\n",
       "      <td>-1.034952524167</td>\n",
       "      <td>-0.944231376158</td>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.827263266077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 18:00:00</th>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-1.034952524167</td>\n",
       "      <td>-0.944231376158</td>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.396224939570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 20:00:00</th>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>-0.944231376158</td>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-0.766937022283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 22:00:00</th>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-1.171264821641</td>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-0.544968991810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 12:00:00</th>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-1.072716182955</td>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-0.826802115377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 14:00:00</th>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.827263266077</td>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-0.675293572874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 16:00:00</th>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>-0.396224939570</td>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.645820969839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 18:00:00</th>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>-0.766937022283</td>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>0.049548481266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 20:00:00</th>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>-0.544968991810</td>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-0.380106393141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 22:00:00</th>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>-0.826802115377</td>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>0.005340784121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 12:00:00</th>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>-0.675293572874</td>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-0.311030357270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 14:00:00</th>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>0.510520561288</td>\n",
       "      <td>-0.645820969839</td>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-0.108406514189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 16:00:00</th>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>0.510520561288</td>\n",
       "      <td>0.028366933984</td>\n",
       "      <td>0.049548481266</td>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.341425261705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 18:00:00</th>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>0.510520561288</td>\n",
       "      <td>0.028366933984</td>\n",
       "      <td>0.887679096954</td>\n",
       "      <td>-0.380106393141</td>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>0.501309618293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 20:00:00</th>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>0.510520561288</td>\n",
       "      <td>0.028366933984</td>\n",
       "      <td>0.887679096954</td>\n",
       "      <td>0.493940864159</td>\n",
       "      <td>0.005340784121</td>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>0.062904952029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 22:00:00</th>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>0.510520561288</td>\n",
       "      <td>0.028366933984</td>\n",
       "      <td>0.887679096954</td>\n",
       "      <td>0.493940864159</td>\n",
       "      <td>1.085698675945</td>\n",
       "      <td>-0.311030357270</td>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>0.581438785582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 12:00:00</th>\n",
       "      <td>0.510520561288</td>\n",
       "      <td>0.028366933984</td>\n",
       "      <td>0.887679096954</td>\n",
       "      <td>0.493940864159</td>\n",
       "      <td>1.085698675945</td>\n",
       "      <td>0.855903154796</td>\n",
       "      <td>-0.108406514189</td>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>0.282106078082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 14:00:00</th>\n",
       "      <td>0.028366933984</td>\n",
       "      <td>0.887679096954</td>\n",
       "      <td>0.493940864159</td>\n",
       "      <td>1.085698675945</td>\n",
       "      <td>0.855903154796</td>\n",
       "      <td>1.069118978596</td>\n",
       "      <td>-0.341425261705</td>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>0.510520561288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 16:00:00</th>\n",
       "      <td>0.887679096954</td>\n",
       "      <td>0.493940864159</td>\n",
       "      <td>1.085698675945</td>\n",
       "      <td>0.855903154796</td>\n",
       "      <td>1.069118978596</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>0.501309618293</td>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>0.510520561288</td>\n",
       "      <td>0.028366933984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 18:00:00</th>\n",
       "      <td>0.493940864159</td>\n",
       "      <td>1.085698675945</td>\n",
       "      <td>0.855903154796</td>\n",
       "      <td>1.069118978596</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>1.169512220301</td>\n",
       "      <td>0.062904952029</td>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>0.510520561288</td>\n",
       "      <td>0.028366933984</td>\n",
       "      <td>0.887679096954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 20:00:00</th>\n",
       "      <td>1.085698675945</td>\n",
       "      <td>0.855903154796</td>\n",
       "      <td>1.069118978596</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>1.169512220301</td>\n",
       "      <td>0.867876173239</td>\n",
       "      <td>0.581438785582</td>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>0.510520561288</td>\n",
       "      <td>0.028366933984</td>\n",
       "      <td>0.887679096954</td>\n",
       "      <td>0.493940864159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 22:00:00</th>\n",
       "      <td>0.855903154796</td>\n",
       "      <td>1.069118978596</td>\n",
       "      <td>0.409668583260</td>\n",
       "      <td>1.169512220301</td>\n",
       "      <td>0.867876173239</td>\n",
       "      <td>1.436146684243</td>\n",
       "      <td>0.282106078082</td>\n",
       "      <td>0.510520561288</td>\n",
       "      <td>0.028366933984</td>\n",
       "      <td>0.887679096954</td>\n",
       "      <td>0.493940864159</td>\n",
       "      <td>1.085698675945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                       target                                  \\\n",
       "feature                           y                                   \n",
       "time step                       t+1             t+2             t+3   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00 -0.675754723574 -1.426387417839 -1.576512508022   \n",
       "2017-11-03 12:00:00 -1.426387417839 -1.576512508022 -1.708220541186   \n",
       "2017-11-03 14:00:00 -1.576512508022 -1.708220541186 -1.398295852747   \n",
       "2017-11-03 16:00:00 -1.708220541186 -1.398295852747 -0.986138148336   \n",
       "2017-11-03 18:00:00 -1.398295852747 -0.986138148336 -0.156759739101   \n",
       "2017-11-03 20:00:00 -0.986138148336 -0.156759739101 -0.777987739546   \n",
       "2017-11-03 22:00:00 -0.156759739101 -0.777987739546 -0.999953355422   \n",
       "2017-11-04 12:00:00 -0.777987739546 -0.999953355422 -1.044163467163   \n",
       "2017-11-04 14:00:00 -0.999953355422 -1.044163467163 -0.788579720265   \n",
       "2017-11-04 16:00:00 -1.044163467163 -0.788579720265 -0.544049104567   \n",
       "2017-11-04 18:00:00 -0.788579720265 -0.544049104567  0.409668583260   \n",
       "2017-11-04 20:00:00 -0.544049104567  0.409668583260  0.106651498474   \n",
       "2017-11-04 22:00:00  0.409668583260  0.106651498474 -0.297215150184   \n",
       "2017-11-05 12:00:00  0.106651498474 -0.297215150184 -0.005251196818   \n",
       "2017-11-05 14:00:00 -0.297215150184 -0.005251196818  0.068429103362   \n",
       "2017-11-05 16:00:00 -0.005251196818  0.068429103362  0.259541078700   \n",
       "2017-11-05 18:00:00  0.068429103362  0.259541078700  0.873863889868   \n",
       "2017-11-05 20:00:00  0.259541078700  0.873863889868  0.972412528554   \n",
       "2017-11-05 22:00:00  0.873863889868  0.972412528554  0.331381604613   \n",
       "2017-11-06 12:00:00  0.972412528554  0.331381604613  1.056226072910   \n",
       "2017-11-06 14:00:00  0.331381604613  1.056226072910  0.898729813998   \n",
       "2017-11-06 16:00:00  1.056226072910  0.898729813998  1.184708465331   \n",
       "2017-11-06 18:00:00  0.898729813998  1.184708465331  1.140039617486   \n",
       "2017-11-06 20:00:00  1.184708465331  1.140039617486  1.594562830398   \n",
       "2017-11-06 22:00:00  1.140039617486  1.594562830398  0.752289101100   \n",
       "2017-11-07 12:00:00  1.594562830398  0.752289101100  1.762648655654   \n",
       "2017-11-07 14:00:00  0.752289101100  1.762648655654  1.434304495819   \n",
       "2017-11-07 16:00:00  1.762648655654  1.434304495819  1.900341990851   \n",
       "2017-11-07 18:00:00  1.434304495819  1.900341990851  1.169512220301   \n",
       "2017-11-07 20:00:00  1.900341990851  1.169512220301  1.838632294958   \n",
       "...                             ...             ...             ...   \n",
       "2017-11-15 12:00:00 -1.223763574538 -0.862262434602 -0.758645966530   \n",
       "2017-11-15 14:00:00 -0.862262434602 -0.758645966530 -1.034952524167   \n",
       "2017-11-15 16:00:00 -0.758645966530 -1.034952524167 -0.944231376158   \n",
       "2017-11-15 18:00:00 -1.034952524167 -0.944231376158 -1.171264821641   \n",
       "2017-11-15 20:00:00 -0.944231376158 -1.171264821641 -1.072716182955   \n",
       "2017-11-15 22:00:00 -1.171264821641 -1.072716182955 -0.827263266077   \n",
       "2017-11-16 12:00:00 -1.072716182955 -0.827263266077 -0.396224939570   \n",
       "2017-11-16 14:00:00 -0.827263266077 -0.396224939570 -0.766937022283   \n",
       "2017-11-16 16:00:00 -0.396224939570 -0.766937022283 -0.544968991810   \n",
       "2017-11-16 18:00:00 -0.766937022283 -0.544968991810 -0.826802115377   \n",
       "2017-11-16 20:00:00 -0.544968991810 -0.826802115377 -0.675293572874   \n",
       "2017-11-16 22:00:00 -0.826802115377 -0.675293572874 -0.645820969839   \n",
       "2017-11-17 12:00:00 -0.675293572874 -0.645820969839  0.049548481266   \n",
       "2017-11-17 14:00:00 -0.645820969839  0.049548481266 -0.380106393141   \n",
       "2017-11-17 16:00:00  0.049548481266 -0.380106393141  0.005340784121   \n",
       "2017-11-17 18:00:00 -0.380106393141  0.005340784121 -0.311030357270   \n",
       "2017-11-17 20:00:00  0.005340784121 -0.311030357270 -0.108406514189   \n",
       "2017-11-17 22:00:00 -0.311030357270 -0.108406514189 -0.341425261705   \n",
       "2017-11-18 12:00:00 -0.108406514189 -0.341425261705  0.501309618293   \n",
       "2017-11-18 14:00:00 -0.341425261705  0.501309618293  0.062904952029   \n",
       "2017-11-18 16:00:00  0.501309618293  0.062904952029  0.581438785582   \n",
       "2017-11-18 18:00:00  0.062904952029  0.581438785582  0.282106078082   \n",
       "2017-11-18 20:00:00  0.581438785582  0.282106078082  0.510520561288   \n",
       "2017-11-18 22:00:00  0.282106078082  0.510520561288  0.028366933984   \n",
       "2017-11-19 12:00:00  0.510520561288  0.028366933984  0.887679096954   \n",
       "2017-11-19 14:00:00  0.028366933984  0.887679096954  0.493940864159   \n",
       "2017-11-19 16:00:00  0.887679096954  0.493940864159  1.085698675945   \n",
       "2017-11-19 18:00:00  0.493940864159  1.085698675945  0.855903154796   \n",
       "2017-11-19 20:00:00  1.085698675945  0.855903154796  1.069118978596   \n",
       "2017-11-19 22:00:00  0.855903154796  1.069118978596  0.409668583260   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                       t+4             t+5             t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00 -1.708220541186 -1.398295852747 -0.986138148336   \n",
       "2017-11-03 12:00:00 -1.398295852747 -0.986138148336 -0.156759739101   \n",
       "2017-11-03 14:00:00 -0.986138148336 -0.156759739101 -0.777987739546   \n",
       "2017-11-03 16:00:00 -0.156759739101 -0.777987739546 -0.999953355422   \n",
       "2017-11-03 18:00:00 -0.777987739546 -0.999953355422 -1.044163467163   \n",
       "2017-11-03 20:00:00 -0.999953355422 -1.044163467163 -0.788579720265   \n",
       "2017-11-03 22:00:00 -1.044163467163 -0.788579720265 -0.544049104567   \n",
       "2017-11-04 12:00:00 -0.788579720265 -0.544049104567  0.409668583260   \n",
       "2017-11-04 14:00:00 -0.544049104567  0.409668583260  0.106651498474   \n",
       "2017-11-04 16:00:00  0.409668583260  0.106651498474 -0.297215150184   \n",
       "2017-11-04 18:00:00  0.106651498474 -0.297215150184 -0.005251196818   \n",
       "2017-11-04 20:00:00 -0.297215150184 -0.005251196818  0.068429103362   \n",
       "2017-11-04 22:00:00 -0.005251196818  0.068429103362  0.259541078700   \n",
       "2017-11-05 12:00:00  0.068429103362  0.259541078700  0.873863889868   \n",
       "2017-11-05 14:00:00  0.259541078700  0.873863889868  0.972412528554   \n",
       "2017-11-05 16:00:00  0.873863889868  0.972412528554  0.331381604613   \n",
       "2017-11-05 18:00:00  0.972412528554  0.331381604613  1.056226072910   \n",
       "2017-11-05 20:00:00  0.331381604613  1.056226072910  0.898729813998   \n",
       "2017-11-05 22:00:00  1.056226072910  0.898729813998  1.184708465331   \n",
       "2017-11-06 12:00:00  0.898729813998  1.184708465331  1.140039617486   \n",
       "2017-11-06 14:00:00  1.184708465331  1.140039617486  1.594562830398   \n",
       "2017-11-06 16:00:00  1.140039617486  1.594562830398  0.752289101100   \n",
       "2017-11-06 18:00:00  1.594562830398  0.752289101100  1.762648655654   \n",
       "2017-11-06 20:00:00  0.752289101100  1.762648655654  1.434304495819   \n",
       "2017-11-06 22:00:00  1.762648655654  1.434304495819  1.900341990851   \n",
       "2017-11-07 12:00:00  1.434304495819  1.900341990851  1.169512220301   \n",
       "2017-11-07 14:00:00  1.900341990851  1.169512220301  1.838632294958   \n",
       "2017-11-07 16:00:00  1.169512220301  1.838632294958  0.914848360427   \n",
       "2017-11-07 18:00:00  1.838632294958  0.914848360427  1.901723028575   \n",
       "2017-11-07 20:00:00  0.914848360427  1.901723028575  1.530091058838   \n",
       "...                             ...             ...             ...   \n",
       "2017-11-15 12:00:00 -1.034952524167 -0.944231376158 -1.171264821641   \n",
       "2017-11-15 14:00:00 -0.944231376158 -1.171264821641 -1.072716182955   \n",
       "2017-11-15 16:00:00 -1.171264821641 -1.072716182955 -0.827263266077   \n",
       "2017-11-15 18:00:00 -1.072716182955 -0.827263266077 -0.396224939570   \n",
       "2017-11-15 20:00:00 -0.827263266077 -0.396224939570 -0.766937022283   \n",
       "2017-11-15 22:00:00 -0.396224939570 -0.766937022283 -0.544968991810   \n",
       "2017-11-16 12:00:00 -0.766937022283 -0.544968991810 -0.826802115377   \n",
       "2017-11-16 14:00:00 -0.544968991810 -0.826802115377 -0.675293572874   \n",
       "2017-11-16 16:00:00 -0.826802115377 -0.675293572874 -0.645820969839   \n",
       "2017-11-16 18:00:00 -0.675293572874 -0.645820969839  0.049548481266   \n",
       "2017-11-16 20:00:00 -0.645820969839  0.049548481266 -0.380106393141   \n",
       "2017-11-16 22:00:00  0.049548481266 -0.380106393141  0.005340784121   \n",
       "2017-11-17 12:00:00 -0.380106393141  0.005340784121 -0.311030357270   \n",
       "2017-11-17 14:00:00  0.005340784121 -0.311030357270 -0.108406514189   \n",
       "2017-11-17 16:00:00 -0.311030357270 -0.108406514189 -0.341425261705   \n",
       "2017-11-17 18:00:00 -0.108406514189 -0.341425261705  0.501309618293   \n",
       "2017-11-17 20:00:00 -0.341425261705  0.501309618293  0.062904952029   \n",
       "2017-11-17 22:00:00  0.501309618293  0.062904952029  0.581438785582   \n",
       "2017-11-18 12:00:00  0.062904952029  0.581438785582  0.282106078082   \n",
       "2017-11-18 14:00:00  0.581438785582  0.282106078082  0.510520561288   \n",
       "2017-11-18 16:00:00  0.282106078082  0.510520561288  0.028366933984   \n",
       "2017-11-18 18:00:00  0.510520561288  0.028366933984  0.887679096954   \n",
       "2017-11-18 20:00:00  0.028366933984  0.887679096954  0.493940864159   \n",
       "2017-11-18 22:00:00  0.887679096954  0.493940864159  1.085698675945   \n",
       "2017-11-19 12:00:00  0.493940864159  1.085698675945  0.855903154796   \n",
       "2017-11-19 14:00:00  1.085698675945  0.855903154796  1.069118978596   \n",
       "2017-11-19 16:00:00  0.855903154796  1.069118978596  0.409668583260   \n",
       "2017-11-19 18:00:00  1.069118978596  0.409668583260  1.169512220301   \n",
       "2017-11-19 20:00:00  0.409668583260  1.169512220301  0.867876173239   \n",
       "2017-11-19 22:00:00  1.169512220301  0.867876173239  1.436146684243   \n",
       "\n",
       "tensor                            X                                  \\\n",
       "feature                      sqrt_A                                   \n",
       "time step                       t-5             t-4             t-3   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00 -0.996730129056 -1.663090542422 -1.839926159973   \n",
       "2017-11-03 12:00:00 -1.663090542422 -1.839926159973 -1.846833763407   \n",
       "2017-11-03 14:00:00 -1.839926159973 -1.846833763407 -1.608749600881   \n",
       "2017-11-03 16:00:00 -1.846833763407 -1.608749600881 -1.011928788461   \n",
       "2017-11-03 18:00:00 -1.608749600881 -1.011928788461 -0.675754723574   \n",
       "2017-11-03 20:00:00 -1.011928788461 -0.675754723574 -1.426387417839   \n",
       "2017-11-03 22:00:00 -0.675754723574 -1.426387417839 -1.576512508022   \n",
       "2017-11-04 12:00:00 -1.426387417839 -1.576512508022 -1.708220541186   \n",
       "2017-11-04 14:00:00 -1.576512508022 -1.708220541186 -1.398295852747   \n",
       "2017-11-04 16:00:00 -1.708220541186 -1.398295852747 -0.986138148336   \n",
       "2017-11-04 18:00:00 -1.398295852747 -0.986138148336 -0.156759739101   \n",
       "2017-11-04 20:00:00 -0.986138148336 -0.156759739101 -0.777987739546   \n",
       "2017-11-04 22:00:00 -0.156759739101 -0.777987739546 -0.999953355422   \n",
       "2017-11-05 12:00:00 -0.777987739546 -0.999953355422 -1.044163467163   \n",
       "2017-11-05 14:00:00 -0.999953355422 -1.044163467163 -0.788579720265   \n",
       "2017-11-05 16:00:00 -1.044163467163 -0.788579720265 -0.544049104567   \n",
       "2017-11-05 18:00:00 -0.788579720265 -0.544049104567  0.409668583260   \n",
       "2017-11-05 20:00:00 -0.544049104567  0.409668583260  0.106651498474   \n",
       "2017-11-05 22:00:00  0.409668583260  0.106651498474 -0.297215150184   \n",
       "2017-11-06 12:00:00  0.106651498474 -0.297215150184 -0.005251196818   \n",
       "2017-11-06 14:00:00 -0.297215150184 -0.005251196818  0.068429103362   \n",
       "2017-11-06 16:00:00 -0.005251196818  0.068429103362  0.259541078700   \n",
       "2017-11-06 18:00:00  0.068429103362  0.259541078700  0.873863889868   \n",
       "2017-11-06 20:00:00  0.259541078700  0.873863889868  0.972412528554   \n",
       "2017-11-06 22:00:00  0.873863889868  0.972412528554  0.331381604613   \n",
       "2017-11-07 12:00:00  0.972412528554  0.331381604613  1.056226072910   \n",
       "2017-11-07 14:00:00  0.331381604613  1.056226072910  0.898729813998   \n",
       "2017-11-07 16:00:00  1.056226072910  0.898729813998  1.184708465331   \n",
       "2017-11-07 18:00:00  0.898729813998  1.184708465331  1.140039617486   \n",
       "2017-11-07 20:00:00  1.184708465331  1.140039617486  1.594562830398   \n",
       "...                             ...             ...             ...   \n",
       "2017-11-15 12:00:00 -1.097120956385 -0.748515136510 -0.954823356878   \n",
       "2017-11-15 14:00:00 -0.748515136510 -0.954823356878 -1.123370332833   \n",
       "2017-11-15 16:00:00 -0.954823356878 -1.123370332833 -1.092516692075   \n",
       "2017-11-15 18:00:00 -1.123370332833 -1.092516692075 -1.294681799052   \n",
       "2017-11-15 20:00:00 -1.092516692075 -1.294681799052 -1.223763574538   \n",
       "2017-11-15 22:00:00 -1.294681799052 -1.223763574538 -0.862262434602   \n",
       "2017-11-16 12:00:00 -1.223763574538 -0.862262434602 -0.758645966530   \n",
       "2017-11-16 14:00:00 -0.862262434602 -0.758645966530 -1.034952524167   \n",
       "2017-11-16 16:00:00 -0.758645966530 -1.034952524167 -0.944231376158   \n",
       "2017-11-16 18:00:00 -1.034952524167 -0.944231376158 -1.171264821641   \n",
       "2017-11-16 20:00:00 -0.944231376158 -1.171264821641 -1.072716182955   \n",
       "2017-11-16 22:00:00 -1.171264821641 -1.072716182955 -0.827263266077   \n",
       "2017-11-17 12:00:00 -1.072716182955 -0.827263266077 -0.396224939570   \n",
       "2017-11-17 14:00:00 -0.827263266077 -0.396224939570 -0.766937022283   \n",
       "2017-11-17 16:00:00 -0.396224939570 -0.766937022283 -0.544968991810   \n",
       "2017-11-17 18:00:00 -0.766937022283 -0.544968991810 -0.826802115377   \n",
       "2017-11-17 20:00:00 -0.544968991810 -0.826802115377 -0.675293572874   \n",
       "2017-11-17 22:00:00 -0.826802115377 -0.675293572874 -0.645820969839   \n",
       "2017-11-18 12:00:00 -0.675293572874 -0.645820969839  0.049548481266   \n",
       "2017-11-18 14:00:00 -0.645820969839  0.049548481266 -0.380106393141   \n",
       "2017-11-18 16:00:00  0.049548481266 -0.380106393141  0.005340784121   \n",
       "2017-11-18 18:00:00 -0.380106393141  0.005340784121 -0.311030357270   \n",
       "2017-11-18 20:00:00  0.005340784121 -0.311030357270 -0.108406514189   \n",
       "2017-11-18 22:00:00 -0.311030357270 -0.108406514189 -0.341425261705   \n",
       "2017-11-19 12:00:00 -0.108406514189 -0.341425261705  0.501309618293   \n",
       "2017-11-19 14:00:00 -0.341425261705  0.501309618293  0.062904952029   \n",
       "2017-11-19 16:00:00  0.501309618293  0.062904952029  0.581438785582   \n",
       "2017-11-19 18:00:00  0.062904952029  0.581438785582  0.282106078082   \n",
       "2017-11-19 20:00:00  0.581438785582  0.282106078082  0.510520561288   \n",
       "2017-11-19 22:00:00  0.282106078082  0.510520561288  0.028366933984   \n",
       "\n",
       "tensor                                                               \n",
       "feature                                                              \n",
       "time step                       t-2             t-1               t  \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-02 22:00:00 -1.846833763407 -1.608749600881 -1.011928788461  \n",
       "2017-11-03 12:00:00 -1.608749600881 -1.011928788461 -0.675754723574  \n",
       "2017-11-03 14:00:00 -1.011928788461 -0.675754723574 -1.426387417839  \n",
       "2017-11-03 16:00:00 -0.675754723574 -1.426387417839 -1.576512508022  \n",
       "2017-11-03 18:00:00 -1.426387417839 -1.576512508022 -1.708220541186  \n",
       "2017-11-03 20:00:00 -1.576512508022 -1.708220541186 -1.398295852747  \n",
       "2017-11-03 22:00:00 -1.708220541186 -1.398295852747 -0.986138148336  \n",
       "2017-11-04 12:00:00 -1.398295852747 -0.986138148336 -0.156759739101  \n",
       "2017-11-04 14:00:00 -0.986138148336 -0.156759739101 -0.777987739546  \n",
       "2017-11-04 16:00:00 -0.156759739101 -0.777987739546 -0.999953355422  \n",
       "2017-11-04 18:00:00 -0.777987739546 -0.999953355422 -1.044163467163  \n",
       "2017-11-04 20:00:00 -0.999953355422 -1.044163467163 -0.788579720265  \n",
       "2017-11-04 22:00:00 -1.044163467163 -0.788579720265 -0.544049104567  \n",
       "2017-11-05 12:00:00 -0.788579720265 -0.544049104567  0.409668583260  \n",
       "2017-11-05 14:00:00 -0.544049104567  0.409668583260  0.106651498474  \n",
       "2017-11-05 16:00:00  0.409668583260  0.106651498474 -0.297215150184  \n",
       "2017-11-05 18:00:00  0.106651498474 -0.297215150184 -0.005251196818  \n",
       "2017-11-05 20:00:00 -0.297215150184 -0.005251196818  0.068429103362  \n",
       "2017-11-05 22:00:00 -0.005251196818  0.068429103362  0.259541078700  \n",
       "2017-11-06 12:00:00  0.068429103362  0.259541078700  0.873863889868  \n",
       "2017-11-06 14:00:00  0.259541078700  0.873863889868  0.972412528554  \n",
       "2017-11-06 16:00:00  0.873863889868  0.972412528554  0.331381604613  \n",
       "2017-11-06 18:00:00  0.972412528554  0.331381604613  1.056226072910  \n",
       "2017-11-06 20:00:00  0.331381604613  1.056226072910  0.898729813998  \n",
       "2017-11-06 22:00:00  1.056226072910  0.898729813998  1.184708465331  \n",
       "2017-11-07 12:00:00  0.898729813998  1.184708465331  1.140039617486  \n",
       "2017-11-07 14:00:00  1.184708465331  1.140039617486  1.594562830398  \n",
       "2017-11-07 16:00:00  1.140039617486  1.594562830398  0.752289101100  \n",
       "2017-11-07 18:00:00  1.594562830398  0.752289101100  1.762648655654  \n",
       "2017-11-07 20:00:00  0.752289101100  1.762648655654  1.434304495819  \n",
       "...                             ...             ...             ...  \n",
       "2017-11-15 12:00:00 -1.123370332833 -1.092516692075 -1.294681799052  \n",
       "2017-11-15 14:00:00 -1.092516692075 -1.294681799052 -1.223763574538  \n",
       "2017-11-15 16:00:00 -1.294681799052 -1.223763574538 -0.862262434602  \n",
       "2017-11-15 18:00:00 -1.223763574538 -0.862262434602 -0.758645966530  \n",
       "2017-11-15 20:00:00 -0.862262434602 -0.758645966530 -1.034952524167  \n",
       "2017-11-15 22:00:00 -0.758645966530 -1.034952524167 -0.944231376158  \n",
       "2017-11-16 12:00:00 -1.034952524167 -0.944231376158 -1.171264821641  \n",
       "2017-11-16 14:00:00 -0.944231376158 -1.171264821641 -1.072716182955  \n",
       "2017-11-16 16:00:00 -1.171264821641 -1.072716182955 -0.827263266077  \n",
       "2017-11-16 18:00:00 -1.072716182955 -0.827263266077 -0.396224939570  \n",
       "2017-11-16 20:00:00 -0.827263266077 -0.396224939570 -0.766937022283  \n",
       "2017-11-16 22:00:00 -0.396224939570 -0.766937022283 -0.544968991810  \n",
       "2017-11-17 12:00:00 -0.766937022283 -0.544968991810 -0.826802115377  \n",
       "2017-11-17 14:00:00 -0.544968991810 -0.826802115377 -0.675293572874  \n",
       "2017-11-17 16:00:00 -0.826802115377 -0.675293572874 -0.645820969839  \n",
       "2017-11-17 18:00:00 -0.675293572874 -0.645820969839  0.049548481266  \n",
       "2017-11-17 20:00:00 -0.645820969839  0.049548481266 -0.380106393141  \n",
       "2017-11-17 22:00:00  0.049548481266 -0.380106393141  0.005340784121  \n",
       "2017-11-18 12:00:00 -0.380106393141  0.005340784121 -0.311030357270  \n",
       "2017-11-18 14:00:00  0.005340784121 -0.311030357270 -0.108406514189  \n",
       "2017-11-18 16:00:00 -0.311030357270 -0.108406514189 -0.341425261705  \n",
       "2017-11-18 18:00:00 -0.108406514189 -0.341425261705  0.501309618293  \n",
       "2017-11-18 20:00:00 -0.341425261705  0.501309618293  0.062904952029  \n",
       "2017-11-18 22:00:00  0.501309618293  0.062904952029  0.581438785582  \n",
       "2017-11-19 12:00:00  0.062904952029  0.581438785582  0.282106078082  \n",
       "2017-11-19 14:00:00  0.581438785582  0.282106078082  0.510520561288  \n",
       "2017-11-19 16:00:00  0.282106078082  0.510520561288  0.028366933984  \n",
       "2017-11-19 18:00:00  0.510520561288  0.028366933984  0.887679096954  \n",
       "2017-11-19 20:00:00  0.028366933984  0.887679096954  0.493940864159  \n",
       "2017-11-19 22:00:00  0.887679096954  0.493940864159  1.085698675945  \n",
       "\n",
       "[103 rows x 12 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs['target'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct validation set (keeping T hours from the training set in order to construct initial features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"6\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"6\" halign=\"left\">sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-21 22:00:00</th>\n",
       "      <td>1.575682208083</td>\n",
       "      <td>1.632785225290</td>\n",
       "      <td>0.964585037877</td>\n",
       "      <td>1.335294706214</td>\n",
       "      <td>1.321018348428</td>\n",
       "      <td>1.564629076663</td>\n",
       "      <td>1.308584179065</td>\n",
       "      <td>1.462396060692</td>\n",
       "      <td>0.736631705371</td>\n",
       "      <td>1.317795121842</td>\n",
       "      <td>1.150170447505</td>\n",
       "      <td>1.602390321075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 12:00:00</th>\n",
       "      <td>1.632785225290</td>\n",
       "      <td>0.964585037877</td>\n",
       "      <td>1.335294706214</td>\n",
       "      <td>1.321018348428</td>\n",
       "      <td>1.564629076663</td>\n",
       "      <td>1.600548132432</td>\n",
       "      <td>1.462396060692</td>\n",
       "      <td>0.736631705371</td>\n",
       "      <td>1.317795121842</td>\n",
       "      <td>1.150170447505</td>\n",
       "      <td>1.602390321075</td>\n",
       "      <td>1.575682208083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 14:00:00</th>\n",
       "      <td>0.964585037877</td>\n",
       "      <td>1.335294706214</td>\n",
       "      <td>1.321018348428</td>\n",
       "      <td>1.564629076663</td>\n",
       "      <td>1.600548132432</td>\n",
       "      <td>1.540683039558</td>\n",
       "      <td>0.736631705371</td>\n",
       "      <td>1.317795121842</td>\n",
       "      <td>1.150170447505</td>\n",
       "      <td>1.602390321075</td>\n",
       "      <td>1.575682208083</td>\n",
       "      <td>1.632785225290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 16:00:00</th>\n",
       "      <td>1.335294706214</td>\n",
       "      <td>1.321018348428</td>\n",
       "      <td>1.564629076663</td>\n",
       "      <td>1.600548132432</td>\n",
       "      <td>1.540683039558</td>\n",
       "      <td>1.040568677181</td>\n",
       "      <td>1.317795121842</td>\n",
       "      <td>1.150170447505</td>\n",
       "      <td>1.602390321075</td>\n",
       "      <td>1.575682208083</td>\n",
       "      <td>1.632785225290</td>\n",
       "      <td>0.964585037877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 18:00:00</th>\n",
       "      <td>1.321018348428</td>\n",
       "      <td>1.564629076663</td>\n",
       "      <td>1.600548132432</td>\n",
       "      <td>1.540683039558</td>\n",
       "      <td>1.040568677181</td>\n",
       "      <td>1.210957841780</td>\n",
       "      <td>1.150170447505</td>\n",
       "      <td>1.602390321075</td>\n",
       "      <td>1.575682208083</td>\n",
       "      <td>1.632785225290</td>\n",
       "      <td>0.964585037877</td>\n",
       "      <td>1.335294706214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                      target                                \\\n",
       "feature                          y                                 \n",
       "time step                      t+1            t+2            t+3   \n",
       "Epoch_Time_of_Clock                                                \n",
       "2017-11-21 22:00:00 1.575682208083 1.632785225290 0.964585037877   \n",
       "2017-11-22 12:00:00 1.632785225290 0.964585037877 1.335294706214   \n",
       "2017-11-22 14:00:00 0.964585037877 1.335294706214 1.321018348428   \n",
       "2017-11-22 16:00:00 1.335294706214 1.321018348428 1.564629076663   \n",
       "2017-11-22 18:00:00 1.321018348428 1.564629076663 1.600548132432   \n",
       "\n",
       "tensor                                                            \\\n",
       "feature                                                            \n",
       "time step                      t+4            t+5            t+6   \n",
       "Epoch_Time_of_Clock                                                \n",
       "2017-11-21 22:00:00 1.335294706214 1.321018348428 1.564629076663   \n",
       "2017-11-22 12:00:00 1.321018348428 1.564629076663 1.600548132432   \n",
       "2017-11-22 14:00:00 1.564629076663 1.600548132432 1.540683039558   \n",
       "2017-11-22 16:00:00 1.600548132432 1.540683039558 1.040568677181   \n",
       "2017-11-22 18:00:00 1.540683039558 1.040568677181 1.210957841780   \n",
       "\n",
       "tensor                           X                                \\\n",
       "feature                     sqrt_A                                 \n",
       "time step                      t-5            t-4            t-3   \n",
       "Epoch_Time_of_Clock                                                \n",
       "2017-11-21 22:00:00 1.308584179065 1.462396060692 0.736631705371   \n",
       "2017-11-22 12:00:00 1.462396060692 0.736631705371 1.317795121842   \n",
       "2017-11-22 14:00:00 0.736631705371 1.317795121842 1.150170447505   \n",
       "2017-11-22 16:00:00 1.317795121842 1.150170447505 1.602390321075   \n",
       "2017-11-22 18:00:00 1.150170447505 1.602390321075 1.575682208083   \n",
       "\n",
       "tensor                                                            \n",
       "feature                                                           \n",
       "time step                      t-2            t-1              t  \n",
       "Epoch_Time_of_Clock                                               \n",
       "2017-11-21 22:00:00 1.317795121842 1.150170447505 1.602390321075  \n",
       "2017-11-22 12:00:00 1.150170447505 1.602390321075 1.575682208083  \n",
       "2017-11-22 14:00:00 1.602390321075 1.575682208083 1.632785225290  \n",
       "2017-11-22 16:00:00 1.575682208083 1.632785225290 0.964585037877  \n",
       "2017-11-22 18:00:00 1.632785225290 0.964585037877 1.335294706214  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back_dt = dt.datetime.strptime(valid_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1)\n",
    "valid = df.copy()[(df.index >=look_back_dt) & (df.index < test_start_dt)][[ 'sqrt_A']]\n",
    "valid[['sqrt_A']] = X_scaler.transform(valid)\n",
    "valid_inputs = TimeSeriesTensor(valid, var_name, HORIZON, tensor_structure,freq = None)\n",
    "valid_inputs.dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a RNN forecasting model with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image('./images/simple_encoder_decoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Flatten\n",
    "from keras.callbacks import EarlyStopping ,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(LATENT_DIM, input_shape=(T,1 ) , return_sequences = True))\n",
    "model.add(LSTM(LATENT_DIM))\n",
    "model.add(RepeatVector(HORIZON))\n",
    "model.add(LSTM(LATENT_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='RMSprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 6, 64)             16896     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 6, 64)             33024     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 6, 1)              65        \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 83,009\n",
      "Trainable params: 83,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val = ModelCheckpoint(str(sat_var) +'_' +  var_name + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103 samples, validate on 13 samples\n",
      "Epoch 1/1000\n",
      "103/103 [==============================] - 2s 20ms/step - loss: 0.8441 - val_loss: 0.8807\n",
      "Epoch 2/1000\n",
      "103/103 [==============================] - 0s 640us/step - loss: 0.5612 - val_loss: 0.3280\n",
      "Epoch 3/1000\n",
      "103/103 [==============================] - 0s 810us/step - loss: 0.6011 - val_loss: 0.3302\n",
      "Epoch 4/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4688 - val_loss: 0.2944\n",
      "Epoch 5/1000\n",
      "103/103 [==============================] - 0s 783us/step - loss: 0.4669 - val_loss: 0.2817\n",
      "Epoch 6/1000\n",
      "103/103 [==============================] - 0s 995us/step - loss: 0.4588 - val_loss: 0.2704\n",
      "Epoch 7/1000\n",
      "103/103 [==============================] - 0s 902us/step - loss: 0.4504 - val_loss: 0.2591\n",
      "Epoch 8/1000\n",
      "103/103 [==============================] - 0s 960us/step - loss: 0.4428 - val_loss: 0.2480\n",
      "Epoch 9/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4358 - val_loss: 0.2370\n",
      "Epoch 10/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.2260\n",
      "Epoch 11/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4236 - val_loss: 0.2150\n",
      "Epoch 12/1000\n",
      "103/103 [==============================] - 0s 868us/step - loss: 0.4179 - val_loss: 0.2039\n",
      "Epoch 13/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.4124 - val_loss: 0.1929\n",
      "Epoch 14/1000\n",
      "103/103 [==============================] - 0s 740us/step - loss: 0.4068 - val_loss: 0.1821\n",
      "Epoch 15/1000\n",
      "103/103 [==============================] - 0s 961us/step - loss: 0.4011 - val_loss: 0.1716\n",
      "Epoch 16/1000\n",
      "103/103 [==============================] - 0s 784us/step - loss: 0.3955 - val_loss: 0.1615\n",
      "Epoch 17/1000\n",
      "103/103 [==============================] - 0s 739us/step - loss: 0.3898 - val_loss: 0.1519\n",
      "Epoch 18/1000\n",
      "103/103 [==============================] - 0s 685us/step - loss: 0.3842 - val_loss: 0.1428\n",
      "Epoch 19/1000\n",
      "103/103 [==============================] - 0s 753us/step - loss: 0.3786 - val_loss: 0.1343\n",
      "Epoch 20/1000\n",
      "103/103 [==============================] - 0s 836us/step - loss: 0.3731 - val_loss: 0.1264\n",
      "Epoch 21/1000\n",
      "103/103 [==============================] - 0s 861us/step - loss: 0.3676 - val_loss: 0.1193\n",
      "Epoch 22/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.1128\n",
      "Epoch 23/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.1071\n",
      "Epoch 24/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.3519 - val_loss: 0.1021\n",
      "Epoch 25/1000\n",
      "103/103 [==============================] - 0s 932us/step - loss: 0.3469 - val_loss: 0.0979\n",
      "Epoch 26/1000\n",
      "103/103 [==============================] - 0s 808us/step - loss: 0.3420 - val_loss: 0.0943\n",
      "Epoch 27/1000\n",
      "103/103 [==============================] - 0s 757us/step - loss: 0.3373 - val_loss: 0.0913\n",
      "Epoch 28/1000\n",
      "103/103 [==============================] - 0s 806us/step - loss: 0.3326 - val_loss: 0.0889\n",
      "Epoch 29/1000\n",
      "103/103 [==============================] - 0s 739us/step - loss: 0.3279 - val_loss: 0.0869\n",
      "Epoch 30/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.0853\n",
      "Epoch 31/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3187 - val_loss: 0.0842\n",
      "Epoch 32/1000\n",
      "103/103 [==============================] - 0s 822us/step - loss: 0.3140 - val_loss: 0.0833\n",
      "Epoch 33/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3093 - val_loss: 0.0828\n",
      "Epoch 34/1000\n",
      "103/103 [==============================] - 0s 781us/step - loss: 0.3045 - val_loss: 0.0826\n",
      "Epoch 35/1000\n",
      "103/103 [==============================] - 0s 926us/step - loss: 0.2996 - val_loss: 0.0828\n",
      "Epoch 36/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2947 - val_loss: 0.0834\n",
      "Epoch 37/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.2898 - val_loss: 0.0844\n",
      "Epoch 38/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2848 - val_loss: 0.0859\n",
      "Epoch 39/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2798 - val_loss: 0.0880\n",
      "Epoch 40/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2748 - val_loss: 0.0907\n",
      "Epoch 41/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2698 - val_loss: 0.0940\n",
      "Epoch 42/1000\n",
      "103/103 [==============================] - 0s 916us/step - loss: 0.2650 - val_loss: 0.0980\n",
      "Epoch 43/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2602 - val_loss: 0.1026\n",
      "Epoch 44/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2557 - val_loss: 0.1080\n",
      "Epoch 45/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2514 - val_loss: 0.1139\n",
      "Epoch 46/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2473 - val_loss: 0.1203\n",
      "Epoch 47/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2436 - val_loss: 0.1267\n",
      "Epoch 48/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2400 - val_loss: 0.1325\n",
      "Epoch 49/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2367 - val_loss: 0.1371\n",
      "Epoch 50/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2336 - val_loss: 0.1396\n",
      "Epoch 51/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2304 - val_loss: 0.1391\n",
      "Epoch 52/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2272 - val_loss: 0.1351\n",
      "Epoch 53/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2237 - val_loss: 0.1267\n",
      "Epoch 54/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2197 - val_loss: 0.1128\n",
      "Epoch 55/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2141 - val_loss: 0.0932\n",
      "Epoch 56/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2055 - val_loss: 0.0839\n",
      "Epoch 57/1000\n",
      "103/103 [==============================] - 0s 947us/step - loss: 0.1987 - val_loss: 0.1245\n",
      "Epoch 58/1000\n",
      "103/103 [==============================] - 0s 758us/step - loss: 0.1949 - val_loss: 0.1870\n",
      "Epoch 59/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1997 - val_loss: 0.2336\n",
      "Epoch 60/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2073 - val_loss: 0.1722\n",
      "Epoch 61/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2096 - val_loss: 0.1094\n",
      "Epoch 62/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2095 - val_loss: 0.0730\n",
      "Epoch 63/1000\n",
      "103/103 [==============================] - 0s 713us/step - loss: 0.2098 - val_loss: 0.0646\n",
      "Epoch 64/1000\n",
      "103/103 [==============================] - 0s 983us/step - loss: 0.2123 - val_loss: 0.0881\n",
      "Epoch 65/1000\n",
      "103/103 [==============================] - 0s 998us/step - loss: 0.2166 - val_loss: 0.1453\n",
      "Epoch 66/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2159 - val_loss: 0.1708\n",
      "Epoch 67/1000\n",
      "103/103 [==============================] - 0s 871us/step - loss: 0.2102 - val_loss: 0.1544\n",
      "Epoch 68/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2056 - val_loss: 0.1380\n",
      "Epoch 69/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2029 - val_loss: 0.1240\n",
      "Epoch 70/1000\n",
      "103/103 [==============================] - 0s 990us/step - loss: 0.2007 - val_loss: 0.1092\n",
      "Epoch 71/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1982 - val_loss: 0.0927\n",
      "Epoch 72/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1949 - val_loss: 0.0761\n",
      "Epoch 73/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1902 - val_loss: 0.0672\n",
      "Epoch 74/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1855 - val_loss: 0.0813\n",
      "Epoch 75/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1833 - val_loss: 0.1170\n",
      "Epoch 76/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1851 - val_loss: 0.1607\n",
      "Epoch 77/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1909 - val_loss: 0.1598\n",
      "Epoch 78/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1957 - val_loss: 0.1052\n",
      "Epoch 79/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 708us/step - loss: 0.1985 - val_loss: 0.0588\n",
      "Epoch 80/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2019 - val_loss: 0.0698\n",
      "Epoch 81/1000\n",
      "103/103 [==============================] - 0s 897us/step - loss: 0.2058 - val_loss: 0.1319\n",
      "Epoch 82/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2034 - val_loss: 0.1515\n",
      "Epoch 83/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1982 - val_loss: 0.1419\n",
      "Epoch 84/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1951 - val_loss: 0.1340\n",
      "Epoch 85/1000\n",
      "103/103 [==============================] - 0s 956us/step - loss: 0.1935 - val_loss: 0.1265\n",
      "Epoch 86/1000\n",
      "103/103 [==============================] - 0s 943us/step - loss: 0.1923 - val_loss: 0.1178\n",
      "Epoch 87/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1911 - val_loss: 0.1078\n",
      "Epoch 88/1000\n",
      "103/103 [==============================] - 0s 958us/step - loss: 0.1898 - val_loss: 0.0971\n",
      "Epoch 89/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1884 - val_loss: 0.0860\n",
      "Epoch 90/1000\n",
      "103/103 [==============================] - 0s 846us/step - loss: 0.1869 - val_loss: 0.0749\n",
      "Epoch 91/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1850 - val_loss: 0.0649\n",
      "Epoch 92/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1827 - val_loss: 0.0588\n",
      "Epoch 93/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1805 - val_loss: 0.0611\n",
      "Epoch 94/1000\n",
      "103/103 [==============================] - 0s 930us/step - loss: 0.1791 - val_loss: 0.0731\n",
      "Epoch 95/1000\n",
      "103/103 [==============================] - 0s 815us/step - loss: 0.1795 - val_loss: 0.0882\n",
      "Epoch 96/1000\n",
      "103/103 [==============================] - 0s 936us/step - loss: 0.1815 - val_loss: 0.0913\n",
      "Epoch 97/1000\n",
      "103/103 [==============================] - 0s 771us/step - loss: 0.1842 - val_loss: 0.0707\n",
      "Epoch 98/1000\n",
      "103/103 [==============================] - 0s 818us/step - loss: 0.1876 - val_loss: 0.0529\n",
      "Epoch 99/1000\n",
      "103/103 [==============================] - 0s 918us/step - loss: 0.1933 - val_loss: 0.1000\n",
      "Epoch 100/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1975 - val_loss: 0.1758\n",
      "Epoch 101/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1934 - val_loss: 0.1665\n",
      "Epoch 102/1000\n",
      "103/103 [==============================] - 0s 819us/step - loss: 0.1885 - val_loss: 0.1495\n",
      "Epoch 103/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1862 - val_loss: 0.1385\n",
      "Epoch 104/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1849 - val_loss: 0.1284\n",
      "Epoch 105/1000\n",
      "103/103 [==============================] - 0s 899us/step - loss: 0.1839 - val_loss: 0.1179\n",
      "Epoch 106/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1829 - val_loss: 0.1071\n",
      "Epoch 107/1000\n",
      "103/103 [==============================] - 0s 740us/step - loss: 0.1820 - val_loss: 0.0967\n",
      "Epoch 108/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1812 - val_loss: 0.0871\n",
      "Epoch 109/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1805 - val_loss: 0.0790\n",
      "Epoch 110/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1801 - val_loss: 0.0729\n",
      "Epoch 111/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1798 - val_loss: 0.0692\n",
      "Epoch 112/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1798 - val_loss: 0.0683\n",
      "Epoch 113/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1801 - val_loss: 0.0708\n",
      "Epoch 114/1000\n",
      "103/103 [==============================] - 0s 878us/step - loss: 0.1807 - val_loss: 0.0781\n",
      "Epoch 115/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1815 - val_loss: 0.0907\n",
      "Epoch 116/1000\n",
      "103/103 [==============================] - 0s 951us/step - loss: 0.1820 - val_loss: 0.1056\n",
      "Epoch 117/1000\n",
      "103/103 [==============================] - 0s 780us/step - loss: 0.1819 - val_loss: 0.1151\n",
      "Epoch 118/1000\n",
      "103/103 [==============================] - 0s 837us/step - loss: 0.1810 - val_loss: 0.1136\n",
      "Epoch 119/1000\n",
      "103/103 [==============================] - 0s 806us/step - loss: 0.1795 - val_loss: 0.1037\n",
      "Epoch 120/1000\n",
      "103/103 [==============================] - 0s 950us/step - loss: 0.1782 - val_loss: 0.0921\n",
      "Epoch 121/1000\n",
      "103/103 [==============================] - 0s 825us/step - loss: 0.1773 - val_loss: 0.0825\n",
      "Epoch 122/1000\n",
      "103/103 [==============================] - 0s 909us/step - loss: 0.1769 - val_loss: 0.0766\n",
      "Epoch 123/1000\n",
      "103/103 [==============================] - 0s 860us/step - loss: 0.1767 - val_loss: 0.0740\n",
      "Epoch 124/1000\n",
      "103/103 [==============================] - 0s 851us/step - loss: 0.1768 - val_loss: 0.0749\n",
      "Epoch 125/1000\n",
      "103/103 [==============================] - 0s 952us/step - loss: 0.1771 - val_loss: 0.0788\n",
      "Epoch 126/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1774 - val_loss: 0.0852\n",
      "Epoch 127/1000\n",
      "103/103 [==============================] - 0s 843us/step - loss: 0.1775 - val_loss: 0.0919\n",
      "Epoch 128/1000\n",
      "103/103 [==============================] - 0s 890us/step - loss: 0.1773 - val_loss: 0.0963\n",
      "Epoch 129/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1769 - val_loss: 0.0966\n",
      "Epoch 130/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1762 - val_loss: 0.0937\n",
      "Epoch 131/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1756 - val_loss: 0.0898\n",
      "Epoch 132/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1752 - val_loss: 0.0870\n",
      "Epoch 133/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1749 - val_loss: 0.0857\n",
      "Epoch 134/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1748 - val_loss: 0.0862\n",
      "Epoch 135/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1747 - val_loss: 0.0875\n",
      "Epoch 136/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1746 - val_loss: 0.0894\n",
      "Epoch 137/1000\n",
      "103/103 [==============================] - 0s 909us/step - loss: 0.1744 - val_loss: 0.0907\n",
      "Epoch 138/1000\n",
      "103/103 [==============================] - 0s 937us/step - loss: 0.1742 - val_loss: 0.0917\n",
      "Epoch 139/1000\n",
      "103/103 [==============================] - 0s 759us/step - loss: 0.1739 - val_loss: 0.0912\n",
      "Epoch 140/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1736 - val_loss: 0.0915\n",
      "Epoch 141/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1732 - val_loss: 0.0901\n",
      "Epoch 142/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1731 - val_loss: 0.0911\n",
      "Epoch 143/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1727 - val_loss: 0.0895\n",
      "Epoch 144/1000\n",
      "103/103 [==============================] - 0s 833us/step - loss: 0.1727 - val_loss: 0.0921\n",
      "Epoch 145/1000\n",
      "103/103 [==============================] - 0s 881us/step - loss: 0.1722 - val_loss: 0.0892\n",
      "Epoch 146/1000\n",
      "103/103 [==============================] - 0s 813us/step - loss: 0.1723 - val_loss: 0.0943\n",
      "Epoch 147/1000\n",
      "103/103 [==============================] - 0s 739us/step - loss: 0.1716 - val_loss: 0.0878\n",
      "Epoch 148/1000\n",
      "103/103 [==============================] - 0s 887us/step - loss: 0.1719 - val_loss: 0.0971\n",
      "Epoch 149/1000\n",
      "103/103 [==============================] - 0s 751us/step - loss: 0.1710 - val_loss: 0.0864\n",
      "Epoch 150/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1714 - val_loss: 0.0987\n",
      "Epoch 151/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1704 - val_loss: 0.0860\n",
      "Epoch 152/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1708 - val_loss: 0.0992\n",
      "Epoch 153/1000\n",
      "103/103 [==============================] - 0s 873us/step - loss: 0.1699 - val_loss: 0.0857\n",
      "Epoch 154/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1703 - val_loss: 0.1003\n",
      "Epoch 155/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1693 - val_loss: 0.0853\n",
      "Epoch 156/1000\n",
      "103/103 [==============================] - 0s 939us/step - loss: 0.1698 - val_loss: 0.1021\n",
      "Epoch 157/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1687 - val_loss: 0.0849\n",
      "Epoch 158/1000\n",
      "103/103 [==============================] - 0s 885us/step - loss: 0.1692 - val_loss: 0.1033\n",
      "Epoch 159/1000\n",
      "103/103 [==============================] - 0s 754us/step - loss: 0.1681 - val_loss: 0.0846\n",
      "Epoch 160/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1687 - val_loss: 0.1049\n",
      "Epoch 161/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1674 - val_loss: 0.0846\n",
      "Epoch 162/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1682 - val_loss: 0.1073\n",
      "Epoch 163/1000\n",
      "103/103 [==============================] - 0s 890us/step - loss: 0.1669 - val_loss: 0.0847\n",
      "Epoch 164/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1675 - val_loss: 0.1092\n",
      "Epoch 165/1000\n",
      "103/103 [==============================] - 0s 956us/step - loss: 0.1661 - val_loss: 0.0848\n",
      "Epoch 166/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1671 - val_loss: 0.1117\n",
      "Epoch 167/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1653 - val_loss: 0.0859\n",
      "Epoch 168/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1666 - val_loss: 0.1130\n",
      "Epoch 169/1000\n",
      "103/103 [==============================] - 0s 772us/step - loss: 0.1645 - val_loss: 0.0871\n",
      "Epoch 170/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1659 - val_loss: 0.1151\n",
      "Epoch 171/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1640 - val_loss: 0.0875\n",
      "Epoch 172/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1654 - val_loss: 0.1199\n",
      "Epoch 173/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1634 - val_loss: 0.0867\n",
      "Epoch 174/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1648 - val_loss: 0.1231\n",
      "Epoch 175/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1629 - val_loss: 0.0877\n",
      "Epoch 176/1000\n",
      "103/103 [==============================] - 0s 984us/step - loss: 0.1642 - val_loss: 0.1252\n",
      "Epoch 177/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1624 - val_loss: 0.0894\n",
      "Epoch 178/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1636 - val_loss: 0.1289\n",
      "Epoch 179/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1619 - val_loss: 0.0903\n",
      "Epoch 180/1000\n",
      "103/103 [==============================] - 0s 884us/step - loss: 0.1628 - val_loss: 0.1299\n",
      "Epoch 181/1000\n",
      "103/103 [==============================] - 0s 900us/step - loss: 0.1612 - val_loss: 0.0870\n",
      "Epoch 182/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1625 - val_loss: 0.1363\n",
      "Epoch 183/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1620 - val_loss: 0.0975\n",
      "Epoch 184/1000\n",
      "103/103 [==============================] - 0s 989us/step - loss: 0.1632 - val_loss: 0.1529\n",
      "Epoch 185/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1604 - val_loss: 0.0811\n",
      "Epoch 186/1000\n",
      "103/103 [==============================] - 0s 727us/step - loss: 0.1596 - val_loss: 0.1074\n",
      "Epoch 187/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1579 - val_loss: 0.0710\n",
      "Epoch 188/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1625 - val_loss: 0.1519\n",
      "Epoch 189/1000\n",
      "103/103 [==============================] - 0s 890us/step - loss: 0.1625 - val_loss: 0.1057\n",
      "Epoch 190/1000\n",
      "103/103 [==============================] - 0s 867us/step - loss: 0.1607 - val_loss: 0.1464\n",
      "Epoch 191/1000\n",
      "103/103 [==============================] - 0s 806us/step - loss: 0.1596 - val_loss: 0.0957\n",
      "Epoch 192/1000\n",
      "103/103 [==============================] - 0s 898us/step - loss: 0.1601 - val_loss: 0.1413\n",
      "Epoch 193/1000\n",
      "103/103 [==============================] - 0s 850us/step - loss: 0.1578 - val_loss: 0.0788\n",
      "Epoch 194/1000\n",
      "103/103 [==============================] - 0s 869us/step - loss: 0.1594 - val_loss: 0.1369\n",
      "Epoch 195/1000\n",
      "103/103 [==============================] - 0s 897us/step - loss: 0.1573 - val_loss: 0.0756\n",
      "Epoch 196/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1590 - val_loss: 0.1386\n",
      "Epoch 197/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1579 - val_loss: 0.0872\n",
      "Epoch 198/1000\n",
      "103/103 [==============================] - 0s 836us/step - loss: 0.1611 - val_loss: 0.1639\n",
      "Epoch 199/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1573 - val_loss: 0.0733\n",
      "Epoch 200/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1559 - val_loss: 0.1062\n",
      "Epoch 201/1000\n",
      "103/103 [==============================] - 0s 879us/step - loss: 0.1540 - val_loss: 0.0618\n",
      "Epoch 202/1000\n",
      "103/103 [==============================] - 0s 938us/step - loss: 0.1578 - val_loss: 0.1446\n",
      "Epoch 203/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1583 - val_loss: 0.0897\n",
      "Epoch 204/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1592 - val_loss: 0.1642\n",
      "Epoch 205/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1560 - val_loss: 0.0852\n",
      "Epoch 206/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1569 - val_loss: 0.1326\n",
      "Epoch 207/1000\n",
      "103/103 [==============================] - 0s 897us/step - loss: 0.1535 - val_loss: 0.0681\n",
      "Epoch 208/1000\n",
      "103/103 [==============================] - 0s 771us/step - loss: 0.1543 - val_loss: 0.1140\n",
      "Epoch 209/1000\n",
      "103/103 [==============================] - 0s 938us/step - loss: 0.1523 - val_loss: 0.0577\n",
      "Epoch 210/1000\n",
      "103/103 [==============================] - 0s 902us/step - loss: 0.1584 - val_loss: 0.1663\n",
      "Epoch 211/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1553 - val_loss: 0.0877\n",
      "Epoch 212/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1562 - val_loss: 0.1331\n",
      "Epoch 213/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1520 - val_loss: 0.0767\n",
      "Epoch 214/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1517 - val_loss: 0.1002\n",
      "Epoch 215/1000\n",
      "103/103 [==============================] - 0s 947us/step - loss: 0.1510 - val_loss: 0.0720\n",
      "Epoch 216/1000\n",
      "103/103 [==============================] - 0s 725us/step - loss: 0.1558 - val_loss: 0.1692\n",
      "Epoch 217/1000\n",
      "103/103 [==============================] - 0s 904us/step - loss: 0.1534 - val_loss: 0.0687\n",
      "Epoch 218/1000\n",
      "103/103 [==============================] - 0s 718us/step - loss: 0.1514 - val_loss: 0.1239\n",
      "Epoch 219/1000\n",
      "103/103 [==============================] - 0s 892us/step - loss: 0.1502 - val_loss: 0.0620\n",
      "Epoch 220/1000\n",
      "103/103 [==============================] - 0s 733us/step - loss: 0.1561 - val_loss: 0.1655\n",
      "Epoch 221/1000\n",
      "103/103 [==============================] - 0s 772us/step - loss: 0.1515 - val_loss: 0.0710\n",
      "Epoch 222/1000\n",
      "103/103 [==============================] - 0s 700us/step - loss: 0.1509 - val_loss: 0.1271\n",
      "Epoch 223/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1502 - val_loss: 0.0819\n",
      "Epoch 224/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1522 - val_loss: 0.1517\n",
      "Epoch 225/1000\n",
      "103/103 [==============================] - 0s 860us/step - loss: 0.1494 - val_loss: 0.0592\n",
      "Epoch 226/1000\n",
      "103/103 [==============================] - 0s 849us/step - loss: 0.1497 - val_loss: 0.1253\n",
      "Epoch 227/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1480 - val_loss: 0.0460\n",
      "Epoch 228/1000\n",
      "103/103 [==============================] - 0s 803us/step - loss: 0.1517 - val_loss: 0.1271\n",
      "Epoch 229/1000\n",
      "103/103 [==============================] - 0s 826us/step - loss: 0.1484 - val_loss: 0.0528\n",
      "Epoch 230/1000\n",
      "103/103 [==============================] - 0s 867us/step - loss: 0.1516 - val_loss: 0.1417\n",
      "Epoch 231/1000\n",
      "103/103 [==============================] - 0s 832us/step - loss: 0.1507 - val_loss: 0.0990\n",
      "Epoch 232/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1467 - val_loss: 0.1200\n",
      "Epoch 233/1000\n",
      "103/103 [==============================] - 0s 966us/step - loss: 0.1475 - val_loss: 0.0919\n",
      "Epoch 234/1000\n",
      "103/103 [==============================] - 0s 823us/step - loss: 0.1457 - val_loss: 0.1132\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1462 - val_loss: 0.0723\n",
      "Epoch 236/1000\n",
      "103/103 [==============================] - 0s 691us/step - loss: 0.1467 - val_loss: 0.1496\n",
      "Epoch 237/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1468 - val_loss: 0.0532\n",
      "Epoch 238/1000\n",
      "103/103 [==============================] - 0s 762us/step - loss: 0.1512 - val_loss: 0.1688\n",
      "Epoch 239/1000\n",
      "103/103 [==============================] - 0s 986us/step - loss: 0.1468 - val_loss: 0.0738\n",
      "Epoch 240/1000\n",
      "103/103 [==============================] - 0s 886us/step - loss: 0.1460 - val_loss: 0.1274\n",
      "Epoch 241/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1435 - val_loss: 0.0705\n",
      "Epoch 242/1000\n",
      "103/103 [==============================] - 0s 940us/step - loss: 0.1449 - val_loss: 0.1299\n",
      "Epoch 243/1000\n",
      "103/103 [==============================] - 0s 840us/step - loss: 0.1430 - val_loss: 0.0596\n",
      "Epoch 244/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1455 - val_loss: 0.1557\n",
      "Epoch 245/1000\n",
      "103/103 [==============================] - 0s 854us/step - loss: 0.1448 - val_loss: 0.0714\n",
      "Epoch 246/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1451 - val_loss: 0.1592\n",
      "Epoch 247/1000\n",
      "103/103 [==============================] - 0s 844us/step - loss: 0.1435 - val_loss: 0.0731\n",
      "Epoch 248/1000\n",
      "103/103 [==============================] - 0s 964us/step - loss: 0.1429 - val_loss: 0.1384\n",
      "Epoch 249/1000\n",
      "103/103 [==============================] - 0s 726us/step - loss: 0.1414 - val_loss: 0.0615\n",
      "Epoch 250/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1426 - val_loss: 0.1444\n",
      "Epoch 251/1000\n",
      "103/103 [==============================] - 0s 997us/step - loss: 0.1409 - val_loss: 0.0531\n",
      "Epoch 252/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1408 - val_loss: 0.1182\n",
      "Epoch 253/1000\n",
      "103/103 [==============================] - 0s 865us/step - loss: 0.1441 - val_loss: 0.0388\n",
      "Epoch 254/1000\n",
      "103/103 [==============================] - 0s 964us/step - loss: 0.1463 - val_loss: 0.1226\n",
      "Epoch 255/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1387 - val_loss: 0.0527\n",
      "Epoch 256/1000\n",
      "103/103 [==============================] - 0s 989us/step - loss: 0.1393 - val_loss: 0.1220\n",
      "Epoch 257/1000\n",
      "103/103 [==============================] - 0s 984us/step - loss: 0.1402 - val_loss: 0.1050\n",
      "Epoch 258/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1397 - val_loss: 0.1431\n",
      "Epoch 259/1000\n",
      "103/103 [==============================] - 0s 679us/step - loss: 0.1395 - val_loss: 0.1089\n",
      "Epoch 260/1000\n",
      "103/103 [==============================] - 0s 728us/step - loss: 0.1379 - val_loss: 0.1241\n",
      "Epoch 261/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1367 - val_loss: 0.0735\n",
      "Epoch 262/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1366 - val_loss: 0.1264\n",
      "Epoch 263/1000\n",
      "103/103 [==============================] - 0s 997us/step - loss: 0.1362 - val_loss: 0.0395\n",
      "Epoch 264/1000\n",
      "103/103 [==============================] - 0s 978us/step - loss: 0.1447 - val_loss: 0.1383\n",
      "Epoch 265/1000\n",
      "103/103 [==============================] - 0s 845us/step - loss: 0.1394 - val_loss: 0.0436\n",
      "Epoch 266/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1376 - val_loss: 0.1282\n",
      "Epoch 267/1000\n",
      "103/103 [==============================] - 0s 943us/step - loss: 0.1341 - val_loss: 0.0713\n",
      "Epoch 268/1000\n",
      "103/103 [==============================] - 0s 618us/step - loss: 0.1352 - val_loss: 0.1336\n",
      "Epoch 269/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1365 - val_loss: 0.1261\n",
      "Epoch 270/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1350 - val_loss: 0.1378\n",
      "Epoch 271/1000\n",
      "103/103 [==============================] - 0s 967us/step - loss: 0.1338 - val_loss: 0.0985\n",
      "Epoch 272/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1325 - val_loss: 0.1066\n",
      "Epoch 273/1000\n",
      "103/103 [==============================] - 0s 751us/step - loss: 0.1310 - val_loss: 0.0454\n",
      "Epoch 274/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1361 - val_loss: 0.1470\n",
      "Epoch 275/1000\n",
      "103/103 [==============================] - 0s 958us/step - loss: 0.1401 - val_loss: 0.0381\n",
      "Epoch 276/1000\n",
      "103/103 [==============================] - 0s 933us/step - loss: 0.1391 - val_loss: 0.1425\n",
      "Epoch 277/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1318 - val_loss: 0.0534\n",
      "Epoch 278/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1294 - val_loss: 0.0954\n",
      "Epoch 279/1000\n",
      "103/103 [==============================] - 0s 827us/step - loss: 0.1280 - val_loss: 0.0466\n",
      "Epoch 280/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1304 - val_loss: 0.1055\n",
      "Epoch 281/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1300 - val_loss: 0.0395\n",
      "Epoch 282/1000\n",
      "103/103 [==============================] - 0s 783us/step - loss: 0.1357 - val_loss: 0.1638\n",
      "Epoch 283/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1343 - val_loss: 0.0949\n",
      "Epoch 284/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1292 - val_loss: 0.1427\n",
      "Epoch 285/1000\n",
      "103/103 [==============================] - 0s 850us/step - loss: 0.1391 - val_loss: 0.2177\n",
      "Epoch 286/1000\n",
      "103/103 [==============================] - 0s 913us/step - loss: 0.1346 - val_loss: 0.1410\n",
      "Epoch 287/1000\n",
      "103/103 [==============================] - 0s 840us/step - loss: 0.1279 - val_loss: 0.1101\n",
      "Epoch 288/1000\n",
      "103/103 [==============================] - 0s 968us/step - loss: 0.1257 - val_loss: 0.0527\n",
      "Epoch 289/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1270 - val_loss: 0.0573\n",
      "Epoch 290/1000\n",
      "103/103 [==============================] - 0s 790us/step - loss: 0.1283 - val_loss: 0.0390\n",
      "Epoch 291/1000\n",
      "103/103 [==============================] - 0s 851us/step - loss: 0.1319 - val_loss: 0.1855\n",
      "Epoch 292/1000\n",
      "103/103 [==============================] - 0s 633us/step - loss: 0.1331 - val_loss: 0.0954\n",
      "Epoch 293/1000\n",
      "103/103 [==============================] - 0s 795us/step - loss: 0.1269 - val_loss: 0.1392\n",
      "Epoch 294/1000\n",
      "103/103 [==============================] - 0s 908us/step - loss: 0.1319 - val_loss: 0.2295\n",
      "Epoch 295/1000\n",
      "103/103 [==============================] - 0s 830us/step - loss: 0.1308 - val_loss: 0.1156\n",
      "Epoch 296/1000\n",
      "103/103 [==============================] - 0s 907us/step - loss: 0.1236 - val_loss: 0.0798\n",
      "Epoch 297/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1237 - val_loss: 0.0374\n",
      "Epoch 298/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1319 - val_loss: 0.1496\n",
      "Epoch 299/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1275 - val_loss: 0.0396\n",
      "Epoch 300/1000\n",
      "103/103 [==============================] - 0s 737us/step - loss: 0.1287 - val_loss: 0.1628\n",
      "Epoch 301/1000\n",
      "103/103 [==============================] - 0s 972us/step - loss: 0.1261 - val_loss: 0.0969\n",
      "Epoch 302/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1209 - val_loss: 0.0829\n",
      "Epoch 303/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1200 - val_loss: 0.0644\n",
      "Epoch 304/1000\n",
      "103/103 [==============================] - 0s 916us/step - loss: 0.1223 - val_loss: 0.0371\n",
      "Epoch 305/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1274 - val_loss: 0.1409\n",
      "Epoch 306/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1248 - val_loss: 0.0380\n",
      "Epoch 307/1000\n",
      "103/103 [==============================] - 0s 981us/step - loss: 0.1296 - val_loss: 0.2208\n",
      "Epoch 308/1000\n",
      "103/103 [==============================] - 0s 962us/step - loss: 0.1364 - val_loss: 0.1706\n",
      "Epoch 309/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1227 - val_loss: 0.1223\n",
      "Epoch 310/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1183 - val_loss: 0.0695\n",
      "Epoch 311/1000\n",
      "103/103 [==============================] - 0s 788us/step - loss: 0.1175 - val_loss: 0.0545\n",
      "Epoch 312/1000\n",
      "103/103 [==============================] - 0s 793us/step - loss: 0.1176 - val_loss: 0.0539\n",
      "Epoch 313/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 716us/step - loss: 0.1165 - val_loss: 0.0516\n",
      "Epoch 314/1000\n",
      "103/103 [==============================] - 0s 746us/step - loss: 0.1165 - val_loss: 0.0794\n",
      "Epoch 315/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1237 - val_loss: 0.0373\n",
      "Epoch 316/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1341 - val_loss: 0.2309\n",
      "Epoch 317/1000\n",
      "103/103 [==============================] - 0s 915us/step - loss: 0.1370 - val_loss: 0.1820\n",
      "Epoch 318/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1230 - val_loss: 0.1252\n",
      "Epoch 319/1000\n",
      "103/103 [==============================] - 0s 966us/step - loss: 0.1165 - val_loss: 0.1332\n",
      "Epoch 320/1000\n",
      "103/103 [==============================] - 0s 923us/step - loss: 0.1152 - val_loss: 0.0948\n",
      "Epoch 321/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1138 - val_loss: 0.0684\n",
      "Epoch 322/1000\n",
      "103/103 [==============================] - 0s 938us/step - loss: 0.1135 - val_loss: 0.0419\n",
      "Epoch 323/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1161 - val_loss: 0.1537\n",
      "Epoch 324/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1196 - val_loss: 0.0359\n",
      "Epoch 325/1000\n",
      "103/103 [==============================] - 0s 795us/step - loss: 0.1276 - val_loss: 0.1915\n",
      "Epoch 326/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1212 - val_loss: 0.0899\n",
      "Epoch 327/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1115 - val_loss: 0.0718\n",
      "Epoch 328/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.1785\n",
      "Epoch 329/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1266 - val_loss: 0.1641\n",
      "Epoch 330/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1157 - val_loss: 0.1116\n",
      "Epoch 331/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1121 - val_loss: 0.0597\n",
      "Epoch 332/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 0.1225\n",
      "Epoch 333/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1137 - val_loss: 0.0357\n",
      "Epoch 334/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1259 - val_loss: 0.1837\n",
      "Epoch 335/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1158 - val_loss: 0.0515\n",
      "Epoch 336/1000\n",
      "103/103 [==============================] - 0s 865us/step - loss: 0.1082 - val_loss: 0.0767\n",
      "Epoch 337/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1088 - val_loss: 0.0398\n",
      "Epoch 338/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1184 - val_loss: 0.1502\n",
      "Epoch 339/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1155 - val_loss: 0.0546\n",
      "Epoch 340/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1086 - val_loss: 0.1494\n",
      "Epoch 341/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1108 - val_loss: 0.0878\n",
      "Epoch 342/1000\n",
      "103/103 [==============================] - 0s 944us/step - loss: 0.1144 - val_loss: 0.2356\n",
      "Epoch 343/1000\n",
      "103/103 [==============================] - 0s 924us/step - loss: 0.1260 - val_loss: 0.1339\n",
      "Epoch 344/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1102 - val_loss: 0.1260\n",
      "Epoch 345/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1084 - val_loss: 0.0442\n",
      "Epoch 346/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1101 - val_loss: 0.1132\n",
      "Epoch 347/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1127 - val_loss: 0.0368\n",
      "Epoch 348/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1205 - val_loss: 0.1635\n",
      "Epoch 349/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1093 - val_loss: 0.0467\n",
      "Epoch 350/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1047 - val_loss: 0.0824\n",
      "Epoch 351/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.0372\n",
      "Epoch 352/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1146 - val_loss: 0.1563\n",
      "Epoch 353/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1117 - val_loss: 0.0486\n",
      "Epoch 354/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1052 - val_loss: 0.1342\n",
      "Epoch 355/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1062 - val_loss: 0.0473\n",
      "Epoch 356/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1137 - val_loss: 0.2062\n",
      "Epoch 357/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1268 - val_loss: 0.1561\n",
      "Epoch 358/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1075 - val_loss: 0.1400\n",
      "Epoch 359/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1030 - val_loss: 0.0560\n",
      "Epoch 360/1000\n",
      "103/103 [==============================] - 0s 933us/step - loss: 0.1020 - val_loss: 0.0547\n",
      "Epoch 361/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1025 - val_loss: 0.0350\n",
      "Epoch 362/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1085 - val_loss: 0.1756\n",
      "Epoch 363/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1152 - val_loss: 0.0350\n",
      "Epoch 364/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1150 - val_loss: 0.1708\n",
      "Epoch 365/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1135 - val_loss: 0.1431\n",
      "Epoch 366/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1036 - val_loss: 0.1077\n",
      "Epoch 367/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0993 - val_loss: 0.0749\n",
      "Epoch 368/1000\n",
      "103/103 [==============================] - 0s 794us/step - loss: 0.1005 - val_loss: 0.0431\n",
      "Epoch 369/1000\n",
      "103/103 [==============================] - 0s 982us/step - loss: 0.1030 - val_loss: 0.0493\n",
      "Epoch 370/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1025 - val_loss: 0.0371\n",
      "Epoch 371/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1048 - val_loss: 0.2345\n",
      "Epoch 372/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1263 - val_loss: 0.1974\n",
      "Epoch 373/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1147 - val_loss: 0.1368\n",
      "Epoch 374/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0996 - val_loss: 0.0637\n",
      "Epoch 375/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0984 - val_loss: 0.0794\n",
      "Epoch 376/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0968 - val_loss: 0.0401\n",
      "Epoch 377/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1010 - val_loss: 0.1982\n",
      "Epoch 378/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1087 - val_loss: 0.0351\n",
      "Epoch 379/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1125 - val_loss: 0.1795\n",
      "Epoch 380/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1037 - val_loss: 0.0769\n",
      "Epoch 381/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0968 - val_loss: 0.1119\n",
      "Epoch 382/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0987 - val_loss: 0.0433\n",
      "Epoch 383/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1047 - val_loss: 0.1065\n",
      "Epoch 384/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1042 - val_loss: 0.0370\n",
      "Epoch 385/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1082 - val_loss: 0.1949\n",
      "Epoch 386/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1128 - val_loss: 0.1612\n",
      "Epoch 387/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1035 - val_loss: 0.1472\n",
      "Epoch 388/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.0492\n",
      "Epoch 389/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0951 - val_loss: 0.0702\n",
      "Epoch 390/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0988 - val_loss: 0.0359\n",
      "Epoch 391/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1037 - val_loss: 0.1771\n",
      "Epoch 392/1000\n",
      "103/103 [==============================] - 0s 977us/step - loss: 0.1056 - val_loss: 0.0381\n",
      "Epoch 393/1000\n",
      "103/103 [==============================] - 0s 984us/step - loss: 0.1071 - val_loss: 0.1314\n",
      "Epoch 394/1000\n",
      "103/103 [==============================] - 0s 964us/step - loss: 0.0981 - val_loss: 0.0426\n",
      "Epoch 395/1000\n",
      "103/103 [==============================] - 0s 720us/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 396/1000\n",
      "103/103 [==============================] - 0s 811us/step - loss: 0.0951 - val_loss: 0.0384\n",
      "Epoch 397/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1010 - val_loss: 0.1631\n",
      "Epoch 398/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0988 - val_loss: 0.0723\n",
      "Epoch 399/1000\n",
      "103/103 [==============================] - 0s 940us/step - loss: 0.1022 - val_loss: 0.2404\n",
      "Epoch 400/1000\n",
      "103/103 [==============================] - 0s 947us/step - loss: 0.1163 - val_loss: 0.1472\n",
      "Epoch 401/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0995 - val_loss: 0.1167\n",
      "Epoch 402/1000\n",
      "103/103 [==============================] - 0s 664us/step - loss: 0.0961 - val_loss: 0.0395\n",
      "Epoch 403/1000\n",
      "103/103 [==============================] - 0s 942us/step - loss: 0.0974 - val_loss: 0.0966\n",
      "Epoch 404/1000\n",
      "103/103 [==============================] - 0s 995us/step - loss: 0.0960 - val_loss: 0.0376\n",
      "Epoch 405/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0989 - val_loss: 0.1727\n",
      "Epoch 406/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0972 - val_loss: 0.0526\n",
      "Epoch 407/1000\n",
      "103/103 [==============================] - 0s 654us/step - loss: 0.0964 - val_loss: 0.0752\n",
      "Epoch 408/1000\n",
      "103/103 [==============================] - 0s 836us/step - loss: 0.0923 - val_loss: 0.0367\n",
      "Epoch 409/1000\n",
      "103/103 [==============================] - 0s 726us/step - loss: 0.0972 - val_loss: 0.1703\n",
      "Epoch 410/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1004 - val_loss: 0.0458\n",
      "Epoch 411/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0932 - val_loss: 0.1406\n",
      "Epoch 412/1000\n",
      "103/103 [==============================] - 0s 967us/step - loss: 0.0947 - val_loss: 0.0399\n",
      "Epoch 413/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1017 - val_loss: 0.1179\n",
      "Epoch 414/1000\n",
      "103/103 [==============================] - 0s 844us/step - loss: 0.0964 - val_loss: 0.0375\n",
      "Epoch 415/1000\n",
      "103/103 [==============================] - 0s 900us/step - loss: 0.0973 - val_loss: 0.1116\n",
      "Epoch 416/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0924 - val_loss: 0.0379\n",
      "Epoch 417/1000\n",
      "103/103 [==============================] - 0s 876us/step - loss: 0.0955 - val_loss: 0.1396\n",
      "Epoch 418/1000\n",
      "103/103 [==============================] - 0s 868us/step - loss: 0.1147 - val_loss: 0.2237\n",
      "Epoch 419/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1045 - val_loss: 0.0908\n",
      "Epoch 420/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0890 - val_loss: 0.0533\n",
      "Epoch 421/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0873 - val_loss: 0.0749\n",
      "Epoch 422/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0872 - val_loss: 0.0473\n",
      "Epoch 423/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0896 - val_loss: 0.1334\n",
      "Epoch 424/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0920 - val_loss: 0.0510\n",
      "Epoch 425/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0899 - val_loss: 0.0855\n",
      "Epoch 426/1000\n",
      "103/103 [==============================] - 0s 807us/step - loss: 0.0985 - val_loss: 0.0345\n",
      "Epoch 427/1000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.1022 - val_loss: 0.2011\n",
      "Epoch 428/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1179 - val_loss: 0.1984\n",
      "Epoch 429/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1006 - val_loss: 0.0675\n",
      "Epoch 430/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0860 - val_loss: 0.0635\n",
      "Epoch 431/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0700\n",
      "Epoch 432/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0842 - val_loss: 0.0598\n",
      "Epoch 433/1000\n",
      "103/103 [==============================] - 0s 754us/step - loss: 0.0846 - val_loss: 0.0631\n",
      "Epoch 434/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0855 - val_loss: 0.0432\n",
      "Epoch 435/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0869 - val_loss: 0.1248\n",
      "Epoch 436/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1140 - val_loss: 0.1835\n",
      "Epoch 437/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0977 - val_loss: 0.1140\n",
      "Epoch 438/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0906 - val_loss: 0.0833\n",
      "Epoch 439/1000\n",
      "103/103 [==============================] - 0s 971us/step - loss: 0.0856 - val_loss: 0.0559\n",
      "Epoch 440/1000\n",
      "103/103 [==============================] - 0s 846us/step - loss: 0.0829 - val_loss: 0.0681\n",
      "Epoch 441/1000\n",
      "103/103 [==============================] - 0s 937us/step - loss: 0.0832 - val_loss: 0.0358\n",
      "Epoch 442/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0864 - val_loss: 0.1014\n",
      "Epoch 443/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0955 - val_loss: 0.0423\n",
      "Epoch 444/1000\n",
      "103/103 [==============================] - 0s 959us/step - loss: 0.1016 - val_loss: 0.1432\n",
      "Epoch 445/1000\n",
      "103/103 [==============================] - 0s 944us/step - loss: 0.1020 - val_loss: 0.1920\n",
      "Epoch 446/1000\n",
      "103/103 [==============================] - 0s 955us/step - loss: 0.0946 - val_loss: 0.0762\n",
      "Epoch 447/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0832 - val_loss: 0.0561\n",
      "Epoch 448/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0819 - val_loss: 0.0764\n",
      "Epoch 449/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0818 - val_loss: 0.0743\n",
      "Epoch 450/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0816 - val_loss: 0.1293\n",
      "Epoch 451/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0857 - val_loss: 0.0487\n",
      "Epoch 452/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0911 - val_loss: 0.0598\n",
      "Epoch 453/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0903 - val_loss: 0.0425\n",
      "Epoch 454/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0905 - val_loss: 0.1598\n",
      "Epoch 455/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1086 - val_loss: 0.2009\n",
      "Epoch 456/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0957 - val_loss: 0.0802\n",
      "Epoch 457/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0821 - val_loss: 0.0644\n",
      "Epoch 458/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0797 - val_loss: 0.0610\n",
      "Epoch 459/1000\n",
      "103/103 [==============================] - 0s 912us/step - loss: 0.0790 - val_loss: 0.0667\n",
      "Epoch 460/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0792 - val_loss: 0.0992\n",
      "Epoch 461/1000\n",
      "103/103 [==============================] - 0s 949us/step - loss: 0.0819 - val_loss: 0.0483\n",
      "Epoch 462/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 0.0741\n",
      "Epoch 463/1000\n",
      "103/103 [==============================] - 0s 993us/step - loss: 0.0942 - val_loss: 0.0504\n",
      "Epoch 464/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1088 - val_loss: 0.1580\n",
      "Epoch 465/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1000 - val_loss: 0.1519\n",
      "Epoch 466/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 0.0699\n",
      "Epoch 467/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0793 - val_loss: 0.0646\n",
      "Epoch 468/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0781 - val_loss: 0.0603\n",
      "Epoch 469/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0782 - val_loss: 0.0623\n",
      "Epoch 470/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0782 - val_loss: 0.0533\n",
      "Epoch 471/1000\n",
      "103/103 [==============================] - 0s 832us/step - loss: 0.0788 - val_loss: 0.0697\n",
      "Epoch 472/1000\n",
      "103/103 [==============================] - 0s 876us/step - loss: 0.0809 - val_loss: 0.0497\n",
      "Epoch 473/1000\n",
      "103/103 [==============================] - 0s 906us/step - loss: 0.0890 - val_loss: 0.2341\n",
      "Epoch 474/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1273 - val_loss: 0.1630\n",
      "Epoch 475/1000\n",
      "103/103 [==============================] - 0s 816us/step - loss: 0.0887 - val_loss: 0.0461\n",
      "Epoch 476/1000\n",
      "103/103 [==============================] - 0s 742us/step - loss: 0.0811 - val_loss: 0.0925\n",
      "Epoch 477/1000\n",
      "103/103 [==============================] - 0s 833us/step - loss: 0.0779 - val_loss: 0.0477\n",
      "Epoch 478/1000\n",
      "103/103 [==============================] - 0s 826us/step - loss: 0.0781 - val_loss: 0.1402\n",
      "Epoch 479/1000\n",
      "103/103 [==============================] - 0s 862us/step - loss: 0.0802 - val_loss: 0.0793\n",
      "Epoch 480/1000\n",
      "103/103 [==============================] - 0s 921us/step - loss: 0.0780 - val_loss: 0.1209\n",
      "Epoch 481/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0936 - val_loss: 0.1995\n",
      "Epoch 482/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0903 - val_loss: 0.0772\n",
      "Epoch 483/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0757 - val_loss: 0.0636\n",
      "Epoch 484/1000\n",
      "103/103 [==============================] - 0s 932us/step - loss: 0.0793 - val_loss: 0.0663\n",
      "Epoch 485/1000\n",
      "103/103 [==============================] - 0s 978us/step - loss: 0.0807 - val_loss: 0.0471\n",
      "Epoch 486/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.1623\n",
      "Epoch 487/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0847 - val_loss: 0.1189\n",
      "Epoch 488/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 0.1838\n",
      "Epoch 489/1000\n",
      "103/103 [==============================] - 0s 977us/step - loss: 0.0844 - val_loss: 0.1465\n",
      "Epoch 490/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0804 - val_loss: 0.1174\n",
      "Epoch 491/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0768 - val_loss: 0.0477\n",
      "Epoch 492/1000\n",
      "103/103 [==============================] - 0s 894us/step - loss: 0.0831 - val_loss: 0.1078\n",
      "Epoch 493/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0861 - val_loss: 0.0493\n",
      "Epoch 494/1000\n",
      "103/103 [==============================] - 0s 1000us/step - loss: 0.0804 - val_loss: 0.1403\n",
      "Epoch 495/1000\n",
      "103/103 [==============================] - 0s 990us/step - loss: 0.0796 - val_loss: 0.0639\n",
      "Epoch 496/1000\n",
      "103/103 [==============================] - 0s 964us/step - loss: 0.0772 - val_loss: 0.1095\n",
      "Epoch 497/1000\n",
      "103/103 [==============================] - 0s 963us/step - loss: 0.0784 - val_loss: 0.0436\n",
      "Epoch 498/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0814 - val_loss: 0.1642\n",
      "Epoch 499/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0898 - val_loss: 0.1384\n",
      "Epoch 500/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0796 - val_loss: 0.1097\n",
      "Epoch 501/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0758 - val_loss: 0.0524\n",
      "Epoch 502/1000\n",
      "103/103 [==============================] - 0s 663us/step - loss: 0.0736 - val_loss: 0.0769\n",
      "Epoch 503/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0750 - val_loss: 0.0611\n",
      "Epoch 504/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0824 - val_loss: 0.1504\n",
      "Epoch 505/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0903 - val_loss: 0.0501\n",
      "Epoch 506/1000\n",
      "103/103 [==============================] - 0s 869us/step - loss: 0.0878 - val_loss: 0.1515\n",
      "Epoch 507/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0759 - val_loss: 0.0503\n",
      "Epoch 508/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0737 - val_loss: 0.0943\n",
      "Epoch 509/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0725 - val_loss: 0.0491\n",
      "Epoch 510/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0744 - val_loss: 0.1244\n",
      "Epoch 511/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1026 - val_loss: 0.1810\n",
      "Epoch 512/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0801 - val_loss: 0.0584\n",
      "Epoch 513/1000\n",
      "103/103 [==============================] - 0s 872us/step - loss: 0.0721 - val_loss: 0.0840\n",
      "Epoch 514/1000\n",
      "103/103 [==============================] - 0s 880us/step - loss: 0.0720 - val_loss: 0.0568\n",
      "Epoch 515/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0715 - val_loss: 0.1184\n",
      "Epoch 516/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0723 - val_loss: 0.0479\n",
      "Epoch 517/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0778 - val_loss: 0.1826\n",
      "Epoch 518/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0834 - val_loss: 0.0491\n",
      "Epoch 519/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0793 - val_loss: 0.0973\n",
      "Epoch 520/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0786 - val_loss: 0.0504\n",
      "Epoch 521/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0721 - val_loss: 0.1238\n",
      "Epoch 522/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0894 - val_loss: 0.2155\n",
      "Epoch 523/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0856 - val_loss: 0.0795\n",
      "Epoch 524/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0702 - val_loss: 0.0786\n",
      "Epoch 525/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0690 - val_loss: 0.0742\n",
      "Epoch 526/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0680 - val_loss: 0.0714\n",
      "Epoch 527/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0667 - val_loss: 0.0715\n",
      "Epoch 528/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0674 - val_loss: 0.0651\n",
      "Epoch 529/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 0.1147\n",
      "Epoch 530/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0740 - val_loss: 0.0980\n",
      "Epoch 531/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0868 - val_loss: 0.2026\n",
      "Epoch 532/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0792 - val_loss: 0.0525\n",
      "Epoch 533/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0720 - val_loss: 0.1057\n",
      "Epoch 534/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0753 - val_loss: 0.0506\n",
      "Epoch 535/1000\n",
      "103/103 [==============================] - 0s 952us/step - loss: 0.0669 - val_loss: 0.1330\n",
      "Epoch 536/1000\n",
      "103/103 [==============================] - 0s 679us/step - loss: 0.0697 - val_loss: 0.0942\n",
      "Epoch 537/1000\n",
      "103/103 [==============================] - 0s 661us/step - loss: 0.0710 - val_loss: 0.1100\n",
      "Epoch 538/1000\n",
      "103/103 [==============================] - 0s 636us/step - loss: 0.0676 - val_loss: 0.0589\n",
      "Epoch 539/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.1820\n",
      "Epoch 540/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0732 - val_loss: 0.0515\n",
      "Epoch 541/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0719 - val_loss: 0.1470\n",
      "Epoch 542/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0735 - val_loss: 0.0540\n",
      "Epoch 543/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0686 - val_loss: 0.1105\n",
      "Epoch 544/1000\n",
      "103/103 [==============================] - 0s 646us/step - loss: 0.0715 - val_loss: 0.0584\n",
      "Epoch 545/1000\n",
      "103/103 [==============================] - 0s 711us/step - loss: 0.0641 - val_loss: 0.1450\n",
      "Epoch 546/1000\n",
      "103/103 [==============================] - 0s 451us/step - loss: 0.0919 - val_loss: 0.1772\n",
      "Epoch 547/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 445us/step - loss: 0.0743 - val_loss: 0.0792\n",
      "Epoch 548/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0617 - val_loss: 0.0708\n",
      "Epoch 549/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0599 - val_loss: 0.0909\n",
      "Epoch 550/1000\n",
      "103/103 [==============================] - 0s 482us/step - loss: 0.0582 - val_loss: 0.0640\n",
      "Epoch 551/1000\n",
      "103/103 [==============================] - 0s 490us/step - loss: 0.0592 - val_loss: 0.1489\n",
      "Epoch 552/1000\n",
      "103/103 [==============================] - 0s 455us/step - loss: 0.0692 - val_loss: 0.0586\n",
      "Epoch 553/1000\n",
      "103/103 [==============================] - 0s 458us/step - loss: 0.0696 - val_loss: 0.1745\n",
      "Epoch 554/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0629 - val_loss: 0.0501\n",
      "Epoch 555/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0668 - val_loss: 0.1443\n",
      "Epoch 556/1000\n",
      "103/103 [==============================] - 0s 442us/step - loss: 0.0699 - val_loss: 0.0561\n",
      "Epoch 557/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0654 - val_loss: 0.1278\n",
      "Epoch 558/1000\n",
      "103/103 [==============================] - 0s 709us/step - loss: 0.0731 - val_loss: 0.0548\n",
      "Epoch 559/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0600 - val_loss: 0.1146\n",
      "Epoch 560/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0619 - val_loss: 0.0538\n",
      "Epoch 561/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0620 - val_loss: 0.1800\n",
      "Epoch 562/1000\n",
      "103/103 [==============================] - 0s 470us/step - loss: 0.0625 - val_loss: 0.0599\n",
      "Epoch 563/1000\n",
      "103/103 [==============================] - 0s 744us/step - loss: 0.0588 - val_loss: 0.1465\n",
      "Epoch 564/1000\n",
      "103/103 [==============================] - 0s 705us/step - loss: 0.0666 - val_loss: 0.0587\n",
      "Epoch 565/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0693 - val_loss: 0.1166\n",
      "Epoch 566/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0698 - val_loss: 0.0494\n",
      "Epoch 567/1000\n",
      "103/103 [==============================] - 0s 442us/step - loss: 0.0606 - val_loss: 0.1194\n",
      "Epoch 568/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0597 - val_loss: 0.0551\n",
      "Epoch 569/1000\n",
      "103/103 [==============================] - 0s 472us/step - loss: 0.0590 - val_loss: 0.1447\n",
      "Epoch 570/1000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0594 - val_loss: 0.0438\n",
      "Epoch 571/1000\n",
      "103/103 [==============================] - 0s 478us/step - loss: 0.0620 - val_loss: 0.1551\n",
      "Epoch 572/1000\n",
      "103/103 [==============================] - 0s 485us/step - loss: 0.0664 - val_loss: 0.0502\n",
      "Epoch 573/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0658 - val_loss: 0.1077\n",
      "Epoch 574/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0645 - val_loss: 0.0473\n",
      "Epoch 575/1000\n",
      "103/103 [==============================] - 0s 450us/step - loss: 0.0588 - val_loss: 0.1125\n",
      "Epoch 576/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0574 - val_loss: 0.0630\n",
      "Epoch 577/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0568 - val_loss: 0.1541\n",
      "Epoch 578/1000\n",
      "103/103 [==============================] - 0s 761us/step - loss: 0.0687 - val_loss: 0.1849\n",
      "Epoch 579/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0741 - val_loss: 0.0670\n",
      "Epoch 580/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0526 - val_loss: 0.0827\n",
      "Epoch 581/1000\n",
      "103/103 [==============================] - 0s 605us/step - loss: 0.0510 - val_loss: 0.0687\n",
      "Epoch 582/1000\n",
      "103/103 [==============================] - 0s 672us/step - loss: 0.0508 - val_loss: 0.0985\n",
      "Epoch 583/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0508 - val_loss: 0.0562\n",
      "Epoch 584/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0538 - val_loss: 0.1174\n",
      "Epoch 585/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0582 - val_loss: 0.0458\n",
      "Epoch 586/1000\n",
      "103/103 [==============================] - 0s 593us/step - loss: 0.0620 - val_loss: 0.1588\n",
      "Epoch 587/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0599 - val_loss: 0.0435\n",
      "Epoch 588/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0579 - val_loss: 0.1354\n",
      "Epoch 589/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0553 - val_loss: 0.0536\n",
      "Epoch 590/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0547 - val_loss: 0.1502\n",
      "Epoch 591/1000\n",
      "103/103 [==============================] - 0s 662us/step - loss: 0.0546 - val_loss: 0.0459\n",
      "Epoch 592/1000\n",
      "103/103 [==============================] - 0s 469us/step - loss: 0.0565 - val_loss: 0.1276\n",
      "Epoch 593/1000\n",
      "103/103 [==============================] - 0s 491us/step - loss: 0.0646 - val_loss: 0.0499\n",
      "Epoch 594/1000\n",
      "103/103 [==============================] - 0s 490us/step - loss: 0.0608 - val_loss: 0.1108\n",
      "Epoch 595/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0569 - val_loss: 0.0506\n",
      "Epoch 596/1000\n",
      "103/103 [==============================] - 0s 496us/step - loss: 0.0552 - val_loss: 0.1419\n",
      "Epoch 597/1000\n",
      "103/103 [==============================] - 0s 454us/step - loss: 0.0536 - val_loss: 0.0703\n",
      "Epoch 598/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0537 - val_loss: 0.1461\n",
      "Epoch 599/1000\n",
      "103/103 [==============================] - 0s 475us/step - loss: 0.0526 - val_loss: 0.0523\n",
      "Epoch 600/1000\n",
      "103/103 [==============================] - 0s 462us/step - loss: 0.0551 - val_loss: 0.1585\n",
      "Epoch 601/1000\n",
      "103/103 [==============================] - 0s 469us/step - loss: 0.0586 - val_loss: 0.0466\n",
      "Epoch 602/1000\n",
      "103/103 [==============================] - 0s 450us/step - loss: 0.0598 - val_loss: 0.0941\n",
      "Epoch 603/1000\n",
      "103/103 [==============================] - 0s 493us/step - loss: 0.0559 - val_loss: 0.0464\n",
      "Epoch 604/1000\n",
      "103/103 [==============================] - 0s 474us/step - loss: 0.0528 - val_loss: 0.1349\n",
      "Epoch 605/1000\n",
      "103/103 [==============================] - 0s 445us/step - loss: 0.0519 - val_loss: 0.0577\n",
      "Epoch 606/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0544 - val_loss: 0.1667\n",
      "Epoch 607/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0532 - val_loss: 0.0557\n",
      "Epoch 608/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.1506\n",
      "Epoch 609/1000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0525 - val_loss: 0.0450\n",
      "Epoch 610/1000\n",
      "103/103 [==============================] - 0s 440us/step - loss: 0.0561 - val_loss: 0.0948\n",
      "Epoch 611/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0583 - val_loss: 0.0432\n",
      "Epoch 612/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0535 - val_loss: 0.1158\n",
      "Epoch 613/1000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0501 - val_loss: 0.0652\n",
      "Epoch 614/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0546 - val_loss: 0.1762\n",
      "Epoch 615/1000\n",
      "103/103 [==============================] - 0s 492us/step - loss: 0.0522 - val_loss: 0.0714\n",
      "Epoch 616/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0505 - val_loss: 0.1306\n",
      "Epoch 617/1000\n",
      "103/103 [==============================] - 0s 451us/step - loss: 0.0482 - val_loss: 0.0495\n",
      "Epoch 618/1000\n",
      "103/103 [==============================] - 0s 442us/step - loss: 0.0516 - val_loss: 0.1201\n",
      "Epoch 619/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0576 - val_loss: 0.0470\n",
      "Epoch 620/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0538 - val_loss: 0.1210\n",
      "Epoch 621/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0515 - val_loss: 0.0555\n",
      "Epoch 622/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0518 - val_loss: 0.1213\n",
      "Epoch 623/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0473 - val_loss: 0.0740\n",
      "Epoch 624/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 438us/step - loss: 0.0691 - val_loss: 0.1409\n",
      "Epoch 625/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0552 - val_loss: 0.1191\n",
      "Epoch 626/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0585 - val_loss: 0.0913\n",
      "Epoch 627/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0448 - val_loss: 0.0577\n",
      "Epoch 628/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0418 - val_loss: 0.0701\n",
      "Epoch 629/1000\n",
      "103/103 [==============================] - 0s 412us/step - loss: 0.0413 - val_loss: 0.0602\n",
      "Epoch 630/1000\n",
      "103/103 [==============================] - 0s 410us/step - loss: 0.0432 - val_loss: 0.0843\n",
      "Epoch 631/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0473 - val_loss: 0.0576\n",
      "Epoch 632/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0621 - val_loss: 0.1243\n",
      "Epoch 633/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0467 - val_loss: 0.0515\n",
      "Epoch 634/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0506 - val_loss: 0.1259\n",
      "Epoch 635/1000\n",
      "103/103 [==============================] - 0s 471us/step - loss: 0.0444 - val_loss: 0.0592\n",
      "Epoch 636/1000\n",
      "103/103 [==============================] - 0s 475us/step - loss: 0.0447 - val_loss: 0.1110\n",
      "Epoch 637/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0557 - val_loss: 0.0473\n",
      "Epoch 638/1000\n",
      "103/103 [==============================] - 0s 443us/step - loss: 0.0519 - val_loss: 0.1214\n",
      "Epoch 639/1000\n",
      "103/103 [==============================] - 0s 460us/step - loss: 0.0455 - val_loss: 0.0747\n",
      "Epoch 640/1000\n",
      "103/103 [==============================] - 0s 446us/step - loss: 0.0554 - val_loss: 0.1235\n",
      "Epoch 641/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0419 - val_loss: 0.0563\n",
      "Epoch 642/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0427 - val_loss: 0.1046\n",
      "Epoch 643/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0437 - val_loss: 0.0504\n",
      "Epoch 644/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0541 - val_loss: 0.1525\n",
      "Epoch 645/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0462 - val_loss: 0.0730\n",
      "Epoch 646/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0429 - val_loss: 0.1021\n",
      "Epoch 647/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0446 - val_loss: 0.0688\n",
      "Epoch 648/1000\n",
      "103/103 [==============================] - 0s 440us/step - loss: 0.0532 - val_loss: 0.1478\n",
      "Epoch 649/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0426 - val_loss: 0.0670\n",
      "Epoch 650/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0460 - val_loss: 0.1509\n",
      "Epoch 651/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0478 - val_loss: 0.0504\n",
      "Epoch 652/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0505 - val_loss: 0.1166\n",
      "Epoch 653/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0425 - val_loss: 0.0768\n",
      "Epoch 654/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0411 - val_loss: 0.0822\n",
      "Epoch 655/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0437 - val_loss: 0.0552\n",
      "Epoch 656/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0503 - val_loss: 0.1522\n",
      "Epoch 657/1000\n",
      "103/103 [==============================] - 0s 464us/step - loss: 0.0447 - val_loss: 0.0834\n",
      "Epoch 658/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.1538\n",
      "Epoch 659/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0555\n",
      "Epoch 660/1000\n",
      "103/103 [==============================] - 0s 489us/step - loss: 0.0440 - val_loss: 0.1526\n",
      "Epoch 661/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.0709\n",
      "Epoch 662/1000\n",
      "103/103 [==============================] - 0s 601us/step - loss: 0.0451 - val_loss: 0.1286\n",
      "Epoch 663/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0475 - val_loss: 0.0543\n",
      "Epoch 664/1000\n",
      "103/103 [==============================] - 0s 412us/step - loss: 0.0473 - val_loss: 0.1076\n",
      "Epoch 665/1000\n",
      "103/103 [==============================] - 0s 464us/step - loss: 0.0389 - val_loss: 0.0875\n",
      "Epoch 666/1000\n",
      "103/103 [==============================] - 0s 469us/step - loss: 0.0427 - val_loss: 0.1038\n",
      "Epoch 667/1000\n",
      "103/103 [==============================] - 0s 475us/step - loss: 0.0428 - val_loss: 0.0560\n",
      "Epoch 668/1000\n",
      "103/103 [==============================] - 0s 520us/step - loss: 0.0459 - val_loss: 0.0959\n",
      "Epoch 669/1000\n",
      "103/103 [==============================] - 0s 439us/step - loss: 0.0454 - val_loss: 0.0656\n",
      "Epoch 670/1000\n",
      "103/103 [==============================] - 0s 468us/step - loss: 0.0505 - val_loss: 0.1095\n",
      "Epoch 671/1000\n",
      "103/103 [==============================] - 0s 454us/step - loss: 0.0385 - val_loss: 0.0810\n",
      "Epoch 672/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0394 - val_loss: 0.1150\n",
      "Epoch 673/1000\n",
      "103/103 [==============================] - 0s 505us/step - loss: 0.0392 - val_loss: 0.0576\n",
      "Epoch 674/1000\n",
      "103/103 [==============================] - 0s 456us/step - loss: 0.0416 - val_loss: 0.1060\n",
      "Epoch 675/1000\n",
      "103/103 [==============================] - 0s 466us/step - loss: 0.0428 - val_loss: 0.0684\n",
      "Epoch 676/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0475 - val_loss: 0.1623\n",
      "Epoch 677/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0416 - val_loss: 0.0867\n",
      "Epoch 678/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0402 - val_loss: 0.1202\n",
      "Epoch 679/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0398 - val_loss: 0.0572\n",
      "Epoch 680/1000\n",
      "103/103 [==============================] - 0s 884us/step - loss: 0.0415 - val_loss: 0.0976\n",
      "Epoch 681/1000\n",
      "103/103 [==============================] - 0s 798us/step - loss: 0.0402 - val_loss: 0.0593\n",
      "Epoch 682/1000\n",
      "103/103 [==============================] - 0s 814us/step - loss: 0.0430 - val_loss: 0.1360\n",
      "Epoch 683/1000\n",
      "103/103 [==============================] - 0s 505us/step - loss: 0.0397 - val_loss: 0.0721\n",
      "Epoch 684/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0425 - val_loss: 0.1349\n",
      "Epoch 685/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0390 - val_loss: 0.0760\n",
      "Epoch 686/1000\n",
      "103/103 [==============================] - 0s 445us/step - loss: 0.0414 - val_loss: 0.1084\n",
      "Epoch 687/1000\n",
      "103/103 [==============================] - 0s 484us/step - loss: 0.0399 - val_loss: 0.0764\n",
      "Epoch 688/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0405 - val_loss: 0.1033\n",
      "Epoch 689/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0396 - val_loss: 0.1054\n",
      "Epoch 690/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0378 - val_loss: 0.0938\n",
      "Epoch 691/1000\n",
      "103/103 [==============================] - 0s 485us/step - loss: 0.0376 - val_loss: 0.0652\n",
      "Epoch 692/1000\n",
      "103/103 [==============================] - 0s 563us/step - loss: 0.0403 - val_loss: 0.1093\n",
      "Epoch 693/1000\n",
      "103/103 [==============================] - 0s 519us/step - loss: 0.0378 - val_loss: 0.0810\n",
      "Epoch 694/1000\n",
      "103/103 [==============================] - 0s 514us/step - loss: 0.0430 - val_loss: 0.1690\n",
      "Epoch 695/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0384 - val_loss: 0.0829\n",
      "Epoch 696/1000\n",
      "103/103 [==============================] - 0s 480us/step - loss: 0.0393 - val_loss: 0.1634\n",
      "Epoch 697/1000\n",
      "103/103 [==============================] - 0s 471us/step - loss: 0.0418 - val_loss: 0.0810\n",
      "Epoch 698/1000\n",
      "103/103 [==============================] - 0s 667us/step - loss: 0.0562 - val_loss: 0.0795\n",
      "Epoch 699/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0390 - val_loss: 0.0744\n",
      "Epoch 700/1000\n",
      "103/103 [==============================] - 0s 455us/step - loss: 0.0355 - val_loss: 0.1042\n",
      "Epoch 701/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 481us/step - loss: 0.0327 - val_loss: 0.0838\n",
      "Epoch 702/1000\n",
      "103/103 [==============================] - 0s 582us/step - loss: 0.0337 - val_loss: 0.1011\n",
      "Epoch 703/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0341 - val_loss: 0.0677\n",
      "Epoch 704/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0344 - val_loss: 0.0889\n",
      "Epoch 705/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0360 - val_loss: 0.0537\n",
      "Epoch 706/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0370 - val_loss: 0.1757\n",
      "Epoch 707/1000\n",
      "103/103 [==============================] - 0s 454us/step - loss: 0.0403 - val_loss: 0.0636\n",
      "Epoch 708/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0372 - val_loss: 0.1344\n",
      "Epoch 709/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0381 - val_loss: 0.0822\n",
      "Epoch 710/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0395 - val_loss: 0.1405\n",
      "Epoch 711/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0408 - val_loss: 0.0818\n",
      "Epoch 712/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0443 - val_loss: 0.1404\n",
      "Epoch 713/1000\n",
      "103/103 [==============================] - 0s 515us/step - loss: 0.0325 - val_loss: 0.0864\n",
      "Epoch 714/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0320 - val_loss: 0.0978\n",
      "Epoch 715/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0336 - val_loss: 0.0659\n",
      "Epoch 716/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0341 - val_loss: 0.0928\n",
      "Epoch 717/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0353 - val_loss: 0.0784\n",
      "Epoch 718/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0404 - val_loss: 0.1392\n",
      "Epoch 719/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0353 - val_loss: 0.0702\n",
      "Epoch 720/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0391 - val_loss: 0.1843\n",
      "Epoch 721/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0425 - val_loss: 0.0827\n",
      "Epoch 722/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0462 - val_loss: 0.1142\n",
      "Epoch 723/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0308 - val_loss: 0.0813\n",
      "Epoch 724/1000\n",
      "103/103 [==============================] - 0s 410us/step - loss: 0.0317 - val_loss: 0.1102\n",
      "Epoch 725/1000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0305 - val_loss: 0.0781\n",
      "Epoch 726/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0331 - val_loss: 0.1075\n",
      "Epoch 727/1000\n",
      "103/103 [==============================] - 0s 409us/step - loss: 0.0354 - val_loss: 0.0770\n",
      "Epoch 728/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0468 - val_loss: 0.0922\n",
      "Epoch 729/1000\n",
      "103/103 [==============================] - 0s 443us/step - loss: 0.0349 - val_loss: 0.1017\n",
      "Epoch 730/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0320 - val_loss: 0.0860\n",
      "Epoch 731/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0322 - val_loss: 0.0833\n",
      "Epoch 732/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0334 - val_loss: 0.1095\n",
      "Epoch 733/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0350 - val_loss: 0.1066\n",
      "Epoch 734/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0365 - val_loss: 0.0984\n",
      "Epoch 735/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0353 - val_loss: 0.1208\n",
      "Epoch 736/1000\n",
      "103/103 [==============================] - 0s 590us/step - loss: 0.0342 - val_loss: 0.0737\n",
      "Epoch 737/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0301 - val_loss: 0.1090\n",
      "Epoch 738/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0306 - val_loss: 0.0693\n",
      "Epoch 739/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0301 - val_loss: 0.1116\n",
      "Epoch 740/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0304 - val_loss: 0.0643\n",
      "Epoch 741/1000\n",
      "103/103 [==============================] - 0s 405us/step - loss: 0.0351 - val_loss: 0.1648\n",
      "Epoch 742/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0451 - val_loss: 0.0708\n",
      "Epoch 743/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0420 - val_loss: 0.1710\n",
      "Epoch 744/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0324 - val_loss: 0.1014\n",
      "Epoch 745/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0320 - val_loss: 0.1375\n",
      "Epoch 746/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0304 - val_loss: 0.0931\n",
      "Epoch 747/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0295 - val_loss: 0.1108\n",
      "Epoch 748/1000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0293 - val_loss: 0.0860\n",
      "Epoch 749/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0310 - val_loss: 0.1242\n",
      "Epoch 750/1000\n",
      "103/103 [==============================] - 0s 493us/step - loss: 0.0328 - val_loss: 0.0906\n",
      "Epoch 751/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0399 - val_loss: 0.1417\n",
      "Epoch 752/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0327 - val_loss: 0.0751\n",
      "Epoch 753/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0332 - val_loss: 0.1357\n",
      "Epoch 754/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0336 - val_loss: 0.0825\n",
      "Epoch 755/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0303 - val_loss: 0.1218\n",
      "Epoch 756/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0282 - val_loss: 0.0791\n",
      "Epoch 757/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0355 - val_loss: 0.1406\n",
      "Epoch 758/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0336 - val_loss: 0.0740\n",
      "Epoch 759/1000\n",
      "103/103 [==============================] - 0s 598us/step - loss: 0.0321 - val_loss: 0.1366\n",
      "Epoch 760/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0308 - val_loss: 0.0841\n",
      "Epoch 761/1000\n",
      "103/103 [==============================] - 0s 446us/step - loss: 0.0263 - val_loss: 0.1003\n",
      "Epoch 762/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0316 - val_loss: 0.0815\n",
      "Epoch 763/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0369 - val_loss: 0.1357\n",
      "Epoch 764/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0326 - val_loss: 0.0940\n",
      "Epoch 765/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0303 - val_loss: 0.1045\n",
      "Epoch 766/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0311 - val_loss: 0.0830\n",
      "Epoch 767/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0296 - val_loss: 0.1072\n",
      "Epoch 768/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0282 - val_loss: 0.1035\n",
      "Epoch 769/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0309 - val_loss: 0.1328\n",
      "Epoch 770/1000\n",
      "103/103 [==============================] - 0s 589us/step - loss: 0.0299 - val_loss: 0.0859\n",
      "Epoch 771/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0307 - val_loss: 0.0875\n",
      "Epoch 772/1000\n",
      "103/103 [==============================] - 0s 474us/step - loss: 0.0291 - val_loss: 0.0846\n",
      "Epoch 773/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0308 - val_loss: 0.1660\n",
      "Epoch 774/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0310 - val_loss: 0.1213\n",
      "Epoch 775/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0285 - val_loss: 0.1429\n",
      "Epoch 776/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0297 - val_loss: 0.0830\n",
      "Epoch 777/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0355 - val_loss: 0.1477\n",
      "Epoch 778/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 430us/step - loss: 0.0341 - val_loss: 0.0916\n",
      "Epoch 779/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0632 - val_loss: 0.1021\n",
      "Epoch 780/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0492 - val_loss: 0.0863\n",
      "Epoch 781/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0306 - val_loss: 0.0909\n",
      "Epoch 782/1000\n",
      "103/103 [==============================] - 0s 609us/step - loss: 0.0262 - val_loss: 0.0990\n",
      "Epoch 783/1000\n",
      "103/103 [==============================] - 0s 644us/step - loss: 0.0240 - val_loss: 0.1049\n",
      "Epoch 784/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0228 - val_loss: 0.0939\n",
      "Epoch 785/1000\n",
      "103/103 [==============================] - 0s 439us/step - loss: 0.0235 - val_loss: 0.0998\n",
      "Epoch 786/1000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0258 - val_loss: 0.1139\n",
      "Epoch 787/1000\n",
      "103/103 [==============================] - 0s 612us/step - loss: 0.0298 - val_loss: 0.0705\n",
      "Epoch 788/1000\n",
      "103/103 [==============================] - 0s 746us/step - loss: 0.0317 - val_loss: 0.1070\n",
      "Epoch 789/1000\n",
      "103/103 [==============================] - 0s 829us/step - loss: 0.0304 - val_loss: 0.0898\n",
      "Epoch 790/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0298 - val_loss: 0.1250\n",
      "Epoch 791/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0294 - val_loss: 0.1058\n",
      "Epoch 792/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0309 - val_loss: 0.1704\n",
      "Epoch 793/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0273 - val_loss: 0.1002\n",
      "Epoch 794/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0259 - val_loss: 0.1384\n",
      "Epoch 795/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0246 - val_loss: 0.1056\n",
      "Epoch 796/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0307 - val_loss: 0.1679\n",
      "Epoch 797/1000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0272 - val_loss: 0.1005\n",
      "Epoch 798/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0282 - val_loss: 0.1259\n",
      "Epoch 799/1000\n",
      "103/103 [==============================] - 0s 504us/step - loss: 0.0271 - val_loss: 0.0838\n",
      "Epoch 800/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0284 - val_loss: 0.1195\n",
      "Epoch 801/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0300 - val_loss: 0.0963\n",
      "Epoch 802/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0362 - val_loss: 0.1211\n",
      "Epoch 803/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0277 - val_loss: 0.0909\n",
      "Epoch 804/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0230 - val_loss: 0.1016\n",
      "Epoch 805/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0226 - val_loss: 0.1079\n",
      "Epoch 806/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0287 - val_loss: 0.1247\n",
      "Epoch 807/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0257 - val_loss: 0.0887\n",
      "Epoch 808/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0242 - val_loss: 0.1201\n",
      "Epoch 809/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0288 - val_loss: 0.0872\n",
      "Epoch 810/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0303 - val_loss: 0.0974\n",
      "Epoch 811/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0311 - val_loss: 0.0812\n",
      "Epoch 812/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0263 - val_loss: 0.1654\n",
      "Epoch 813/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0237 - val_loss: 0.0951\n",
      "Epoch 814/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0229 - val_loss: 0.1281\n",
      "Epoch 815/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0245 - val_loss: 0.0801\n",
      "Epoch 816/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0241 - val_loss: 0.1241\n",
      "Epoch 817/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0228 - val_loss: 0.0723\n",
      "Epoch 818/1000\n",
      "103/103 [==============================] - 0s 445us/step - loss: 0.0306 - val_loss: 0.1537\n",
      "Epoch 819/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0272 - val_loss: 0.0793\n",
      "Epoch 820/1000\n",
      "103/103 [==============================] - 0s 445us/step - loss: 0.0223 - val_loss: 0.0966\n",
      "Epoch 821/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0282 - val_loss: 0.0814\n",
      "Epoch 822/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0281 - val_loss: 0.1025\n",
      "Epoch 823/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0249 - val_loss: 0.1409\n",
      "Epoch 824/1000\n",
      "103/103 [==============================] - 0s 522us/step - loss: 0.0236 - val_loss: 0.1255\n",
      "Epoch 825/1000\n",
      "103/103 [==============================] - 0s 826us/step - loss: 0.0232 - val_loss: 0.1229\n",
      "Epoch 826/1000\n",
      "103/103 [==============================] - 0s 840us/step - loss: 0.0212 - val_loss: 0.1067\n",
      "Epoch 827/1000\n",
      "103/103 [==============================] - 0s 530us/step - loss: 0.0213 - val_loss: 0.0946\n",
      "Epoch 828/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0239 - val_loss: 0.1436\n",
      "Epoch 829/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0293 - val_loss: 0.0910\n",
      "Epoch 830/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0281 - val_loss: 0.1487\n",
      "Epoch 831/1000\n",
      "103/103 [==============================] - 0s 473us/step - loss: 0.0432 - val_loss: 0.1029\n",
      "Epoch 832/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0602 - val_loss: 0.0956\n",
      "Epoch 833/1000\n",
      "103/103 [==============================] - 0s 570us/step - loss: 0.0341 - val_loss: 0.0913\n",
      "Epoch 834/1000\n",
      "103/103 [==============================] - 0s 828us/step - loss: 0.0223 - val_loss: 0.1043\n",
      "Epoch 835/1000\n",
      "103/103 [==============================] - 0s 716us/step - loss: 0.0194 - val_loss: 0.0939\n",
      "Epoch 836/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0181 - val_loss: 0.0971\n",
      "Epoch 837/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0185 - val_loss: 0.0916\n",
      "Epoch 838/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0192 - val_loss: 0.1124\n",
      "Epoch 839/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0204 - val_loss: 0.0678\n",
      "Epoch 840/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0260 - val_loss: 0.1436\n",
      "Epoch 841/1000\n",
      "103/103 [==============================] - 0s 410us/step - loss: 0.0259 - val_loss: 0.0879\n",
      "Epoch 842/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0228 - val_loss: 0.1585\n",
      "Epoch 843/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0245 - val_loss: 0.1133\n",
      "Epoch 844/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0341 - val_loss: 0.1062\n",
      "Epoch 845/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0230 - val_loss: 0.1593\n",
      "Epoch 846/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0233 - val_loss: 0.1028\n",
      "Epoch 847/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0202 - val_loss: 0.1080\n",
      "Epoch 848/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0199 - val_loss: 0.1046\n",
      "Epoch 849/1000\n",
      "103/103 [==============================] - 0s 477us/step - loss: 0.0181 - val_loss: 0.1313\n",
      "Epoch 850/1000\n",
      "103/103 [==============================] - 0s 777us/step - loss: 0.0239 - val_loss: 0.1175\n",
      "Epoch 851/1000\n",
      "103/103 [==============================] - 0s 679us/step - loss: 0.0283 - val_loss: 0.1523\n",
      "Epoch 852/1000\n",
      "103/103 [==============================] - 0s 757us/step - loss: 0.0259 - val_loss: 0.0791\n",
      "Epoch 853/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0264 - val_loss: 0.1729\n",
      "Epoch 854/1000\n",
      "103/103 [==============================] - 0s 474us/step - loss: 0.0269 - val_loss: 0.1117\n",
      "Epoch 855/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 420us/step - loss: 0.0456 - val_loss: 0.1016\n",
      "Epoch 856/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0267 - val_loss: 0.0849\n",
      "Epoch 857/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0187 - val_loss: 0.1146\n",
      "Epoch 858/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0174 - val_loss: 0.0980\n",
      "Epoch 859/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0167 - val_loss: 0.1174\n",
      "Epoch 860/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0178 - val_loss: 0.0841\n",
      "Epoch 861/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0198 - val_loss: 0.1059\n",
      "Epoch 862/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0199 - val_loss: 0.1066\n",
      "Epoch 863/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0275 - val_loss: 0.1927\n",
      "Epoch 864/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0381 - val_loss: 0.0962\n",
      "Epoch 865/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0254 - val_loss: 0.1088\n",
      "Epoch 866/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0185 - val_loss: 0.0973\n",
      "Epoch 867/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0195 - val_loss: 0.1276\n",
      "Epoch 868/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0171 - val_loss: 0.0879\n",
      "Epoch 869/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0203 - val_loss: 0.1490\n",
      "Epoch 870/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0314 - val_loss: 0.1005\n",
      "Epoch 871/1000\n",
      "103/103 [==============================] - 0s 442us/step - loss: 0.0213 - val_loss: 0.1377\n",
      "Epoch 872/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0215 - val_loss: 0.0894\n",
      "Epoch 873/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0245 - val_loss: 0.1312\n",
      "Epoch 874/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0216 - val_loss: 0.1116\n",
      "Epoch 875/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0205 - val_loss: 0.1258\n",
      "Epoch 876/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0218 - val_loss: 0.0982\n",
      "Epoch 877/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0200 - val_loss: 0.1287\n",
      "Epoch 878/1000\n",
      "103/103 [==============================] - 0s 585us/step - loss: 0.0188 - val_loss: 0.0920\n",
      "Epoch 879/1000\n",
      "103/103 [==============================] - 0s 821us/step - loss: 0.0171 - val_loss: 0.0963\n",
      "Epoch 880/1000\n",
      "103/103 [==============================] - 0s 468us/step - loss: 0.0223 - val_loss: 0.1176\n",
      "Epoch 881/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0241 - val_loss: 0.1205\n",
      "Epoch 882/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0205 - val_loss: 0.1030\n",
      "Epoch 883/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0197 - val_loss: 0.1707\n",
      "Epoch 884/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0214 - val_loss: 0.0929\n",
      "Epoch 885/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0230 - val_loss: 0.1639\n",
      "Epoch 886/1000\n",
      "103/103 [==============================] - 0s 606us/step - loss: 0.0247 - val_loss: 0.1013\n",
      "Epoch 887/1000\n",
      "103/103 [==============================] - 0s 849us/step - loss: 0.0215 - val_loss: 0.1309\n",
      "Epoch 888/1000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0201 - val_loss: 0.0935\n",
      "Epoch 889/1000\n",
      "103/103 [==============================] - 0s 412us/step - loss: 0.0198 - val_loss: 0.1433\n",
      "Epoch 890/1000\n",
      "103/103 [==============================] - 0s 477us/step - loss: 0.0180 - val_loss: 0.1093\n",
      "Epoch 891/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0219 - val_loss: 0.1729\n",
      "Epoch 892/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0238 - val_loss: 0.1189\n",
      "Epoch 893/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0227 - val_loss: 0.1199\n",
      "Epoch 894/1000\n",
      "103/103 [==============================] - 0s 688us/step - loss: 0.0194 - val_loss: 0.0909\n",
      "Epoch 895/1000\n",
      "103/103 [==============================] - 0s 920us/step - loss: 0.0191 - val_loss: 0.1457\n",
      "Epoch 896/1000\n",
      "103/103 [==============================] - 0s 675us/step - loss: 0.0179 - val_loss: 0.1024\n",
      "Epoch 897/1000\n",
      "103/103 [==============================] - 0s 439us/step - loss: 0.0194 - val_loss: 0.1325\n",
      "Epoch 898/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0210 - val_loss: 0.0980\n",
      "Epoch 899/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0220 - val_loss: 0.1250\n",
      "Epoch 900/1000\n",
      "103/103 [==============================] - 0s 439us/step - loss: 0.0263 - val_loss: 0.0954\n",
      "Epoch 901/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0225 - val_loss: 0.1387\n",
      "Epoch 902/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0169 - val_loss: 0.1112\n",
      "Epoch 903/1000\n",
      "103/103 [==============================] - 0s 591us/step - loss: 0.0155 - val_loss: 0.1205\n",
      "Epoch 904/1000\n",
      "103/103 [==============================] - 0s 837us/step - loss: 0.0177 - val_loss: 0.1086\n",
      "Epoch 905/1000\n",
      "103/103 [==============================] - 0s 440us/step - loss: 0.0186 - val_loss: 0.1176\n",
      "Epoch 906/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0183 - val_loss: 0.1007\n",
      "Epoch 907/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0204 - val_loss: 0.1403\n",
      "Epoch 908/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0204 - val_loss: 0.0974\n",
      "Epoch 909/1000\n",
      "103/103 [==============================] - 0s 464us/step - loss: 0.0166 - val_loss: 0.1334\n",
      "Epoch 910/1000\n",
      "103/103 [==============================] - 0s 407us/step - loss: 0.0295 - val_loss: 0.0996\n",
      "Epoch 911/1000\n",
      "103/103 [==============================] - 0s 827us/step - loss: 0.0207 - val_loss: 0.1636\n",
      "Epoch 912/1000\n",
      "103/103 [==============================] - 0s 789us/step - loss: 0.0173 - val_loss: 0.1253\n",
      "Epoch 913/1000\n",
      "103/103 [==============================] - 0s 446us/step - loss: 0.0145 - val_loss: 0.1180\n",
      "Epoch 914/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0142 - val_loss: 0.1164\n",
      "Epoch 915/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0165 - val_loss: 0.1481\n",
      "Epoch 916/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0211 - val_loss: 0.1216\n",
      "Epoch 917/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0241 - val_loss: 0.1373\n",
      "Epoch 918/1000\n",
      "103/103 [==============================] - 0s 514us/step - loss: 0.0157 - val_loss: 0.1162\n",
      "Epoch 919/1000\n",
      "103/103 [==============================] - 0s 643us/step - loss: 0.0146 - val_loss: 0.1207\n",
      "Epoch 920/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0173 - val_loss: 0.1120\n",
      "Epoch 921/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0204 - val_loss: 0.1503\n",
      "Epoch 922/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0171 - val_loss: 0.1100\n",
      "Epoch 923/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0210 - val_loss: 0.1129\n",
      "Epoch 924/1000\n",
      "103/103 [==============================] - 0s 471us/step - loss: 0.0297 - val_loss: 0.0973\n",
      "Epoch 925/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0164 - val_loss: 0.1321\n",
      "Epoch 926/1000\n",
      "103/103 [==============================] - 0s 478us/step - loss: 0.0142 - val_loss: 0.1143\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_inputs['X'],\n",
    "          train_inputs['target'],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(valid_inputs['X'], valid_inputs['target']),\n",
    "          callbacks=[earlystop ,best_val],\n",
    "          verbose=1 , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(np.array(history.history['val_loss']))+1\n",
    "model.load_weights(str(sat_var) +'_' +  var_name + '_{:02d}.h5'.format(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.fit(valid_inputs['X'],\n",
    "          valid_inputs['target'],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=100,\n",
    "          callbacks=[earlystop ,best_val],\n",
    "          verbose=1 , shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"6\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"6\" halign=\"left\">sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-25 22:00:00</th>\n",
       "      <td>0.329539415970</td>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>0.931886794099</td>\n",
       "      <td>0.666633367882</td>\n",
       "      <td>0.654660349438</td>\n",
       "      <td>0.593411804245</td>\n",
       "      <td>1.071883468639</td>\n",
       "      <td>0.549662843424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 12:00:00</th>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>0.666633367882</td>\n",
       "      <td>0.654660349438</td>\n",
       "      <td>0.593411804245</td>\n",
       "      <td>1.071883468639</td>\n",
       "      <td>0.549662843424</td>\n",
       "      <td>0.329539415970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 14:00:00</th>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>0.654660349438</td>\n",
       "      <td>0.593411804245</td>\n",
       "      <td>1.071883468639</td>\n",
       "      <td>0.549662843424</td>\n",
       "      <td>0.329539415970</td>\n",
       "      <td>0.018233689588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 16:00:00</th>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>0.593411804245</td>\n",
       "      <td>1.071883468639</td>\n",
       "      <td>0.549662843424</td>\n",
       "      <td>0.329539415970</td>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 18:00:00</th>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>1.071883468639</td>\n",
       "      <td>0.549662843424</td>\n",
       "      <td>0.329539415970</td>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                      target                                 \\\n",
       "feature                          y                                  \n",
       "time step                      t+1            t+2             t+3   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-25 22:00:00 0.329539415970 0.018233689588  0.201057023550   \n",
       "2017-11-26 12:00:00 0.018233689588 0.201057023550  0.145796194985   \n",
       "2017-11-26 14:00:00 0.201057023550 0.145796194985  0.744459196048   \n",
       "2017-11-26 16:00:00 0.145796194985 0.744459196048  0.112178064184   \n",
       "2017-11-26 18:00:00 0.744459196048 0.112178064184 -0.327148903700   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                       t+4             t+5             t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00  0.145796194985  0.744459196048  0.112178064184   \n",
       "2017-11-26 12:00:00  0.744459196048  0.112178064184 -0.327148903700   \n",
       "2017-11-26 14:00:00  0.112178064184 -0.327148903700 -0.628321385686   \n",
       "2017-11-26 16:00:00 -0.327148903700 -0.628321385686 -0.366752336535   \n",
       "2017-11-26 18:00:00 -0.628321385686 -0.366752336535 -0.332675469409   \n",
       "\n",
       "tensor                           X                                \\\n",
       "feature                     sqrt_A                                 \n",
       "time step                      t-5            t-4            t-3   \n",
       "Epoch_Time_of_Clock                                                \n",
       "2017-11-25 22:00:00 0.931886794099 0.666633367882 0.654660349438   \n",
       "2017-11-26 12:00:00 0.666633367882 0.654660349438 0.593411804245   \n",
       "2017-11-26 14:00:00 0.654660349438 0.593411804245 1.071883468639   \n",
       "2017-11-26 16:00:00 0.593411804245 1.071883468639 0.549662843424   \n",
       "2017-11-26 18:00:00 1.071883468639 0.549662843424 0.329539415970   \n",
       "\n",
       "tensor                                                            \n",
       "feature                                                           \n",
       "time step                      t-2            t-1              t  \n",
       "Epoch_Time_of_Clock                                               \n",
       "2017-11-25 22:00:00 0.593411804245 1.071883468639 0.549662843424  \n",
       "2017-11-26 12:00:00 1.071883468639 0.549662843424 0.329539415970  \n",
       "2017-11-26 14:00:00 0.549662843424 0.329539415970 0.018233689588  \n",
       "2017-11-26 16:00:00 0.329539415970 0.018233689588 0.201057023550  \n",
       "2017-11-26 18:00:00 0.018233689588 0.201057023550 0.145796194985  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back_dt = dt.datetime.strptime(test_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1)\n",
    "test = df.copy()[test_start_dt:][['sqrt_A',]]\n",
    "test[[ 'sqrt_A']] = X_scaler.transform(test)\n",
    "test_inputs = TimeSeriesTensor(test, var_name, HORIZON, tensor_structure,freq =None)\n",
    "test_inputs.dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"6\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"6\" halign=\"left\">sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-25 22:00:00</th>\n",
       "      <td>0.329539415970</td>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>0.931886794099</td>\n",
       "      <td>0.666633367882</td>\n",
       "      <td>0.654660349438</td>\n",
       "      <td>0.593411804245</td>\n",
       "      <td>1.071883468639</td>\n",
       "      <td>0.549662843424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 12:00:00</th>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>0.666633367882</td>\n",
       "      <td>0.654660349438</td>\n",
       "      <td>0.593411804245</td>\n",
       "      <td>1.071883468639</td>\n",
       "      <td>0.549662843424</td>\n",
       "      <td>0.329539415970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 14:00:00</th>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>0.654660349438</td>\n",
       "      <td>0.593411804245</td>\n",
       "      <td>1.071883468639</td>\n",
       "      <td>0.549662843424</td>\n",
       "      <td>0.329539415970</td>\n",
       "      <td>0.018233689588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 16:00:00</th>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>0.593411804245</td>\n",
       "      <td>1.071883468639</td>\n",
       "      <td>0.549662843424</td>\n",
       "      <td>0.329539415970</td>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 18:00:00</th>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>1.071883468639</td>\n",
       "      <td>0.549662843424</td>\n",
       "      <td>0.329539415970</td>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 20:00:00</th>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>0.549662843424</td>\n",
       "      <td>0.329539415970</td>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 22:00:00</th>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>0.329539415970</td>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 12:00:00</th>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>0.018233689588</td>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 14:00:00</th>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>-1.147318784535</td>\n",
       "      <td>0.201057023550</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 16:00:00</th>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>-1.147318784535</td>\n",
       "      <td>-0.947457016901</td>\n",
       "      <td>0.145796194985</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 18:00:00</th>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>-1.147318784535</td>\n",
       "      <td>-0.947457016901</td>\n",
       "      <td>-0.764172532240</td>\n",
       "      <td>0.744459196048</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 20:00:00</th>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>-1.147318784535</td>\n",
       "      <td>-0.947457016901</td>\n",
       "      <td>-0.764172532240</td>\n",
       "      <td>-0.113933079899</td>\n",
       "      <td>0.112178064184</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>0.327697227327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 22:00:00</th>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>-1.147318784535</td>\n",
       "      <td>-0.947457016901</td>\n",
       "      <td>-0.764172532240</td>\n",
       "      <td>-0.113933079899</td>\n",
       "      <td>-0.424777655362</td>\n",
       "      <td>-0.327148903700</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>-0.242415472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 12:00:00</th>\n",
       "      <td>-1.147318784535</td>\n",
       "      <td>-0.947457016901</td>\n",
       "      <td>-0.764172532240</td>\n",
       "      <td>-0.113933079899</td>\n",
       "      <td>-0.424777655362</td>\n",
       "      <td>-1.241721895234</td>\n",
       "      <td>-0.628321385686</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>-0.903708056080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 14:00:00</th>\n",
       "      <td>-0.947457016901</td>\n",
       "      <td>-0.764172532240</td>\n",
       "      <td>-0.113933079899</td>\n",
       "      <td>-0.424777655362</td>\n",
       "      <td>-1.241721895234</td>\n",
       "      <td>-1.410268871190</td>\n",
       "      <td>-0.366752336535</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>-1.147318784535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 16:00:00</th>\n",
       "      <td>-0.764172532240</td>\n",
       "      <td>-0.113933079899</td>\n",
       "      <td>-0.424777655362</td>\n",
       "      <td>-1.241721895234</td>\n",
       "      <td>-1.410268871190</td>\n",
       "      <td>-1.375730853365</td>\n",
       "      <td>-0.332675469409</td>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>-1.147318784535</td>\n",
       "      <td>-0.947457016901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 18:00:00</th>\n",
       "      <td>-0.113933079899</td>\n",
       "      <td>-0.424777655362</td>\n",
       "      <td>-1.241721895234</td>\n",
       "      <td>-1.410268871190</td>\n",
       "      <td>-1.375730853365</td>\n",
       "      <td>-1.031268147101</td>\n",
       "      <td>0.327697227327</td>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>-1.147318784535</td>\n",
       "      <td>-0.947457016901</td>\n",
       "      <td>-0.764172532240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 20:00:00</th>\n",
       "      <td>-0.424777655362</td>\n",
       "      <td>-1.241721895234</td>\n",
       "      <td>-1.410268871190</td>\n",
       "      <td>-1.375730853365</td>\n",
       "      <td>-1.031268147101</td>\n",
       "      <td>-0.481419521869</td>\n",
       "      <td>-0.242415472100</td>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>-1.147318784535</td>\n",
       "      <td>-0.947457016901</td>\n",
       "      <td>-0.764172532240</td>\n",
       "      <td>-0.113933079899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:00:00</th>\n",
       "      <td>-1.241721895234</td>\n",
       "      <td>-1.410268871190</td>\n",
       "      <td>-1.375730853365</td>\n",
       "      <td>-1.031268147101</td>\n",
       "      <td>-0.481419521869</td>\n",
       "      <td>-0.379647656817</td>\n",
       "      <td>-0.903708056080</td>\n",
       "      <td>-1.147318784535</td>\n",
       "      <td>-0.947457016901</td>\n",
       "      <td>-0.764172532240</td>\n",
       "      <td>-0.113933079899</td>\n",
       "      <td>-0.424777655362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                       target                                  \\\n",
       "feature                           y                                   \n",
       "time step                       t+1             t+2             t+3   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00  0.329539415970  0.018233689588  0.201057023550   \n",
       "2017-11-26 12:00:00  0.018233689588  0.201057023550  0.145796194985   \n",
       "2017-11-26 14:00:00  0.201057023550  0.145796194985  0.744459196048   \n",
       "2017-11-26 16:00:00  0.145796194985  0.744459196048  0.112178064184   \n",
       "2017-11-26 18:00:00  0.744459196048  0.112178064184 -0.327148903700   \n",
       "2017-11-26 20:00:00  0.112178064184 -0.327148903700 -0.628321385686   \n",
       "2017-11-26 22:00:00 -0.327148903700 -0.628321385686 -0.366752336535   \n",
       "2017-11-27 12:00:00 -0.628321385686 -0.366752336535 -0.332675469409   \n",
       "2017-11-27 14:00:00 -0.366752336535 -0.332675469409  0.327697227327   \n",
       "2017-11-27 16:00:00 -0.332675469409  0.327697227327 -0.242415472100   \n",
       "2017-11-27 18:00:00  0.327697227327 -0.242415472100 -0.903708056080   \n",
       "2017-11-27 20:00:00 -0.242415472100 -0.903708056080 -1.147318784535   \n",
       "2017-11-27 22:00:00 -0.903708056080 -1.147318784535 -0.947457016901   \n",
       "2017-11-28 12:00:00 -1.147318784535 -0.947457016901 -0.764172532240   \n",
       "2017-11-28 14:00:00 -0.947457016901 -0.764172532240 -0.113933079899   \n",
       "2017-11-28 16:00:00 -0.764172532240 -0.113933079899 -0.424777655362   \n",
       "2017-11-28 18:00:00 -0.113933079899 -0.424777655362 -1.241721895234   \n",
       "2017-11-28 20:00:00 -0.424777655362 -1.241721895234 -1.410268871190   \n",
       "2017-11-28 22:00:00 -1.241721895234 -1.410268871190 -1.375730853365   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                       t+4             t+5             t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00  0.145796194985  0.744459196048  0.112178064184   \n",
       "2017-11-26 12:00:00  0.744459196048  0.112178064184 -0.327148903700   \n",
       "2017-11-26 14:00:00  0.112178064184 -0.327148903700 -0.628321385686   \n",
       "2017-11-26 16:00:00 -0.327148903700 -0.628321385686 -0.366752336535   \n",
       "2017-11-26 18:00:00 -0.628321385686 -0.366752336535 -0.332675469409   \n",
       "2017-11-26 20:00:00 -0.366752336535 -0.332675469409  0.327697227327   \n",
       "2017-11-26 22:00:00 -0.332675469409  0.327697227327 -0.242415472100   \n",
       "2017-11-27 12:00:00  0.327697227327 -0.242415472100 -0.903708056080   \n",
       "2017-11-27 14:00:00 -0.242415472100 -0.903708056080 -1.147318784535   \n",
       "2017-11-27 16:00:00 -0.903708056080 -1.147318784535 -0.947457016901   \n",
       "2017-11-27 18:00:00 -1.147318784535 -0.947457016901 -0.764172532240   \n",
       "2017-11-27 20:00:00 -0.947457016901 -0.764172532240 -0.113933079899   \n",
       "2017-11-27 22:00:00 -0.764172532240 -0.113933079899 -0.424777655362   \n",
       "2017-11-28 12:00:00 -0.113933079899 -0.424777655362 -1.241721895234   \n",
       "2017-11-28 14:00:00 -0.424777655362 -1.241721895234 -1.410268871190   \n",
       "2017-11-28 16:00:00 -1.241721895234 -1.410268871190 -1.375730853365   \n",
       "2017-11-28 18:00:00 -1.410268871190 -1.375730853365 -1.031268147101   \n",
       "2017-11-28 20:00:00 -1.375730853365 -1.031268147101 -0.481419521869   \n",
       "2017-11-28 22:00:00 -1.031268147101 -0.481419521869 -0.379647656817   \n",
       "\n",
       "tensor                            X                                  \\\n",
       "feature                      sqrt_A                                   \n",
       "time step                       t-5             t-4             t-3   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00  0.931886794099  0.666633367882  0.654660349438   \n",
       "2017-11-26 12:00:00  0.666633367882  0.654660349438  0.593411804245   \n",
       "2017-11-26 14:00:00  0.654660349438  0.593411804245  1.071883468639   \n",
       "2017-11-26 16:00:00  0.593411804245  1.071883468639  0.549662843424   \n",
       "2017-11-26 18:00:00  1.071883468639  0.549662843424  0.329539415970   \n",
       "2017-11-26 20:00:00  0.549662843424  0.329539415970  0.018233689588   \n",
       "2017-11-26 22:00:00  0.329539415970  0.018233689588  0.201057023550   \n",
       "2017-11-27 12:00:00  0.018233689588  0.201057023550  0.145796194985   \n",
       "2017-11-27 14:00:00  0.201057023550  0.145796194985  0.744459196048   \n",
       "2017-11-27 16:00:00  0.145796194985  0.744459196048  0.112178064184   \n",
       "2017-11-27 18:00:00  0.744459196048  0.112178064184 -0.327148903700   \n",
       "2017-11-27 20:00:00  0.112178064184 -0.327148903700 -0.628321385686   \n",
       "2017-11-27 22:00:00 -0.327148903700 -0.628321385686 -0.366752336535   \n",
       "2017-11-28 12:00:00 -0.628321385686 -0.366752336535 -0.332675469409   \n",
       "2017-11-28 14:00:00 -0.366752336535 -0.332675469409  0.327697227327   \n",
       "2017-11-28 16:00:00 -0.332675469409  0.327697227327 -0.242415472100   \n",
       "2017-11-28 18:00:00  0.327697227327 -0.242415472100 -0.903708056080   \n",
       "2017-11-28 20:00:00 -0.242415472100 -0.903708056080 -1.147318784535   \n",
       "2017-11-28 22:00:00 -0.903708056080 -1.147318784535 -0.947457016901   \n",
       "\n",
       "tensor                                                               \n",
       "feature                                                              \n",
       "time step                       t-2             t-1               t  \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-25 22:00:00  0.593411804245  1.071883468639  0.549662843424  \n",
       "2017-11-26 12:00:00  1.071883468639  0.549662843424  0.329539415970  \n",
       "2017-11-26 14:00:00  0.549662843424  0.329539415970  0.018233689588  \n",
       "2017-11-26 16:00:00  0.329539415970  0.018233689588  0.201057023550  \n",
       "2017-11-26 18:00:00  0.018233689588  0.201057023550  0.145796194985  \n",
       "2017-11-26 20:00:00  0.201057023550  0.145796194985  0.744459196048  \n",
       "2017-11-26 22:00:00  0.145796194985  0.744459196048  0.112178064184  \n",
       "2017-11-27 12:00:00  0.744459196048  0.112178064184 -0.327148903700  \n",
       "2017-11-27 14:00:00  0.112178064184 -0.327148903700 -0.628321385686  \n",
       "2017-11-27 16:00:00 -0.327148903700 -0.628321385686 -0.366752336535  \n",
       "2017-11-27 18:00:00 -0.628321385686 -0.366752336535 -0.332675469409  \n",
       "2017-11-27 20:00:00 -0.366752336535 -0.332675469409  0.327697227327  \n",
       "2017-11-27 22:00:00 -0.332675469409  0.327697227327 -0.242415472100  \n",
       "2017-11-28 12:00:00  0.327697227327 -0.242415472100 -0.903708056080  \n",
       "2017-11-28 14:00:00 -0.242415472100 -0.903708056080 -1.147318784535  \n",
       "2017-11-28 16:00:00 -0.903708056080 -1.147318784535 -0.947457016901  \n",
       "2017-11-28 18:00:00 -1.147318784535 -0.947457016901 -0.764172532240  \n",
       "2017-11-28 20:00:00 -0.947457016901 -0.764172532240 -0.113933079899  \n",
       "2017-11-28 22:00:00 -0.764172532240 -0.113933079899 -0.424777655362  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs.dataframe.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 12)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs.dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6917563   ,  0.24262266  ,  0.22719222  ,  0.28696585  ,\n",
       "         0.4290662   ,  0.39893463  ],\n",
       "       [ 0.3483508   , -0.036233913 , -0.05260812  , -0.18223044  ,\n",
       "        -0.06523806  , -0.29652646  ],\n",
       "       [-0.15085994  , -0.16838273  ,  0.09653863  , -0.15122715  ,\n",
       "        -0.095460564 , -0.40536705  ],\n",
       "       [-0.25189507  ,  0.10488853  , -0.19355524  , -0.21186623  ,\n",
       "        -0.3963569   , -0.44327196  ],\n",
       "       [ 0.39475784  , -0.0036518835, -0.10132682  , -0.20630407  ,\n",
       "        -0.3344156   , -0.45097885  ],\n",
       "       [ 0.70721686  ,  0.77805245  ,  0.518649    ,  0.9282283   ,\n",
       "         0.59483635  ,  1.0405345   ],\n",
       "       [ 0.96074355  ,  0.41442063  ,  0.55076885  ,  0.4809657   ,\n",
       "         0.9446499   ,  0.80569404  ],\n",
       "       [ 0.5068792   ,  0.49169174  ,  0.90519255  ,  0.54049754  ,\n",
       "         0.8726349   ,  0.50837064  ],\n",
       "       [-1.0098432   , -0.48989135  , -0.665987    , -0.8365729   ,\n",
       "        -0.9079364   , -0.9869681   ],\n",
       "       [-1.3051845   , -0.5128123   , -0.6061362   , -0.889179    ,\n",
       "        -0.98545355  , -1.0532341   ],\n",
       "       [ 0.29453963  , -0.10721627  , -0.25177813  , -0.38609377  ,\n",
       "        -0.43064758  , -0.41857624  ],\n",
       "       [ 1.0769868   ,  1.039638    ,  0.8663901   ,  0.75702137  ,\n",
       "         0.8512387   ,  1.0134165   ],\n",
       "       [ 0.08252829  , -0.07756814  , -0.16148588  ,  0.1022647   ,\n",
       "         0.22326043  ,  0.31127927  ],\n",
       "       [-0.1327943   , -0.005528029 ,  0.59778214  ,  0.3554509   ,\n",
       "         0.47068033  ,  0.4952992   ],\n",
       "       [-1.4112334   , -0.62475955  , -0.46307015  , -1.1672412   ,\n",
       "        -1.2887125   , -1.1903      ],\n",
       "       [-1.6736292   , -0.70389676  , -0.774971    , -1.1885437   ,\n",
       "        -1.3242662   , -1.3740029   ],\n",
       "       [ 0.25219402  , -0.105838835 , -0.23425068  , -0.36723742  ,\n",
       "        -0.4384593   , -0.4927049   ],\n",
       "       [ 0.68675876  ,  0.2635016   ,  0.08112715  ,  0.03348205  ,\n",
       "         0.037119646 ,  0.053119976 ],\n",
       "       [-0.44741413  , -0.39113715  , -0.40431628  , -0.120824456 ,\n",
       "         0.37140098  ,  0.27459154  ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp    h         prediction             actual\n",
      "0 2017-11-25 22:00:00  t+1 5,153.679122074512 5,153.677621840000\n",
      "1 2017-11-26 12:00:00  t+1 5,153.677699753201 5,153.676332470000\n",
      "2 2017-11-26 14:00:00  t+1 5,153.675632115875 5,153.677089690000\n",
      "3 2017-11-26 16:00:00  t+1 5,153.675213647281 5,153.676860810000\n",
      "4 2017-11-26 18:00:00  t+1 5,153.677891962482 5,153.679340360000\n",
      "              timestamp    h         prediction             actual\n",
      "109 2017-11-28 14:00:00  t+6 5,153.671326949908 5,153.670415880000\n",
      "110 2017-11-28 16:00:00  t+6 5,153.670566086720 5,153.670558930000\n",
      "111 2017-11-28 18:00:00  t+6 5,153.674216258086 5,153.671985630000\n",
      "112 2017-11-28 20:00:00  t+6 5,153.676476962462 5,153.674263000000\n",
      "113 2017-11-28 22:00:00  t+6 5,153.677394256172 5,153.674684520000\n",
      "(114, 4)\n"
     ]
    }
   ],
   "source": [
    "eval_df = create_evaluation_df(predictions, test_inputs, HORIZON, y_scalar)\n",
    "print(eval_df.head())\n",
    "print(eval_df.tail())\n",
    "print(eval_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h\n",
       "t+1   0.000000567804\n",
       "t+2   0.000000558260\n",
       "t+3   0.000000636456\n",
       "t+4   0.000000539660\n",
       "t+5   0.000000430203\n",
       "t+6   0.000000425914\n",
       "Name: APE, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual']\n",
    "eval_df.groupby('h')['APE'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002712806543018942"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(eval_df['prediction'], eval_df['actual'])\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "a = mean_absolute_error(eval_df['prediction'], eval_df['actual'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     5,153.679122074512\n",
       "1     5,153.677699753201\n",
       "2     5,153.675632115875\n",
       "3     5,153.675213647281\n",
       "4     5,153.677891962482\n",
       "5     5,153.679186109217\n",
       "6     5,153.680236169263\n",
       "7     5,153.678356348207\n",
       "8     5,153.672074367992\n",
       "9     5,153.670851119849\n",
       "10    5,153.677476877445\n",
       "11    5,153.680717626999\n",
       "12    5,153.676598766203\n",
       "13    5,153.675706940336\n",
       "14    5,153.670411884998\n",
       "15    5,153.669325091001\n",
       "16    5,153.677301489847\n",
       "17    5,153.679101375590\n",
       "18    5,153.674403843963\n",
       "19    5,153.677261847073\n",
       "20    5,153.676106875393\n",
       "21    5,153.675559539733\n",
       "22    5,153.676691378109\n",
       "23    5,153.676241824056\n",
       "24    5,153.679479496962\n",
       "25    5,153.677973402106\n",
       "26    5,153.678293444543\n",
       "27    5,153.674227911274\n",
       "28    5,153.674132976926\n",
       "29    5,153.675812879773\n",
       "             ...        \n",
       "84    5,153.672496447016\n",
       "85    5,153.672175385494\n",
       "86    5,153.674473287864\n",
       "87    5,153.679782620817\n",
       "88    5,153.677181652361\n",
       "89    5,153.678206419209\n",
       "90    5,153.670919343712\n",
       "91    5,153.670772086973\n",
       "92    5,153.674440933162\n",
       "93    5,153.676410692092\n",
       "94    5,153.677795222765\n",
       "95    5,153.677909261987\n",
       "96    5,153.675028792418\n",
       "97    5,153.674577995105\n",
       "98    5,153.674421000035\n",
       "99    5,153.674389079580\n",
       "100   5,153.680566648429\n",
       "101   5,153.679593983268\n",
       "102   5,153.678362525420\n",
       "103   5,153.672169112496\n",
       "104   5,153.671894651136\n",
       "105   5,153.674523285098\n",
       "106   5,153.680454330887\n",
       "107   5,153.677546209884\n",
       "108   5,153.678308385933\n",
       "109   5,153.671326949908\n",
       "110   5,153.670566086720\n",
       "111   5,153.674216258086\n",
       "112   5,153.676476962462\n",
       "113   5,153.677394256172\n",
       "Name: prediction, Length: 114, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot actuals vs predictions at each horizon for first week of the test period. As is to be expected, predictions for one step ahead (*t+1*) are more accurate than those for 2 or 3 steps ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA40AAAHuCAYAAAA7q+nkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsvXmYFOW99n8/s+8rDIvDqiirbCNKQAKicd81bnkjSTTxGJOck+U9nvOe+BpzruuY+MtrjPtONIoKRkXjFhVEEFFWQQHZYYBhhhlmY5bunq7fH995qKWX6Z7pZbr7/lxXXVNVXVX9THUtz/18N2UYBgghhBBCCCGEEH+kxbsBhBBCCCGEEEL6LxSNhBBCCCGEEEICQtFICCGEEEIIISQgFI2EEEIIIYQQQgJC0UgIIYQQQgghJCAUjYQQQgghhBBCAkLR6EApdbdS6qBSamP3dFGA7Z5RStUqpbaEsr9SaoZl3Sal1JWWfUqUUkuUUtuUUluVUjN7aONtSqnN3cdaqZQaH4n/nRBCCCGEEEKcqFSu06iUmgtggWEYCyzr7gbQahjG/9fDvnMAtAJ4zjCMiT3tr5TKA+AyDMOjlBoCYBOAod3LfwXwiWEYTymlsgDkGYbRGOS7iwzDaO6evwzA7YZhXBDGv04IIYQQQgghIUFLYy8xDGMFgIYwtm8zDMPTvZgDwABEAAKYA+Dp7u1cWjAqpU5WSr2rlFqnlPpEKTW2e5tmy6Hz9bEIIYQQQgghJNJQNPrnDqXUl90uqKWR2l8pdaZS6isAmwHc1i0iRwOoA/CsUmqDUuoppVR+9y5PAPiZYRjTAfwawCOWY/1UKbULwB8B/Lx3/yYhhBBCCCGEBCcl3VOVUmsAZAMoAFAGYH/3R/8OYCOAoxDr3e8BDDEM44cBjjMSwFsO99RBPe2vlBoH4K8QC+NEAJ8BmGUYxhql1AMAmgH8ASImt1t2zTYMY5zjWDcCON8wjJvDOwuEEEIIIYQQ0jMZ8W5APDAM40zAf0yjFaXUkwDeCvPYR3ra3zCMrUqp4xDBWA2g2jCMNd0fLwFwJ8QK3GgYxpQevvIlAI+G00ZCCCGEEEIICRW6pzroTlKjuRLAlkDbhrO/UmqUUiqje34EgNMA7DUMowbAAaXUad37zAfwdXfc4h6l1LXd+yil1OTu+TGW77gYwI5w2kgIIYQQQgghoZKSlsYe+KNSagrEvXQvgJ8AgFJqKICnDMPQJTQWAZgLYIBSqhrA/zUM4+lA+wOYDeBOpZQbgBeS8fRo92c/A/BCd+bU3QB+0L3+JgCPKqX+C0AmxKq4CRIzeS4AN4BjAOiaSgghhBBCCIkKKRnTSAghhBBCCCEkNOieSgghhBBCCCEkIBSNhBBCCCGEEEICknIxjQMGDDBGjhwZ72YQQgghhBBCSFxYt27dUcMwBoa6fcqJxpEjR2Lt2rXxbgYhhBBCCCGExAWl1L5wtqd7KiGEEEIIIYSQgFA0EkIIIYQQQggJCEUjIYQQQgghhJCApFxMIyGEEEIIISS5cbvdqK6uRkdHR7ybEldycnJQWVmJzMzMPh2HopEQQgghhBCSVFRXV6OwsBAjR46EUirezYkLhmGgvr4e1dXVGDVqVJ+ORfdUQgghhBBCSFLR0dGB8vLylBWMAKCUQnl5eUSsrRSNhBBCCCGEkKQjlQWjJlLngKKREEIIIYQQQuLE8uXL8emnn/bpGAUFBRFqjX8oGgkhhBBCCCEkTkRCNEYbikZCCCGEEEJIcqJUdKcgXHHFFZg+fTomTJiAJ554AgDw7rvvYtq0aZg8eTLmz5+PvXv34rHHHsP999+PKVOm4JNPPsGCBQuwZMmSE8fRVsTW1lbMnz8f06ZNw6RJk/DGG29E77w5YPZUQgghhBBCCIkwzzzzDMrKytDe3o4zzjgDl19+OW699VasWLECo0aNQkNDA8rKynDbbbehoKAAv/71rwEATz/9tN/j5eTk4LXXXkNRURGOHj2Ks846C5dddllMYjcpGgkhhBBCCCEkwvzlL3/Ba6+9BgA4cOAAnnjiCcyZM+dE+YuysrKwjmcYBv7zP/8TK1asQFpaGg4ePIgjR45g8ODBEW+7E4pGQgghhBBCCIkgy5cvxwcffIDVq1cjLy8Pc+fOxeTJk7F9+/Ye983IyIDX6wUgQtHlcgEAXnjhBdTV1WHdunXIzMzEyJEjI1JOIxQY00gIIYQQQghJTgwjulMAmpqaUFpairy8PGzbtg2fffYZOjs78fHHH2PPnj0AgIaGBgBAYWEhWlpaTuw7cuRIrFu3DgDwxhtvwO12nzhmRUUFMjMzsWzZMuzbty9aZ80HikZCCCGEEEIIiSAXXHABPB4PTj/9dPz2t7/FWWedhYEDB+KJJ57AVVddhcmTJ+O6664DAFx66aV47bXXTiTCufXWW/Hxxx9jxowZWLNmDfLz8wEAN910E9auXYuqqiq88MILGDt2bMz+H2UEUcjJSFVVlbF27dp4N4MQQggh/RDDALZvBwoKgMrKeLeGENJbtm7dinHjxsW7Gf0Cf+dCKbXOMIyqUI/BmEZCCCGEkG7uuw945x3JpP+b3wAXXhjvFhFCSPyheyohhBBCCIC6OhGMgFgcX3wxvu0hhJD+AkUjIYQQQgiAb76xLx88CHQnLSSEkJSGopEQQgghBMDOnfZlwwAOHYpPWwghpD9B0UgIIYQQAl/RCAAHDsS+HYQQ0t+gaCSEEEIIgYhGwwDa2oDOTllXXR3fNhFCSH+AopEQQgghKU9rK1BTA7S0AEeOiFuqy0VLIyGkdzQ2NuKRRx4JefuHHnoIp5xyCpRSOHr0aBRb1jsoGgkhhBCS8uzaJX/b28117e20NBJCekcg0bhw4ULcfffdPutnzZqFDz74ACNGjIhB68KHdRoJIYQQkvLoeEaPx1zn8dDSSEiiM29edI+/bJn/9XfeeSd27dqFKVOm4LzzzsN9990X9DhTp06NQusiB0UjIYQQQlIeHc9oFY1uN9DYCBw/DuTnx69thJDE495778WWLVuwcePGeDclIlA0EkIIISTl2bkT8Hpl0mgBWV0NnHZafNpFCEl86uvrMX/+fABAQ0MDXC4XXn/9dQDA888/j0mTJsWzeSFB0UgIIYSQlMbjAfbutVsZ9XrDoGgkhPSN8vLyExbHhQsXYu/evX7jGvszFI2EEEIISWm0YHSKRsMAuroY10hIIhMo5jDaFBYWoqWlJT5fHgWYPZUQQgghKY0zCY5S5mceDzOoEkLCp7y8HLNmzcLEiRPxm9/8psft//KXv6CyshLV1dU4/fTTccstt8SglaFDSyMhhBBCUhotGt1u+VtRIbUa9TpaGgkhveHFF1/0WbdgwQK/2/785z/Hz3/+8yi3qPfQ0kgIIYSQlMZpaRw1yvxMWxoNI/btIoSQ/gJFIyGExJiPPwYeewz47DN2RAmJN4bhKxpPOgnIyDDXtbVJ6Q1CCElV6J5KCCEx5O23AV3f9+WXgcpK4OqrgQsuAHJy4ts2QlKRI0ekDqOu0ZiZCRQUAMXFQH29KSQPHABKS+PbVkIIiRe0NBJCSIxoaRELo5XqauCBB4BrrwUefxyorY1P2whJVbSVsatLhGNZGZCWBhQVyXprrUZCCElVKBoJISRGPPusCEd/tLYCL70E3HAD8PvfA9u2xbZthKQqTtfUsjJg0CCgpMRc7/UyGQ4hJLWheyohhMSAPXuAN97oeTuvF/joI5kmTACuuQY4+2wgPT36bSQkFdmxQ/5aRWNZGTB4sLkNy24QQlIdWhoJISTKGAbw0EMiCDVDhgAvvgh873umG5yTr74Cfvc74KabgFdekbgrQkhkcVoay8vFyjhihLmNx0NLIyEkPBobG/HII4+EvP1NN92E0047DRMnTsQPf/hDuHUNoH4CRSMhhESZVauA9evt626/XYTjj34kgvBXv7J3Uq0cOQI8+qjEPT74IHDoUPTbTEgq0NJixhF7PBLLWFIiCW9Gjza383iAgwftAz+EEBKMQKJx4cKFuPvuu33W33TTTdi2bRs2b96M9vZ2PPXUUzFoZejETDQqpS5QSm1XSu1USt3p5/NspdTL3Z+vUUqN7F6fqZT6q1Jqs1Jqq1LqP0I9JiGExBuXC3C+M6ZNA2bNMpezs4FLLpGYxz/8Aaiq8n+s9nbg738X6+Rvfwts2sSSHYT0BW1lBEQYlpSYwnHoUDOjsccjExNVEUJC5c4778SuXbswZcoU/OY3v+lx+4suughKKSilMGPGDFT3M5/4mMQ0KqXSATwM4DwA1QC+UEotNQzja8tmPwJwzDCMU5RS1wP4A4DrAFwLINswjElKqTwAXyulFgE4EMIxCSEkrixeDBw+bC6npQF33AEo5butUsCMGTLt3Qu8+irw/vsiPK0YBrBypUxjxkjc4znnmHXlCCGhYRWNbrfUZwTE0uhyiet4R4d8BkhcozXWkRCSGPgx7EX92Pfeey+2bNmCjRs3hnU8t9uN559/Hg888EDfGxdBYmVpnAFgp2EYuw3DcAF4CcDljm0uB/DX7vklAOYrpRQAA0C+UioDQC4AF4DmEI9JCCFx4+hR4G9/s6+7/HJg1Kie9x05UlxWX34Z+OEPJTGHP3bsAP7nf4Drr5fvamrqc7NJAuF2y6DE3r10newNWjQahpTc0PdZcbEIR2fZDcY1EkJ6Q319PaZMmYIpU6bgrrvuwmOPPXZiefPmzbZtb7/9dsyZMwdnn312nFrrn1iNS58EsQxqqgGcGWgbwzA8SqkmAOUQAXk5gMMA8gD8m2EYDUqpUI5JCCFx48knxUqhKSwEfvCD8I5RUgL8r/8lonDZMrFcWq0jmvp64OmngeefB77zHbE+BoqRJIlNV5cMSNTWAg0Npotyfj4wcGB825ZoWJPg6BqNBQVAZqbce1bRaBjMoEoI6R3l5eUnLI4LFy7E3r17/cY1/u53v0NdXR0ef/zxGLewZ2IlGv04YsEZiRNomxkAugAMBVAK4BOl1AchHlMOrNSPAfwYAIYPHx5ikwkhpPd8/bW4llr50Y9EOPaGzEwRg+edJ7GMixcDq1f7xjS6XMBbb8l0xhmSPKeqyr87LEkcvF4ZGKitlb/aqqiUXBtut32AgvSM2w3s2yfz2pJYWioTAOTmAgMGyLzXy1qNhCQy0XRPDURhYSFaAhVn9sNTTz2F9957Dx9++CHS0vpfrtJYicZqAMMsy5UAnPn/9DbV3a6oxQAaANwI4F3DMNwAapVSqwBUQayMPR0TAGAYxhMAngCAqqoqpo0ghEQVw5Asp1ZGj5ZkN31FKWDKFJkOHpTEOG+/7V8wfPGFTCNGiOXxvPMk6Q5JDLxe4NgxEYpHj4qFUVNSAlRUiKipqQF27/aNfSXB2bvXPKcej1gYs7Pl3AJyr1nHmd1uWhoJIaFTXl6OWbNmYeLEibjwwgtx3333Bd3+tttuw4gRIzBz5kwAwFVXXYW77rorFk0NiViJxi8AjFFKjQJwEMD1EDFoZSmAmwGsBnANgI8MwzCUUvsBnKOU+hvEPfUsAH8G8HUIxySEkJjz3nvAtm32dXfcAaSnR/Z7TjoJ+NnPxOX1H/8QAekvu+O+fcCf/gQ89RRw2WUSV1leHtm2kMhgGEBjo/yOdXWmBQwQK3VFhUxW8Z+VJX8pGsPDmTlVxzNqSyMgscXWbWpqRDxmZsakiYSQBOfFF1/0WbdgwQK/23qsD/x+SExEY3eM4h0A3gOQDuAZwzC+UkrdA2CtYRhLATwN4Hml1E6IhfH67t0fBvAsgC0Ql9RnDcP4EgD8HTMW/w8hhASirQ144gn7ujlzgKlTo/edBQXAddeJNfGTT8R19Ws/eaSbmiTmcdEiybZ6zTWSfZXEF8MAmptNoWgVf/n5plDMzfW/P0Vj7wgkGktK5DdRSs57fj5w/LgZ13joEOOFCSGpR8wStBuG8TaAtx3r7rLMd0DKazj3a/W3PtAxCSEknjz/vLgUajIzgdtui813p6cDc+fK9PXXwJIlwMcf+2bV9Hgk3vL994HJkyXuceZMKQdCYocuLF9XZ3cvzs01hWJ+fs/H0aJRl4UgoRFINBYVAZ9/LpZdnUFVi0ZAXFQpGgkhqQarehFCSISorhahZuX664EhQ2LflvHjgbvuElHy2mvAm29Kx9fJpk0yDR0KXH01cOGFgS1apO8cPy6/SW0t0N5urs/ONoViuMmSaGkMH8MILBqzsuS36eyUmozFxVLWhGU3CCGpDEUjIYREiEcftcegDRgA3BjnSOuKCuAnPwG+/33g3XeBV1+VBDpODh2S5D3PPANcfDFw1VXAoEGxb28y0t4u1sTaWqC11VyflSUlMioqxJrV2wy3mZmyr8tlulWS4NTUiCs5IJb4tDSx6iplinCvV9bpshvakstkOISQVISikRBCIsAXXwCffmpfd9ttQE5OfNrjJDcXuPJK4IorpFTHkiXAhg2+2x0/Drzyinw+Z47EPU6YEPv2JjqdnaZQbG4212dkmEKxpCQyAk+X3XC5RNho0UMC48/KqJRYFa0W2+xsWQdIplXDoKWREJKaUDQSQkgf8XiAhx+2r5swQZLN9DeUAr71LZl27hRx+OGHdgspIFaW5ctlGjdO4h7nzIl8Bthkwu02hWJjo7k+PV2szhUVEiMXjdjRrCwROy4XRWMoOEWjziZcUmKPL3W5TPdyw5BtaWkkhKQiFI2EENJH3njDLBIOiDD7+c/7v5vgKacAd94J/PjH8j+88YZkWHWydStwzz0ieq68UtxXw427S1Y8HqmhWFsrCZCM7krAaWkiRCoqxIoVbbHNuMbwCJY51Rpr2t4utRrT0mQgxeMBGhrEtTUvL7ZtJoQkFo2NjXjxxRdx++23h7T9j370I6xduxaGYeDUU0/FwoULUVBQEOVWhg5z5RFCSB9obASefda+7oILgFNPjU97ekNZmdR6fPll4Ne/ttems1JbCzz+OPDd7wIPPJC6FpeuLjkXW7aIS/K2bSIkABGK48aJJXfCBHFFjYV1lqIxPHbsMOedNRqtlsb2drES636btsj7iwsmhBArjY2NeOSRR3zWL1y4EHfffbfP+vvvvx+bNm3Cl19+ieHDh+Ohhx6KQStDh5ZGQgjpA888Y89KmpcH3Hpr/NrTF7KzxYp40UXAunXiurpmje92HR3A66+LZXLmTIl7nDKl/1tW+4LXK8KwthaorxfhqCktFYvigAHxK/pO0Rg6TU3iRqwxDLEwAhK/aLW2t7fL71tcLLGpOhnOgQOscUoICc6dd96JXbt2YcqUKTjvvPNw3333Bd2+qDvrlmEYaG9vh+pnL1WKRkII6SU7dwJvvWVf9/3vSyczkVEKqKqSad8+ybj6/vuS3MWKYYil7dNPgZNPFvE4f378hFOkMQxxOa2tFRdUa9xnUZFZIqM/xBBSNIbOrl325eJiM840L0+8BzIy5Pdub5fPdQZVa61GQkjisHx5dI47d27gz+69915s2bIFGzduDPl4P/jBD/D2229j/Pjx+NOf/tT3BkYQikZCCOkFhiElKnQMGwBUVkqtw2RixAjgl78EbrlFaj2+9ppY2pzs2gX84Q/AE09IhtbLLjOtN4mEYYilqbZWrFHasgSIi6IWiv0lK66GojF0rPGMgD0+V/+u+fkiGF0ue9kN1mokhPSG+vp6zJ8/HwDQ0NAAl8uF119/HQDw/PPPY9KkSQCAZ599Fl1dXfjZz36Gl19+GT/4wQ/i1mYnFI2EENILPv4Y+PJL+7qf/lQsFMlIURFw003AddcBy5YBixfb48I0x45JjOff/gacd55YH0eNin17w6W52RSKVotqXp4pFPtz4hOKxtCxikav1xSN6enm/avFo8sl87Q0EpLYBLMIxoLy8vITFseFCxdi7969fuMaASA9PR3XXXcd7rvvPopGQghJZDo7gUcfta8780zgrLPi055YkpEhYvDcc4HNmyXuceVKu8UVEAvd22/LVFUl4nHGjP4V99jaKkKxttae/CQnxxSK/ShxXVAoGkMnWOZUPWCQmyvXalOTuK7qbbq6ZDpwQK75/nQ9E0L6F4WFhWhpaQlpW8MwsGvXLpxyyikwDANvvvkmxo4dG+UWhgdFIyGEBMAwgA8+kEyJF1wADB4s6196SYSGJj0dCDGjdtKgFHD66TIdPixxj2+/bS9XoFm7Vqbhw8V99/zzJelOPGhrE2vikSMyr8nKMoWitiolEhSNoeFy2cvjuN1mDLK13EZOjikIOzqAk04y4xw9Hkl+1dSUmC7YhJDYUF5ejlmzZmHixIm48MILgybCMQwDN998M5qbm2EYBiZPnoxHnaPTcYaikRBCAnDgALBqlcy/8YbUM6yrAxYtsm931VUiiFKVIUOAO+6Qsh1vvy0C8sgR3+327wfuvx946imJebziCsk4Gm06OuR3q60FrIO+mZlSEqOiQpKdJLLVKCNDLGIej1jCYlHmIxHZu1dcUjUlJabgtpbbyM01k+O0t4ulsahIMuh6PDLoUV1N0UgICc6LL77os27BggU+69LS0rBKdzj6KRSNhBASAGsttsOHgUOHgKeftse8lZRIxlQiCUOuvVZE9KpVEve4ZYvvdi0twAsviMV23jxxXT3ttMi2xeUyhaK1hEJGhgjVigoRCYksFJ1kZYnocbspGgPhTIJjHbQoKTFFY06OXTSWlNhFIyCDShMnRr/NhBDSH6BoJISQANTU2JdffVWSwFi55ZbEiXuLFenpwJw5Mm3bJnGPy5fbaxsCsvzBBzJNmiSCc9Yss7MeLm63lMaorZWyCTrOMi3NFIplZb0/fn9Hi0advIX44kzeVFxszmtRmJYm51ILb6toBJgMhxCSmlA0EkKSA7cb+Mc/pNd39dX23mAvsYpGwwCee04yaOrO5JgxwIUX9vlrkpqxY4H/+i/gJz+Rch1vvWV3EdVs3izTkCFiqbzootCylXZ1mUKxocEUikqZQrG8PDUsb4xr7BmnpdE64JObK391PGNGhpxTl0u2048UXYaFZTcIIakERSMhJDm45x7gv/9b5l96CXjvvT75Hno84t6o2bFDxEl5uWlx+NnPktdqFWkGDpSY0O9/X36aJUv8W2oOHwYefljKdlx8MXDllSIkrXi9Uiuytlb+6hg1pcTldNAgEYzJWv4kEBSNwTEMqSdqXc7MNJf1+bNaaXNz5XxmZ5ulOWhpJCRxMAwDKpniEHqB4Uxv3ktS7JVKCElKPB7ggQfM5X/+E/j6a2DChF4fsq7OFCMuF7B+vcy3tEjncf58cakk4ZGTA1x+uSTCWbNG4h71ubXS1iafLVkCzJ4txuOTTpLf5ehRu6trcbFYFAcONDv+qQhFY3AOHbJn983LM0VjVpY5AKQtjnq+qUlEo7Y0ejwiOKurWXaDkP5MTk4O6uvrUV5enrLC0TAM1NfXIycCMQsUjYSQxGfjRl+fx1Wr+iQaDx825w8eNDviLpd0FH/yk14fmkA62medJdPu3SIO//lP04oDyHl2u8Uy+dZbQGUl8O1vA1OmSIyZLpERr/Id/Q2KxuA4XVMrK03BZ63R6LQ0AjJIUVYm11pnpyy73WLtHjQo+m0nhIRPZWUlqqurUWd1G0pBcnJyUFlZ2efjUDQSQhKfjz/2XbdypfhD9hIdz9jUJBlA8/NNXTp5sogVEhlGjwb+9/8Gbr0VeP11SThUVyedcmt5hIMHRVyuWiUZV089lYLRCkVjcJyi0XoPl5aaVkinpRGQz0pLxTW9rk4GNzIyxNpI0UhI/yQzMxOjRo2KdzOSBkbjEEISH3+isY/1jmpqxNL1+efSOdTxTPn5YnHwV8Se9J7WVuDYMWDcOOAXv5BkOIMHSwKbnBzprBcVyfyxY8CTT0q21fvvZ0ISDUVjcJyisbzcnHeW29BYRaM1g6pOhsO4RkJIqkDRSAhJbLq6gE8+8V2/e7cEMfUCw5Di9AcPypSVJRat7GzgjDNkm40b+9BmAkDiFvfuFWG+di2wf7903AsKxJL40ksiDufN85/9tLMTWLpUkuvceSewbp2ZPTUVoWgMjlM06oEgQAShHgiyikadwVeLRmtcI8ABC0JI6kD31Djz5ZeS5W/o0Hi3hJAEZfNmKcrnj1WrxBwVJseOiXhZv17EihYs06YBI0bI/Lp1Eo+XorH1vaa9Xdz7amvFuqjJypJENhUVYs3R53XaNJkOHBC31XffNWPPrKxZI9Po0SI4589PvaQ4OqkLRaMvjY2SQEmTkWHPnJqfLyVbMjPtWXfT082yG/n5rNVICEldaGmMI4YB3HcfcNNNEnr14ou9NowQkrqsWBH4s166qNbUyL3Y0CAdRqVkuusu0wpx9Ciwb1+vDp9ydHZK53r9ehF2u3eLYMzIkHIakycDM2dK3cviYv9CfNgw4F//VTKq3nqrDLb5Y/du4I9/BK67Tsp2HDsW3f+tP5GeLufU67UnFCK+VsZRo4DmZnNZ39f+EgxqF9XcXF/RSEsjIcmBYch7P5W9VXqClsY4smePOUq5Y4dMTz4pHae5c2WiBZKQHvAXz6hZubJXh6ypEQMmYFqrzj5b4u127wa++ELWrV0LjBzZq69Ietxu06JoNQSnp4vgq6iQxCLh1rksLARuvBH47nflp1+yBNi2zXe7xkbguedkMO7cc8XgPHp03/6nRCArSwSNy5V6dSqD4RSNJ58sSa40Wixak+BorGU3nLUaDx82k+IQQhKTtjbgjjukXz58OPDII+JZQOzwMRdHli3zv94pIOfNEwHpLHBNSMpjGMEtjRs3ikmroCCsw65da2ZP1aLxxhvl7/TppmjcuhU4fpwvF43HIxbY2lqx8OkR27Q0STpSUSFJhPzFJ4ZLRoa4oJ5zDvDVV2KB/OQT31Fij0dcWt99F5g6VcRjMrsVZ2VJB8jlMuPxiK9oHDYM+OYbmc/NNa+bYJbGjAwRjnl5co69Xrm2Dx2SjiYhJDF55RURjIDE1v/jHzIwSexQNMaR0aPFLevLLwObw7WAfOIJSS+vLZAUkIQA+Ppre6BSUZEExu3aJctdXeIPOX9+WId9/31zPitLhOJpp8ny4MFS3626Wg6/YYMUn09VurqA+noRig2hUrFhAAAgAElEQVQNZokMpUQgDhokgjFalhilgIkTZTp8GHjtNXnht7X5brthg0yVlRL3eP75/kVCIsO4Rv84ReOAAaZotJbbCCYaOzvFfbq4WK4vj0eeD9XVFI2EJCqG4WvE0V0IYoeiMY7MmydTQ4MYS5YvDy4gv/lGJgpIQrpxuqbOni0qxfrEX7UqLNH41VdmB1Mp6YTfdJN9m6oq07V83Tpg1qzktVz5w+uV51ZtrQjGri7zs5ISsSgOHGhPNBILhgwBbr8dWLAAeOcdSZxz+LDvdtXVwJ//DDz1FHDJJcCVVyZP3U1mUPWls1OsB1Z0FlTAXm7Dn3uqNYOqrtWo3VK1aCSEJCZ79vg+H7SnEbFD0dgPKCsDrrhCpvp6cbGigCQkBJyi8dvfFrPWs8+a68KMa1y40JzPypI4xilT7NtMmCDujh0d4oa5e7fESCUzhiH/a22tGHetiVaKikyhmJ0dvzZq8vKAq68WMbhqlcQ9fvml73atrVLW45VX5Pl57bXA2LExb25EoWj0Zc8e+7t06FBTJAKBy21oAtVqZDIcQhKfjz7yXedvsJFQNPY7ysspIAkJCcPwLxpLSuzrVq8OOVNFTY3dTSUrS2IZnVbEzEwRkp99Jstr1yanaDQMSQBSWytJbXRBc0DCRCsqZOqvLp5paZLA6Oyz5Rm5eLH8vlbLKCCW048+kmnCBBGPs2dHJvYy1lA0+uJ0TT3lFHtypuJiEYRK+b+WA5Xd0PcDLY2EJCaGIX1sJ0ePyv0da2+Z/g5FYz/Gn4BctkyyOoYqIOfNk340BSRJOr75BjhyxFzOz5eCfhkZErCkYx1bW+WmmTq1x0O+8ordAjF8eOB4xenTTdG4fTvQ0mIvFp7ItLTIqa2rs9dEzMszhWKiJVk59VTg//wf4Cc/AV5/HVi6VP5PJ199JdOgQcBVVwEXX5xYiY4oGn3pSTTm5opozM4O7GaemyvnlGU3CEkedu4EDh70XW8Y8g6srIx9m/ozrNOYIGgB+cADMlr+i18Ap58ePI7qm2+Axx8XS8lPfiJuWDS5k6TBaWWcNUuGBZWSeSsh1GtsbJQEKtbO9vXXB77HBg4ERoyQea9XEqwkMsePixvfmjUSp1ldLYIxJ0fEc1UVMGOGlBhJNMFoZcAA4JZb5Dn6y18GTmBy5Ajw6KNidXzwwcSpoUvR6Is/0Wit32mtwxgI/VlOjgwOKWWKxvp6072VEJI4BKpiALC/7A9aGhMQpwVSJ9EJxQL5+OOSBXLuXFogSYLjzzVVM2sW8MYb5vLKlVKEKQivvipWRu1ylp8vcXHBqKoC9u2T+XXrxCoZbt3BeNLeLq6ntbUiGjVZWaZFUVtVko3sbODSSyURzuefS9zj2rW+27W3A3//u2RlnTVLROSkSf038RFFox2v1zcT4ujRwD//aS7rcxbMzVqLxuxsuccLC8WJwTDkWqiulhJZhJDEwF/WVCsUjb5QNCY45eWS7OHKK0MXkNu3y0QBSRKWQPGMGqdP6cqVZu/OD21tIgrcbvO+Oeusni1q48aZNduamsSiceqpYf4vMaajQ9xOa2vt7pmZmWI9raiQGK/+KooijVLAmWfKtGePDB68/749fhOQ62LlSpnGjJGSHeec0/+KumsBpK/lVPkdA3HokN3lvLhYrnVdGqagwLQYhiIavV4RjkVFQHOzLKeni4sbRSMhicP27cGzpFI0+tLPXnekL1BAkpRhzx57IEJOjpj9NNOmyTrdWzx4UHJqa39SB0uXiqVNW2eys0UQ9ERGhoRKau/XtWv7p2h0uUyh2NRkrk9PN4ViaSkFxqhRwK9/Le6rS5dK7KPVjVGzYwfwP/8jseNXXCEWS2sJh3iiy8S43TJpEZmq+HNNtd4DpaXBy21o9ABSR4dZdgOQc5yezrhGQhINp5VRJ7vSsOyGLxSNSQoFJElqnFbGmTPttR6ys4EzzpDsUZqVK/2KRpdL4tv0PACMHx96se7p003RuGOHdEj7g4BwuyUXUG2txGvq+z4tTeL6Kiqk3E8iudPGipIS4PvfB264QTKqLl7sv9hzfT3w9NPA888D3/mOWB8DjEvElKws+f1dLorGnuIZeyq3oemp7AYzqBKSOPjLmnrRRTJQqKGl0ReKxhQgUgJy7lxg8OAYNpyQQARzTdXMnm0XjatWATfd5LPZ++9LoXpAOtkZGVKrL9RrvaxMYqR275b7af16yVocD7q6TKHY0GDe30rJc6CiQgRjIpaSiAeZmcD554sg3LhR4h5Xr/Z9brpcwFtvyTRjhojHqqr4WW6zsuyW81Smp8ypoVoarWU3CgrMgSFmUCUk8fj6a3lParKzpY9sFY2JkvwsllA0phhOAfnxxzKFKiDHjpX+OQUkiSuhiEZnBtWVK3026eoCFi2SecOQDuG4cfICCef6rqoS0QiIaJwzJ3bCzOuVe7m2Vv7qWC2lpENcUSEuqP0t9i6RUErckKdOFYvSq68C77xjL0ei+fxzmUaMEPF43nl2I3gsYDIcE3+icf16c7mgQGKS09N7rsnWU9kNxpCSWNLYKJ4iyZqsLJo4XVNnzpTyGhkZ5j3d0iLPhkTOFh5p2I1IYcrLpQ7ZVVeFLiC3bZOJApLEjf37gb17zeWsLMli4uRb37Ivb9kib9mSkhOrVqwwRxP1i2L8eOlIhlNz8bTTZJ/WVnnRfPONiM9o4fWKi11trVgWrcXqi4tNoZjqronRoLJSSh798IdSouW11+wj1pp9+4A//Ql46ingssuAyy+XZ24soGgUjh2Td5smMxMYNgz48ENzXW6udAyDuaZat21qkm3z8kRo6udGa6skxukPrukkuenqksGQw4dF5MyaxcGKcPDnmnrOOSLABw+2u5ofPgycfHJMm9evoWgkACggSQLhtDKeeaZ/v7LSUmDCBKnUDsiFvHo1cOGFJxZfeMHc3OWSl0N+fvjXcHq6WKG0N+zatZEXjYYhmlcLRWt2z8JCs0RGrK1aqUphodTxvOYa+d0XLwa2bvXdrqlJYh4XLZKOybXXirUrmlA0Cv5KbaSn291TtVgM5pqqsZbdUEosPNYMxNXVFI0kuhw/Lq6VukSSxyP3OZ/7obN5s30wKTdXwgoAisaeoGgkPvgTkMuXi6EmVAGpk+hQQJKIE4prqmb2bFM0AhLX2C0av/jC3ql0uYCJE2W+N9ft9OlmZY9du8TKUVoa/nGcNDebtRStIiA/3xSKoXR4SXTIyJAY1nnz5FJbskQuUeez0uOR+Nn33wcmTxbxOHNmdBIRUTQKO3bYl085RQZbtNBTynTbDsXSqN3UMjJM0aiTTOlajRMmRK79hFg5fFiuaa9XrkXDkMRM7e0UjeHgdE391rfM8+dM/MhkOHYoGklQ+iIgH3uMApJEgXBE46xZMpKhscQ1Wq2MgFghtJWgN9dqSYl0SnVHdd064Nxzwz8OIK5uWihaa8zl5ppCMT+/d8cm0WPCBJmOHBG31bfeMi0CVjZtkmnoUODqq2UcI5LCn6JR6KncRnGxabEPx9LodoulubhY3oMej7i+MhkOiQYej4Q8aDf4QYOktNM335ii0RJ1QYLg9fp2IazltZyikWU37FA0kpCxCsijR80srBSQJGYcOmTvCWZkiLkmELNn25c//xxwufDVjix8+aX9I2th7t5en1VVpmjcsEGsT6EmxGlrM4ViW5u5PjvbFIrhxFmS+DFoEHDbbVK24913JXGOv0x8hw4BDz4IPPMMcMkl8mytqOj791M0Ck7ROGaM3TU11HIbmmBlNzIzWXaDRJ6WFnFHbW+Xd8mYMeb7yXo9ktDYtMlecicvT6pzaZyikRlU7VA0kl4xYIB/Abl5c+B9/AnIuXOlg0VISKxYYV+uqgpuchs5Ut4C2sekvR3YsAEvLrUnzpk0yXwBZ2ZKGY3eMGaMdCSbm8XCtHWr6fLqj44OUyi2tprrMzNNoVhUxCQHiUpenjwjr7hCwmkXL5ZOi5Pjx4GXX5bP58yROMm+uDlSNMq9ZbX8KQWMGmX3Vg+13IbGWXZDi0a3W/anpZFEkupqCXUwDLnexo+3Z/KkaAwfp2vq7Nn2rMl0Tw0ORSPpMxSQJGaE45oKSE9x9mzpjXezZ+lmfPqpXTSee66Zhn/QoN7HmaWlAdOmmZnZ1q2zi0a3WwRlc7OMdjY3m59lZEjG04oKsWJQKCYPaWniKT1rlliilywBPvrIzLyp8Xrl2lm+XBIpXXtt78q3ZGbK9ePxyDGjETfZ39mzx+4Bc9JJ0uG2WhqLi03RGIqlETDLbuTl+ZbdOHiQZTdI33G7pX+kk7WcdJIkY3HexxSN4dHV5Tvu7Kyp7M89lfe0CUUjiSj+BOSyZeLCGgingJw3T7QABSTxIVzRCEhP3SIaX3o9B7C4AI4da09Y01fX6WnT5Lrv6pLrevNmEYTNzb4v9/R0uWcqKqQNqdi5TzXGjAH+4z+AH/8YeOMNmayDB5qtW4F77pFr48orxX21oCD078nKkjqSLlfogiiZ8BfPCPi6prW1ybkK9d6zlt3IzpZ9tWjs7ATq6iLjYkxSk6YmcUft7JT3xmmnyWCiPygaw2PDBntMc2GhOCtZKSw0nwuA/A6NjZFJapcMUDSSqNEXAfnooxSQxEFtrb2mgTbf9IQlrrEGg/DBjhFAhQFAhg5vvFESl2h6Kxo7O00rYmamael45x0znbcuxKyn0tLwrUgkOSgvl1qPN90E/POfYn3ct893u9payeX0178CF1wgiXMqK3s+PkWjfVmLRqulUddoDCcJkXYPDFZ2g6KRhIthmCWIDUOuq/Hjg9+7mZlmMXq32+5mSXzx55qa4VBBSkkfYPduc93hwxSNGopGEhOsArKuzp5EJxBWATlunJlEhwIyRXH6lUybZvqHBWPyZIl7PH4cr+C78Lo9MjSbm4fhw+XF8fDD5uahiMauLrOYt546O83Phw8Htm+Xl/iRI5KZtaxMmkE3F2IlO1usiBdfLPU9Fy+WcjBOOjqA118Xy+TMmRL3OGVK4Osp1eMaQxGNOs1+OKJaC0x9fouL7dbLAwfk0URIqLhcMh6qr6PhwyX+NpR3RW6uDFq0t1M0BsPjMesoa5yuqZqhQ31F4/jx0WtbIkHRSGLOwIEyWn711aELyK1bZaKATGF645oKyFDiWWeh8cO1+AculnVNTUBuHm64QUZodeyIUv6vqfZ2u0BsbfXNGJyRYVoQJ02SWAhtgWhslI4AIYFQSrL4nXGGWByXLJGajk7RZxjAp5/KdMopIh7POce3w5jKotHrtddgBeRcdXaabmcZGaaVIRxLo3Vbfc93dcmUni5xjYSESkOD9G20pXDcuPASsVlFYyhjqKnKunV2j4CiImDqVP/bMhlOYCgaSVyhgCQh01vRCACzZ+PVD0fAhe6edFMTBk4agnPPNQPdAXEZdHa+d+70TaWvlJk9UU+5ufaR4RkzgA8/lPm1a4HTTw+9uSS1GTEC+NWvgFtuAd58UyyMemDDys6dwL33ivvqFVeIJ4eOe0xl0Vhdbbf8l5RIR1zXuQPEQqi36Y2lsbPTt+xGejozqJLQMAwJYdi/X5ZLS6U/o+/bUNHXLuMag+N0TZ0zx9c1VeP0NqJoNKFoJP0GfwJy2TJ7inQn/gTk3LmMKUk66uvt6Xh1VtQQaZt+Nl6D5a3a2ITrrpOXhrV4r3OEETDjHcvLpaNZVCTB8j3FIk6dKtev1ysdg9paXpckPIqLge99D7j+esm2umSJWQfUyrFjwLPPSmzkU0+ZCVqA1BSN/lxTlbK7kYZbo1ETqOyGxyPnnbUaSU90dEiym+ZmuS5HjhRPlN6ELjAZTs+43cDKlfZ1gVxTAVoag0HRSPolkRCQOokOO+pJgPOJf/rpYUWmLz36LRzHZyeWi9prcPGMOgADbaLROcLY1iYvnKwscTkNh4ICuQ71Nbt2LXDRReEdgxBABje+8x3gvPNk7GTxYmDVKl8X6epquc5mzaJotOIvnjHcGo1WgpXdOHRI5gNZMUhqU1cn8e56kGH8eBkc6i0UjT3zxRdSC1dTUiKpDgLhr+wGEfhYI/0ep4D8+GNxYQ1FQD7yiDyUtQsrBWSC0gfXVJcLWPxWrqi41lYAwNV4FTnrAQy7PKho1Om5e/tSnz7dvE6//FLqQYbrfkSIxyMDGG1tYuW+/nrgW98C3n1XYhvb28VKUVxsWsYpGk38ldsoLJRzk5YW/j2py27k5oo7e16eKRq9XulkhpLhlqQOXq9cl4cOyfKAAVJOo6/Jaygae8bpmvrtbwf3FHKKxiNHzJjlVIeikSQUAwdK4odrrhF3Px0DGUxAfv21TBSQCUwfROP770uyARSXAK2tyEEHrsRrwKrB8F56ua3chjMutq+icdQocWutrxerxldfBQ6+J8mPYYhQ0QKwvd133t86t9v/8UpLpfD3xo2y7PGYsY8UjSb+LI15eWJ9yMkJ3y3QWnYDEGujtf7bgQMUjcSkrU36IK2tcq2dfHLkrg9dY9TtpoXbH52d4pVhJZhrKiD3dWmpOcjk9Up/01/4SqrBy4skLBUVFJApQVOT2SvWzJkT0q5dXcCiRd0LxUXAQeBSvIlCtAIrV6KhweyQFxb6Fk/XncySkt41XSmxNr7/viyvXUvRmCx4vSLqgok9f+u83si2QwsYQDpIR4/KfKqKxoYGu0UxOxsYNkzmraIxJ8cUjeHiLLtRVGRPVMS4RqKpqZE45K4uuW7Gj5d3TaRQSo57/LgMTDrfYanO55/brbDl5aGFmgwZYn+O1NRQNAIUjSRJoIBMYlautPe0J0wQ354QWLHCdAdCcTEy4MG1WCzLa9eiZm8HAOk1Ol1TOzvlJZyeLvUVe8uUKZJFtatL0vEfPsyXT39Du3+GKvza2+XacMYURov0dOkY5uXJpOdzc+Va+uwzuUW8XjNpQ3q6TF1dqWWBcFoZR40SS4xh2DuBWiyGG89o3Ue7FhYVyTk2DOnEUzSSri7gm29Md/FBg4AxY6JzH2rR2N5O0ejEn2tqWlrP+w0ZIv1DzeHDHPAFKBpJEkIBmWT00jXVMIAXXrCsyM7Bd0q+wMDGblOM242aT3YAkGHHYPGMvclqp8nLk2tKJ39duxa49NLeH48Exur+GaoADOb+GQ2ysgILwEDrsrICX4OVlcD/+39m+QhrjG5WlvyfLlfqikbtmqrPA2CPYeyLpbGrS36f4mK59rq65Dyz7EZq09oq/Y32dhEoY8ZEd6CQcY3+6eyUmG8rPbmmalh2wz8p8hohqYpTQH78sUwUkAlEL0XjF1/YC3wrBVw/twZ43VxX89leYFzPorGvVFWZonHzZsmEqeOhiH+8XrHmhWr503+7umLXxtzc0AWgno+0eNO1RbVotNYitIpGqxtrMhNKEpySEjNzam9Eo7XsRmGhmUHV7Zbfl5bG1OXgQXnveL3ioTJhQvTvPdZq9M/q1fZ6rQMHyu8RCiy74R+KRpIyVFQA114rU28E5IQJolcoIGNISwuwbp19XYjxjDYrY/duwwaPPyEaDQCHv6wDxslyNEXj8OHywqqrk47m5s0iJFMFjyc818+2tti6f6alhWf5y8uTjloobk7RprTUbjk7dsx0R03FuEZnHctIl9vQWMtuFBbKoJTOoFpXJ8fvjSAliYnHA2zbZsYUDx0q114snhG0NPpn+XL78ty5oXsNDR1qX6ZoFCgaSUrSGwH51VcyUUDGkE8/tZuOTj3VV9354auvpMSFlRtvBNA568RyKwpwfG8dYBjIylYoKzO39XjExSgtzbQi9AWlRCS+844sr10rCXL64vYaD7T7ZyA3z0BiMJaiRZdACEcABnP/7O9kZIgIamiQZbdbhOPAgaknGtvbxdKjUQoYPVrmraKxpMTsYPdW2OXlycCSHjwoKDBFIyDtOPnk3h2bJBZNTVLiq6ND7sfTTpP7L1ZQNPrS3i6x3lZCdU0F6J4aiJiJRqXUBQAeAJAO4CnDMO51fJ4N4DkA0wHUA7jOMIy9SqmbAPzGsunpAKYZhrFRKXUDgP+EGA0OAfieYRhHo//fkGTCn4BcvtweBO2EAjJGrFhhXw7RNfXFF+3LVVWiN+GdIKbDpibUYDDQ0Q7U1WHQ9AqbaNBWxsLCyI0Un3468MEH0qmvqZFOZTzT8lvdP8PJABpL98+cnPBcP/PyUid2z8qgQaYrttstmTxTUTTu3m23TldWmh1qq3tqQYFcx5mZvb9e9HG16PRXdoOiMbkxDPmd9+yR+cJCCWnprfW6t+iyMZ2d8lzvDx4Q8ebTT+2uqYMGAWPHhr5/RYWcR52D79gxOV6qh5XE5PWqlEoH8DCA8wBUA/hCKbXUMAxrt/xHAI4ZhnGKUup6AH+ACMcXALzQfZxJAN7oFowZEBE63jCMo0qpPwK4A8DdsfifSHISCQGpYyBjOdKYtPQinnHPHt/g9xtv7J5JS5Oq6O+8I6IRAA4cwOCL7Wo/kq6pmtxcuT509ZB16yInGq3un6G6gcba/bMnsedcl5vLzk+oWEfFPR5xjxw7NvVEY6B4RsBuaczNFU+CvriPsuxGauNyiXVRD0YMG2Zm6o01Ssm1rEsA9SXbd7LgdE2dNy88b5L0dOkPWhOL1dQAI0ZEpHkJS6zGZGcA2GkYxm4AUEq9BOByANau+OUwBd8SAA8ppZRh2Lo1NwDQVddU95SvlKoHUATA8cogpPf0RUA+/DAFZJ9pa5MiS1ZCEI0n6jJ2M3aslL04wezZdtG4fz+GDJlu2ycaohEQi6cWjVu2AOefb++4Ot0/QxWA8XD/DNX1MzdXRmcT1f0zEaioEIuZLvuwf7+sp2g05521GyMtGouL7Vl4KRqTl2PHRDC6XPI8HDtWElLFk9xcsxRQqovGtjZgzRr7unBcUzVDhthF46FDFI2xEo0nAbAmoa4GcGagbQzD8CilmgCUA7C6m14HEZcwDMOtlPoXAJsBHAewA8BPo9J6kvJQQMaBzz6z98JGj+7RNFdTIzURrdx4o0OwzJK4RqtotFpqvF7JvwNEXjSedJJYhWpq5F/729+ks28VgPFw/wzV9TM316xNR/oPOoOqjqnTJR8oGuWvYdgtjdrFrC9uhHrf9HR5vhQVyb2r3QMpGpMPwxBPFj0oU1ICjBvXP1wWGddosnKlveswZIiUPQmXIUOADRvMZcY1xk40+htjdjpGBd1GKXUmgDbDMLZ0L2cC+BcAUwHsBvAggP8A8N8+X67UjwH8GACGDx/ei+YTYkIBGSN64Zr68stmDAIgWUtnz3ZsdMYZcGXkocEjmW/SGusx0H0IgKRLa2kx06VHOj5OJ8R56y1ZjlTH0ur+GU4NQLp/JgdaNOoOo04Gk0qisatLYhqtaNHY0mIOxuTmms+IvlgarWU3CgrMhFkej6xnrcbkoqNDrItNTfIcHzFCpv7iQUHRaOJ0TT3nnN79Ts6yG1arY6oSK9FYDWCYZbkSkrjG3zbV3fGKxQAaLJ9fD9M1FQCmAIBhGLsAQCn1CoA7/X25YRhPAHgCAKqqqmIUxUNSAX8CctkyebkEggIyRMIUjY2NwNtv29fdcIOfl0VeHo5MnA9jo3wwAEeR+fkqYMS1J44DyChyNJg0Sf41bc10kpERXuxfXh7dP1OdsjL7AMeRI/JXi0a3W6wkyXyNVFfbxXFpKU5kRI50uQ1NXp58Z36+TOnppmhsbpYpEtmXSXw5elTKaXg88qwdNy5674fewlqNQkuL1Gi20hvXVIC1Gv0RK9H4BYAxSqlRAA5CBOCNjm2WArgZwGoA1wD4SMczKqXSAFwLwFqg7SCA8UqpgYZh1EGS7ATpqhMSXSIhIOfNk3qCKS8gOzp882X3IBpffdXeaayoAM491/+2NRPmAxslyGkwaoBVq+SHQ/TiGTXZ2cAtt4ibkz+BSPdPEi5lZfbrpq5O/iol691umaz1HJONUJPgRKLchiY3V46ts1cWFtrLblRXSzZNkph4vZKVWFvuy8slfrE/PqNpaRRWrbLfg5WVZtmdcGHZDV9iIhq7YxTvAPAepOTGM4ZhfKWUugfAWsMwlgJ4GsDzSqmdEAvj9ZZDzAFQrRPpdB/zkFLqdwBWKKXcAPYBWBCL/4eQnrAKyCNHTBfWUATkQw+ZAvLb3wYGDIhZs/sPn39uz5c9fDgwcmTQXd5/37783e8Gdi+tGT4D8jjqFo0rJUjFMMQ6AERPNOpj25LzENIHtHuqpsHio5OVJYLR5Upd0WhNglNUJI8WnXGyLzjLbhQX27+LojFxaWuTkJPWVrPeZ2Vl/7XW62tRZ8Xur+2MNsuW2Zd765oK+Lc0pvK5BWJYp9EwjLcBvO1Yd5dlvgNiTfS373IAZ/lZ/xiAxyLaUEIizKBBImC++93eCciJE00X1pQRkE7X1Dlz/G/XTW2tTJqsLODiiwNvX1M+ATbRuHE/0NqK4yiAxyOdwP6Q3ICQUCgvlwESpaRT09IincecHLkXjh9P/rjGUC2N+fkS3xgJl27dUdfPiqIi08oLMK4xUTlyBPjmG7lOcnNF+BcWxrtVwUlLk+uws1Pu/VjXiuwPNDdLKSsrvXVNBcSVXZ9TQAYSWlpS2+WcaRAIiSFaQD7yCPDSS8C//IvERwRjyxYRj9deC/zsZ+KGefRo8H0SnhUr7Ms9uKZu2WJfHjs2sBXB6wWOtBcBZZIjfTBqpHewZk3UXVMJiQY5OSKGtLXR6wX27pX5VEiGYxi+otGaLdFq/dPPhb5aGQH/tRqd7qkkcejqktjFrVtlvqICmD69/wtGjdXamIp88ok9+/iIET06KAVFKbqoOqFoJCRO+BOQY8cG3yclBKTbDXz6qX1dD6Lxq6/syxMmBN62vr67Yzd8OIrQjDx0B4GsWhX1JDiERAtnMpw9e+RvKojGhgbfkhonnWQuWz/THetIWGKsotEwTNGoq0tTNCYOra1ipaqpEanGnjcAACAASURBVKvdaaeJhTHSGbSjSarHNfpzTe0rTIZjJ4FuB0KSF38urMuWyahnILZsMUXkxIlmEp2Ed2Fdu1b8QDRDhth9zfzgtDROnBh42xMP/eHDMXijJYf2ypVo6k6cQ0sjSTSccY379snfVBCNTivjySeb5WS8XjNOGTDPUSQsjbrshmGIuCgulu/zeuWzAwcYA5UIHDok15AutTR+vPxNNFJZNDY2AuvX29fNndv347Lshh2KRkL6GVYBWVNjxkCGIiAffFAE06WXAuedl6CdFX+lNoL8Ix0dvp3GYMknTjz0hw0T19Ru2j/dAFd7FzJz0pGXF2abCYkzzgyq2sqViqLROsbU1GTWZbRmN41UzJcuu5GXJ8fW2WrT0yUWqr4+CQbykhSPB9i+3YxD1eOT6enxbVdvSeWyGytWmBZ+QBIXRaIsOy2NdigaCenHDB4MXHedTOEKyPZ24PLLY9bUyBFmfcatW81OISAZ7oK5l54QjeXlGFzSCXS7rjUdTwd270bxzDEB9yWkv+K0NOrOTaqLRme5DR3vFQlLI2CW3cjLkyQZxcX2uMYDByga+yPNzZIdtaNDROJpp0kMYyKTypbGaLimAr6i8ZCzwnyKwZjGeLNhgz1Kn5AAaAH56KPAokXAbbcFj4F8663YtS1ieDzAypX2dWHGMwZzTTUMi2hUCoNnjjrxWROKgS1b6JpKEhKnaKytles91UWj9fUayRqNGmfZDSbD6d8Yhgj5DRtEMBYWAlVViS8YgdQVjfX1wKZN9nWRcE0F6J7qhKIxnhiGZDQZOFAC0u6/3/ftR4gfQhGQO3cmYJKcDRskI4GmoqLH7EDOeMZgSXBaWsxwyexsoHSeWSyxCcXA5s0UjSQhKSuTOD4dy9fSIrdSsovGtk07UP3JbuBIDQDjRE09jdXSqAWdjkWMBD2JRpbd6D+4XMDmzcCuXdL9qqwEpk5NnvIUGRkycOT12sscJztO19QxY+yJsPqCP9Fo/a5Ug6IxnmzbJk+vri7xOfzlL+VqHz8e+Pd/F4uLNX8wIX6wCkinYPrss/i0qdf4q88YJJ7RMMTFyEowS6N1lHDwYEDNngUAcCETbchD+uaNKCxI4TcCSVjKy+VW0dbG9nYZgdf1Gz0euxt3UtDait3n/4tk/dm6Fag+iGHD7DVWraIxkuU2NP7Kbrjd5ucHD0buu0jvOXZMcqw1NMg9MmmSWKTTkqwXnIrWxmi5pgLidm4tueLxJOBgfARJstslwXjzTf/rt24F/vhH4OyzpWd7881SW6GlJbbtIwnHzJn25dWr49OOXuNPNAZh/377bVFQILWZAuEUjZg2DcjJESsjgKKju6AO7A+z0YTEn7Iy+WsVjUePimBMWmvjO+9g55ECc/nQQZ9Ey1b31EiW23AeU2dR9RfTSOKHYUj5mU2b5PovLhZ31PLyeLcsOqRarcajR8V6bKWHiJawYTIcE4rGeFJWFtyXDpA74rnngGuukWj6888HHn5YesuEODjrLPvyunUJ1FHs6pLqvFZ6ePr7c00NljHWRzRmZwNnnHFCNBajyTemkpAEQHeCtWhsaxNLI5DEonH1auyERSW2tWHMgAbbJs76jUBkLY3p6XLcvDx5hBUVyV/twnboEB2G4kVnJ7Bxo1l+ZsQIYMoUuyU62Ug1S+Py5fblsWN9RV5fGTrUvkzRSOLDLbdIr3fXLuDPfxaberBKsi4X8P77wB13mE+/3/4W+OKLJPQ7Ir1h9GgJkdXol2ZCsHmz5MfXlJUF9zWFbxKcnsZgfEQjAMyebReNq1aF2GBC+g+FhWZMEyDWrqTPoOoUjQBOObrmxLzbbXoiKGW+XiMdw5abK+IxJ0fOf06OaW3s6mLyjHhQXy/uqE1Ncv1PngyMGpWgZajCINVEYzRdUzUn+grdUDSS+DJ6NPCLXwAffigFgxYtAm68MXjdAED8Lf77v4EZMyTq99ZbxeXVWhidpBRK+VobEyau0emaevbZPQacOC2NwTRmZ6fEswByWC2uu86ahVYUQMFAEZppaSQJiVIyzmIdd9SukUkpGjs70bVuI3ZjtG31ydvfPjFvHYMqLjZjDSNpaQTMjrqu78oMqvHD65UkcJs3y+9dVibuqKWl8W5ZbEilWo1HjvjmNIi0aypA91QrFI39jZIS4PrrgRdekJzpy5ZJghxnoIaTmhrgqaeAyy4TP6XLLgOefDK1r+4UxSkaV69OkGxfYdZnbGqyxwspFTzR6pEj5vzAgWbnumniLBhQKEQL0uEVJWr1aSMkQfBXq7GrK0lF4/r1OOAeBDfMf7gc9ShdZdYaina5DY0zVtKZDIdxjbGhvV0ScFdXy/vg5JMl4U2kMuUmAqlkaXS6pk6YEJ3SKSy7YULR2J/JzJRiM3/6E/DNN2aCnNmzg1tgOjrE4vjjH4sz9owZwO9/L5bJhFAPpC9Mn25/SdbUJEAIrNcrebOthFmf8ZRTgrud+XVNBdCkSoCRo8Q1FZB7JOEyCBEiojEtzRwQOX5cxj+SUjSuXo3DsPfmRmIvsHevZD6BfeynpMRMDhIt0aiP60yGQ0tj9KmtFXfUlhb5HaZOBYYNS353VCdZWeIq7fHYBy6SkVi4pgK0NFqhaEwUtBnlN7+RZCFHjgB//askyCkoCL7vF18Ad90lMZAjRkhM5HvvpVYhnxQiO1temFb6vQb6+mszawcgva7Jk4Pu4hSNPYQ/BhaNTQAmTTJFI8C4RpKQBMqgmqyisQFltlUD0J0L/6OPANhFY16ejAfpTnUk8Vd2g6IxNnR1Adu3yyukq0u8SKqq5DdIVVLB2nj4sPzuGqWi45oKAIMG2Qcfjh5NfkEeCIrGRGXAAOD73wcWL5Yr+L33RAwOHx58vwMHJPvqBRfIMa65RsRnXV1s2k1iQsKV3nC6ps6e3WPPzl/m1GD4E41eL9DcDF/RyLhGkoAEyqCarKLxGOyBaqXo9kftFo1W91QdbxiNQu7OshtO0Uj31Ohw/Diwfr0IiLQ04NRT5T0QLJ9gKpAKotFpZZw0KXplVDIzpbusMQx7uEsqQdGYDGRnA9/5DvDgg+Kas2mTuKPOmBF8v9ZWqf+4YIH0omfPFvfXrVvpxprgOOMat2zp52U+w6zP6PEA27bZ1wWzNHZ12R/yWjS2topwzDtjPDJh6eV9/nmS9bBJKqAtjbrT3N6epKLxwAHg4EEfS2MZujNdffQRYBg2S6N2HY20aypglt0oKJBnTWGhXTTW1tKxJ9IcPiwlpY4flwGBadN8SyOkKqlQq9EZzzhvXt+PuWED8Nhjcl05oYuqQNGYbCgFnH468F//BaxZI0WinnxSEuMEG2L1esUl79//HRg/HhgzRhLwLFuWunb4BGbQIGDkSHPZ6xUv5X6JYYSdBGfnTnsHuLw8eAD80aNmrbTiYvNW0NkVi8cOtfc4dEYFQhIIp6UxaUVjt+vECUtjYRGQlm6KxpoaYOtWm6VRn4NoiEZAnik5OfI4S0+XZWt9xoMHo/O9qYbHI66o27fLe23wYInj7ylKJ5VIdktjdTWwY4e5HAnX1OPHgbfekkfH22/7PitZdkOgaIwz7e1RfpEPGSL1IN94Q3rOb74ppTl6qn66axdw//0SWVxRISVAFi1iVskEwumi2m9Lb2zfLkPxmvx8GTYOgtM1ddKk4AkPrK6p1ktfX84lpQqYNcu+E+MaSYIRSDSmp8vk9dotYAnLp58CgGlpLCkBSopN0Qig873lJzrNGRnRq9Goyc2VZxDLbkSPlhaxAtXWyvU8bpykeoh0jGqik+yi0TnGPGVK30uqbNpkDvJ0dfkO8tDSKFA0xpnVq4E//EHCCj/5RAyDUfMMzcsDLrkEeOIJeYN98QXw29/KHReMxkazduTAgSIk//xnEZak3+J0UV2zRjqN/Q5n1tRZs+x1A/zgTILTm3hGw7BYGosh7tlWGNdIEgwtGjMyRMC0tUlHu7MzyayN3ZbGE6KxuAgoKTVjGgE0/tN0rSgujl7mVI2z7IYzgyrjGvvGgQMSv9jeLlbF6dPFo4b4kuy1GruTI5/g7LP7djzD8HVJdQ7yOF2fU1U0pni4cPzZvVtGNfbskenDD0XbjRolNYZOPrm7Qxtp0tIkxVhVFXDPPVKT4a23gKVLxSU1UM/C45HPly0D/u3fxJX10ktlOussDvn1IyZMkNgaHcvY3Czhqj0JrJgTpmuqYUjhZiu9yZza1iaXc3Z290vWn6XRMFIvZztJWEpKzMs1M1PEotdruqhqzxZtDUtIOjpOuI6fEI1FRUB2js3SeGzV10CVF0hLQ2mpKRqjaWkE5FnS3CxNssZR09LYO9xuiV/XybVPOkn6RcGqjqU62dlyflwu6V8mW7esqcm+3NdY1n377MnbAd9BHrqnCrzt4khXlzwQ29vtFqC2NrGkLF0qHqIPPig+1tu3RzGYfvhw4PbbgXffFTfWV18Fbr7ZnjLKH19/LabS2bPlrlqwAPj73yXDCIkr6em+uZD6XRbVXsQz1tXZH/BZWVKjMdhX+BONNisjICU+8vPNDWtrJXiSkAQhPd28npM2Gc66dYDbjU5koQ15otKyspFeXIDCEnMcvLFZnVBthYXyf6elRa/QuxaN2dnyt6jIng6AojF8Ghul9mJ9vVzPEydKugUKxuAoldzWRqdo7KthxV/imwMH7F5/dE8VeOvFkfR04Hvfkz7yyJHmS8blsl+s9fWSzHHRItFnzzwj/ewDB6LkblhYCFx1FbBwofS2V640E+QE4+hR8bO9+mrxk7rgAuCRR+iXE0ecLqr9TjTu3m0PHsjNBc44I+guTivj2LHBU6w3N5svzpwc8wXjIxozMnxPGF1USYKR9MlwnElwugvylZYpqHlzT2x2DKUn/Ni0ZTU7O3qOAz2V3aBoDB3DMBPBd3bKM7qqqucxbGKSzHGNTtHYl5qcbW1i+9A4B9s0AwbY+xktLbJvqkHRGGe6usQvf/Royf1x7rnycBwyRDq47e32F4/XK56ky5YBTz8tFTJeflnCExsaAn9Pr0lPF7e9e+8V8+fOnWL+nDcvuM+DyyW1I3/6U7FiTp0K3HWXDBv2y8C65GTGDHsnafdue86ZuOO0Ms6c2aMpwBnPGK5rqj4f+sVTUmLZ2BnXyGQ4JMFI+lqNznjGIhn1KS2FxNt304gSH9EYLddUwCy7kZ8v7/WCAvurrrGxn5c96id0dopY3LtXxOOIEZJ2IVqxqMlKMovG5mb7cl8sjdYEOCedJH1xjXWgRym6qAKMaYw7JSUS0O12SyHiY8fk4ajrben6cg0N8llrq+mvDkicxtatMgHy4hw9Wnz+R42Kwkvy5JOBf/1XmY4dE3fWN98E3nkneGbVjRtl+v3vRRFfcomUAZk/P7pv8hSnqEhiGK3ZRj/7TE59yLjd8vsWFEjEeQ9JasIiTNdUwDdzam+S4HR0yJSR4YjvYjIckuAktaXRMHwtjcViZigrg000HkOpBCt1dSErKx1ud/SFR26uOOq43eazxRoWffCgeEYQ/zQ0SF/G7Zbrddy4vmfFTFWStVajy2X/n9LTex+j7UyAM326lN745htZPnDAnidyyBC7kDx0SLrEqQRFYz8hM1MqW+hac21tplDMyjIDfdvbpRPc2CifezzyuX4pHTsmN8G6dbJOj5ycfDJQWRnhgOjSUuCGG2Ryu6WD/eabEowZLLPq4cNSO/LJJ+XJdt55kkjnkkt8h3JIn5k5sw+i8fhxMX/reh0DB4pP9Q9+IHUu+opTNM6ZE3Tz9nbfS6s3otHqmmpzVzvzTBmR0SaC7dsliHLgwOBfQkg/QQ84aleqtjaJHNAiMqFF4/79J4b3G1Am92p3gb6yMojKGDQIxpEjYml0u4BDh5CTMwzHj0d/fDI3V86zPteFhTLwq3+LAwcoGv3h9YpRWEeylJbKTxmt+NNUIFktjU4rY1FR713O9++XZyMgxpiJE0UIapyRVc64RmvfIlWgaOyn5OXJVFkpD9TmZtMSqR8GumRAba0pIq31qAxDRkWqq6WqQVaWxE7qrKzl5RGM78jMFJfVefOAP/1J0p0tXSoicvXqwC6p7e2y3dKlsjxjhgjIyy7rufgeCYmzzhJ9rlm/XlyAdMKGgBiGiENrgce6OnFPvv9+8adesEAGDXoTbLJvn0yarCwRbUHYts1+KVVW9uyaElISHE1hoQwtrl9vrvv0U+Dyy4N/CSH9BC0a09NFU+mMqVosJrRotARlH0Op3K9K3G7KyiDvi3POQfui1+GCKI6s/TthGMMAxMbSCMi72+WSDu3Ro+Y7mXGNvnR0SExZc7P8fKNGAcOG8dXfV5I1EU4k4xmtVsZJk0wDjR43rquT61OfSybDoWhMCNLSumsXl8gD1e02RWJurhmT1dUlF3lDg0wtLXZXVpdLzO7a9F5UZArIUaPsiSP7hFIyTDhunCTQqauT9K9vvilxjsEyq37+uUy//a3EQl52mYjIb387BJVD/DFqlFiwdSxjZ6d4CvegzyTr0uLFgT9fv16mX/1KfqcFCyT5UbCsNFacVsYzz+zRFOB0Te3J2NnRIQMtgHSitcEwoGgEJIbXKhpXraJoJAmDdk9VSsbydKdRx9Mli2hsQNmJeEbA4sZ4zjk4tsh8tpTs34TOznkAYmNpBKSTqUWjddCKOeHs1NWJM4cufTR+fJRKjKUgOTnyDNBld5Il42yk4hnb2+0JcKZPl79ZWZJn5PBhGTc/eNB0QWVMIxPhJCSZmdL5Pe00sSKdeaakoR40SNxRx4+X0KxzzpGLvbRUHhqdnfasrM3NUu5qyRLgvvuAxx8HPvhAkqVYk+/0mYEDpXzHkiUy7Pruu1LeY9iw4Pvt3w889BBw/vliybr2WuC550x/AhISSomLqpUes6i+8w7wn/8Z2he43VKi5dJLxfT361/7qjt/rFhhXw4hntGZBKcn11RrnbSBA0U4ut3idZuWJoYKHxjXSBIYLRoBu2jUnS232/4eSCicorHYNDNoCyvOOUdcU7sp3b0eHU1SqyralkYdW6W/p7iYGVT94fXK4PVXX8n5GTBAEgBSMEaOtDQR4oaRXHGNkSq3sWmTeW8OHWq3Ilq7ptaBHloaKRqTgtxcEYsTJ0p/d+pUcUMdOFD+Tp4sXqMzZ0qfvqBABKRTGB4+LP3j554TI9Pf/iaeeUeORLCTkZ0tIvDhh8U1ceNG4J57eiyzgNZWEZ033yzq+OyzRelu25bAPaDY4a/0RsDTtmOHuJxaNygrkzf8okXy+wXyHTpyRNyTJ02SXsDDDwdO6xtmEhzDCD9zqvWh7nRNLSoKMPo66/9n78zj26jO9f+MJMuW9y2LHW9JyA6BQBKSsBRCWygBukCAkkJLF0pv6S2Flu7tbbltoXt/dKOXFloKlwKl7BQuJOwJSSAQCEnI6iXe912ypPn98fj4zIxW25Isyef7+czHnrGWkSydOc95n/d9TzPv79yZfh4fRdpiFY2iLHxXF/d13dw/MGUYGuIq5yghI41z56Jr1pKx4zneLvh27zGlbsQLIRbDtd2Y7perwUHaAhsbOf4uWMBxPJb11RQkHfMaYyEagxXAMRKtaGxunn7fZ2VPTTM0jV+iggIKRq9X5kJ2dkr/t9/PgF1HB62u3d1mK+vICLtriN7mubmyoM68eSEiNBM52RNP5Pbd7/Iq8sQTzG989tnQy2N+P9Xtyy8DN93Ezu7Cxnr66fGfGaQgK1bw/+vmgjtaW1nSfO5cyw37+mjFNI7MNhv7uixdyu3yyzn7uftu9vIUfmcroiLTDTdI++q55/L/09goP1wAj1nDoRbq6szO5txcOpjDYbSGiQE/rDUV4ApMTQ3fIIBfhp07uVChUCQ5YxE38GvV08OJTUcHhZXoBZxyRUZef92kwLpyKk0vYux1axq6TzgDaKEvPQcDwK5dcL3v5LiforHthtdLEWlcmBoa4nXYKOynE83NXJP0+Sholi0bq2OkiAMuF+d+SjSaqa+nNRrgEGJdfK6okL+LhR5N47w3O1suxLndfH+NY266oyKNaY7DwYjjwoXSyrpwIXPcysuZdrh2LTtfLFggL2ZWK2t/P7B7N/CvfzGQ9PvfMz3x4MEYrlqXlwOf+xxzHzs6KB4/+9nIFVUPHgR++UuGU2fOBDZtAu67L3wLkGlGZiaFo5EAi6rfD1x1lezfIvjZz1hB1UhFBfDNbzLS++qrwDXXhM5I93gYJb7gAi7h3XQT8Je/mG+zalXEpNq33zbvL1sWuVjCuIrgGLFGG1W/RkWKkJkpbZIZGTI1ob09xdtuWAaszvwa075x4tZ93Mqx3zPhBt54I2F9/kTbDa+X45O1HcB0tKj6fLys7Nsne1OvXKkEY7xJx7YbwaqnjhdrARxruYzCQvnZdLulwNQ0ZVFVIZlphsvFrbycorCvT7b2yMyUYfn+fjoNe3qo3wCzfaS1ldvWrVxdraqSUciyshhUPsvOZuTwwgs569m5U7bz2L079P26uoB77+XmcLCFg4hCGru2TkPWrjUXQt22DbjiCsMNfvQj4OGHzXfatAn4yldCP6hImFy7lhVVH36Y0cdnnw3u22hupgi1MoF8xkjWVFEYSjBrFo/19fG0w15sTj8duOceua/yGhUpREkJV8PFmD04yKFRtFxKddE4hCwM58pwncNhXnPqKl8G4P8AAFkYRv++fXD5+gHEX6W4XLx8iYbhLpe5EEl9Pc0104W+PhYcGRri52/BAtVZK1Eoe2ogQ0PmuYTVmgpwflBZKdfP6+tlO7yyMnPbr+bmyLUV0gklGqcxYuKcny+trN3d0soqVlr8fu53dPBv3d1csRYXQZ+PPZaOHOF+djb1mbCzTjq53WZjK47Vq4Gbb2Yu5OOPU0Bu2RI61On1Aps3c7v+en6zhRA99dQYN61Mfqx5je+8w1W7/Hzwvfze98w3WLEC+NOfol8ByM6mCr3iCo6ywr564EDk+0bozyjO10ikgbqtTU7ciopoFevuppbNzY3gYrZGGl99Nb1K0CnSmpISfgVtNn7Oh4b48RURh5QTjboe2G7DkM9YXCyHKV0HC+GUlAId7XBiBPD7kPXWa8Dyc+J+qi4X33cxYRdtN0SUdzpFGhsaWFjP76eoX7Zs4o3YFeNnOojG8UYad++WLveyssDIocAqGoW4VJFGhWIUh4NVzETLveFhGYV0OuXxkRFGGUWu5MAAo5Tioj04yAm+mOSXlkoBWVMTg84Z1dXAF7/IrbcXeOYZRiGfeEKGRYOxZw+3W26hZ3fDBkYhP/CBaeGTmTmT/4fDh7mv68COHcA5c/YBn/iE+calpfQiT/QKX1nJ6qvf/CYF1113MS9S1P03YrMFijQLPT3myZbNFrlJdjhramFh4O1NLFvG1Q5xh64uXkGm05KiImUJVUF1YIA/U64QTm2t6QvdmVkO5MrQotGa2tc3ulg0dy5cHfXwj2bhZG17HrgyMaJR/PR4OIw0N0vROB3aboyMsJWGKHReXs7SA2rNLbEYezWKvLxUZzItN4IVwAn1nhjzGo3fWWuUvLEx+udPB5RoVIQkK4uDvdHKKqKQTifrhQCciLS2cn7d3s7bGq2s7e3ctm/nRaOiQvaHFI1UJ0x+PnDJJdx8Pq5GCxvrvn2h79fWRiFz111UsevXyyikcbRIM9aulaIRALZuGcY5D3zYLObsdvZnrK6e/BNqGgXhaacBv/kNheiddzL6K+yrn/hExOVCqzV1/vzIPdeCiUaR5hrxQmOzAevWsfWI4OWXlWhUpATWYjiicEN/PyM+KRdptOQzdi1eC2jywmHKZxSp7HPnomjnIxgGZ86uV54FcHOcT9TcdkP0ajRWUD12LO6nMKX09NCO6nbzs7dokeyPq0gsdjvnah4P/x+JyuuNJ5OJNDY0yH7VGRnh+zyXl/P98/kYixgc5He7vNx8O+M8YzqgRKMiKoxW1upqfpG6u2UkUuST6Dr329ul1TUjQwpDv59VMOvq6CzNymIFTyEix8qmTwS7nblop5/OniEHDlBAPvYY8NJL0qtoxe2mOHjqKfaPXLGC4vGii4CTT06P5blR1qwxpurp2P4/b8HXehAmo+6vfgWcdVbsnzw7mzmSmzYxcvDUUxTsmzZFvKvVmhpusBdYRaOuy1XKqFYnTz/dLBpfeQX4/OejuKNCMbUYRZS1V2M6iMbO+asAQycf43VjTDTW1CAfPRhGFjToyHxrO2d/cS5dKibmor1Jfr45snvsWHo63XWd1/WjR+XrXro0PYRKKiMi3kND6fG/mEykMVIBHCMOB62owuHU0MAiksqeqlBMALud115x/R0elnZVp1NOWnw+aWXt7ORKt9HKOjxM15/wjhcVSQE5d+4kB7kFC9ju4YYbeAJPPUUB+dRTgctVRnbt4vbDH3JZ6YILKCDXr48c3kpyli5lZb++PgBHjqCvdRDvYilOwKgq+9SngOuui/+JVFcD114b9c2tkcZIAT9dDxSN/f2y1HtU7QZOP928r4rhKFKEUPbU3l5OelJeNM45wSQajSK5q2v0l+xs5NbMBI62IBNu2OBnb9iPfSyupyrabuTmSluqcXLq9bLIXKhcqlTE4+E1XLz3VVW8fqfRemvK4nJxujM0NMlF+STA65WuCYCfr2gzi4aHIxfAsVJZKUVjfT1Fo9We2trKecV0KZGhRKMiJmRlyaRiXecEXQhFsWIDcOASVVk7OzkIGK2sXV0slLpzJweEOXNkVdaKikl8MYuKZJGWkRFGHh99lCLS6Ne00tjIYjB/+hMjZR/4AKOQF1zAcpwphs3GGkDP/m8ro30AtmItReOqVcAf/pCwK73fz89KpP+p1xvoNI5UObWnRxb9cLm46i1sYVGvTK5axQ+nCBMcOcLPg9WfolAkGaFEo1grSynRODQEvPmm6VBnyXGm/aCRRgBZJy8Bjr6NLIwO4ko8+gAAIABJREFUBps3x100Aua2GxkZgd2E6uvTRzR2dnJ89nj4WpcsmV5965KddCqGE6xyarTTlbfflpfy2bOju4xXVsr1KpHXmJnJ8UYskPj9FI7p8n2OhBKNipgjmqDm5XHFUVhZRSRSDGK6LvMghdXVbpciQte5ytPQwAVip5Orl6KoTknJBPVNRgajhuvX04q5d68UkFu3Bm8VAXCJ65FHuGkaq7kKG+vxx6fMsuqaWUfw7F6Z2b0Na3DNrEeBhx5KmH+lqQn4298oYs8/P3zk8OBB8yS3tDRyjow1yqhpUfZnNOJycTnS2KfksceURVWR9BhFo8MhLV0DA3LilDIWyZ07zUmB8+ahayTPdJOgkUYAWaeeCO9D98OF0Rnz5s1xPFGJEI0jI/w9K4uLWOIS0dDAy0cqo+tcR6ur435hIQXjpAvdKWJKOvVqnGi7DV3nMCIIVwDHiLG8hdFWXlZmHmeam5VoVChihtXK6nabC+qISpair54QkL29Ziurx8OKbPv3c7+gQArIefMmWOhT0+jZXLoU+MY3uGT05JMUB08/LcsNWtF14LXXuH3nO7Rbin6Q73tflP7HKaCrC6t//BHY/L8cqyp4RJuHlv95FLPCFADq7eXEJxYvS9dZ6FasfD7wACcf555rjjoLrPmM0ejzcEVwIlZONbJhg1k0PvCAEo2KpMcoojSN4kVUTxwelgH0lJjgW6ypWLsWnZ3mQyEjjaeeiH6bHVn+0Rnz3r1csYrzDM/l4nsrRHlODi8los1PqldQHR5msZveXn6mamq4QJwi66bTinSKNFrzGaMtgtPYSIcbELkAjvXxRRH1kRFpKy8r4+ff+PgrVkT3mKlOKqwzKtKMzExO5JcuZYHKlSsp+kpLaRlYvJjHzz6bZbqF2LRaqnp6mHr44IPsF3/77ewpf+SIeWF6XMycyby+f/6TIdCnngK+8IXIFVVra4HbbgM++EG+kEsvZZ/CcC1AEo3PB3z848g7shvHw6DEFizANn/oZe+6OuCNN7j5/ZM/jX37AnuV7dwJ3HEHFw2sBBONkbCKxsFBDvpO5zjTUjduNO9v2SLLrykUSUpurnkBRtPkmChyglLGohpENBpX+QFzDr0xGpFRmAssWiTtqQC/w3FGLGCKn9YKqnV1oQ0tyU57O8drsah70klcM1WCMTlJJ9E40UijsQDO8cePz1BVWSl/F4s907kYjhKNiilFJDJXVQEnnsjODMuX84taXMyfJ5zA4N2aNdwXth9jMVRd5xf35ZeBv/6VxVP//nfON1paJniBzsoCzjsP+P3veZXftQv4wQ+ocsPR18eI1FVXUYSeeSbw85/LEOlU8e1vM3oKYC1GJ2Jl5UD5nIB5maC+XqZ8Dg9Pvry0309hLzAO+i0tTB3dtUv+v3Q9UDRG0/XCOIjPnj0Ba6pg0SJ+IAV+P228CkUSo2mBFVSFWBTmiZQQjboeIBr1NYGRRvFae3vl2CFyCrFihbSnAgmxqIqJupicWkXjG28AF1/MIflvf2O/3GAtbJMJv58Fyd95h6+lpISXwnGPqYqE4nBw8/lS5Dsfhom023C7mc8oiFQAx/oeRSMap1PbDWVPVSQVdjsnAMXFtJ0KK2tXF1c1xSDh93PFs6uLdqTubkaRxGrnyAhz4Q4e5H5eHqOZYsvLC/78IdE0LqmedBLwve/Rj/D448yFfO650AkDfj+L7rz0EvC1r7Giq7Cxnnaa9CvFm3/8g0p6lDXYhtvzb+L5gELN7Tbb1RoagEOH+PusWRR1tbUUYRPNhdq1SwZfs7Lo9Ny7lwFdr5f/t0ceYbR4wwb+X43B2sxMRp/DMTQkLy4OBwO/Bw5wf0ITnEsvBXbvlvsPPDCuyq8KxVRQUmK2ZA0N8fPf389jKTGBPHpUvggAyM7G0ILlpnPPzJQizWhNLSwcja6cfDKy7jWMzwkQjUIsOp0cV/PyAjs+dXUBr77KTSCcNosXMz9wwYLksBAPDtKO19/PS+H8+WndzjjtcLm4KDE0lLyZM9EwkXYbxgI4s2bJ/uLBaGri2v6iRVIYGj/nwiE1nSONSjQqkhphZRV99gYGzFVZZ87k7Twezi16eykyhofNg2NfH/DWW9wADh4iH7K6OnguXVjKy4FrruE2MEDh+OijFJLGSY6VAweAX/yCW1ERq8BcdBET+uK1ZPvWW8DVV5sOVZeNYNaqJWjppfrzeLj6vXYt/37smBTcixbx/R8Y4KShqSn8wBuKkRHg+efl/umn0751yikcmB98UNpTd+/mOVgL1C5aFFlnG1f9Zs7kQsSEI40ALarf+Y7cf/55/o9TsHquYvoQqoJqSolGqwVi1Sp09poHgKIiuVhotK2KSKN9+TI4nRogXu+RI9zmzo3baRvbbohexQsWRHa2NzZyE7rWZuNpCiG5eDH3E1nev6UFeO892a5ItG1SpA5G0ZjKkeHxRhrHUwDH7+caFWAubDN7tswB7+ri+2htu6FEo0KRhAgra24uLQN+v2zdIfpDCvr7KUBEdVZNM19oW1q4bd3K41VVsj+kqLYZNTk5FH4XXcST2rGDhXQefdTsi7DS1QXccw83h4MeXBGFjNWEpr0d+MhHzAkNTie0fz2EtdtcePhheXjrVorGY8dkZM7YzLamhtakujoeG2+0cds2acHKy2PrD8GsWcDnPseI465dPNbRwf3eXt5e06JLYLfmM7rdfPl2e/Q9nUwsXEjvtFhxEBbVL3xhAg+mUCQGoz3V4ZD2VGHhTEnRGKQIjvF1GiONIp8wqzCLA9sLL8g/btkSV9EIBLbdEG1+d+6kCHO7Iz+G30+3x6FDLB4GSLfFkiVSSJaXxz6n0OfjdUCMpzNncihMlDlGETvSJa9xvJHGpib5+XU4zJkmVlpb5XdSFL7JyOC8obx8rEMZGhr4PbDZZI2Hri4GKhJUfH5KUV9/Rcpis3GVWVTO83iklbWzUwoEv18KS2FnzciQF1mfTy4+P/ssJxvGqqzjWpkTjRBPPRX47//m0pWwsT7/vPRJWPF6Ga187jngy19mtrZo57F69cT8oF4vcNllcvlM8Mc/AqeeijWASTRu22YWjAsWmHsZlZbyPe3v52r4eOxJg4PMNxWcdVZgdNfpBD78Yb7njz0mo8cdHbzYlZZGl89oFY3GKOOEJ1aXXipFI0CLqhKNiiTGGGm022VOnc/HCU46ikZjpNGUV7h+vVk0bt4MfPrTMT1VKy4Xx0vRdmNwkHb8q67i/+DoURYFE9vhw9EVGnO72aTc2Kg8L48uDKOQnEyvxP5+2lEHB3npWbBg+rQUSEfSRTSOtxCOsQDOsmWhRZ2uy3xFu53fz44OGVGsrJSisb6e37NZs8wRxuZmLqynO0o0KtIGp5NfZOEaHBiQYlHktgG8iIvWHl1dvJ0xSjk4yIiaKMBSWioFZE3NOHNMamqA667j1tvLQjSPPcZlY+vsx4g4gZ/8hEu8GzZQQH7gA4GdokNx002B+TvXXTdmVV2xgq9FrK41NFDXzpnDlexgFtS5cxk8FdHGaG1SL70kn6e0NHx56hNOoFi99175Fg0OUqhGY4sKJxonzMaNrFoheOEFPpHVp6JQJAlG0ahpskCMsGsnvWgcHDQv1ADAmjXoesV8KGS7jSwp2HDOOcD3vy//uHmz7EESJ4RoFGK9t5eTUdGLWDhbNmzg391upgTs3SuF5LFj0T1XXx8jmEYr3owZ5vzIhQuju3QcO8bIpt/P2y9dGv0lR5GcpEuvxvGIxvEUwOns5DwwM5MC8eBBs2g0LpALcTl7tlk0NjUp0ahQpDQ5OdyMVlYRhczIkFG0gQFpZe3o4FzCKIba27m99hpXXSsrZSSyvHwcQcD8fIqPjRs5k9i6VdpYw1VWbW0F7ryTW2YmJ0AXXghccEHocN/ddwO/+pX52JlnAr/85diu08mB9NVXOcCKYgfve1/ohy0poXDr66OIM1YWC0V3N7B9u9w/55zI71lJCavl5uVJS0pODl2hXV3Mhww23/N6zW07Zs2SF45JicYFC1gE6c03uS8sqv/xH5N4UIUiflgjTSKKlTKicedOc8nR+fOBmTOjtqc6nRSNWVkAVq2SzRIBzvD27aOaihPZ2TK3EeB1pacndAQwM5PREKOboq+PlwajkAy31mikrY3bSy9xX9M4XhuF5Pz50vHh9fK5xPhZXs6/JzJ/UhEf0jXSGC6n8Z135Bg3Y0b4uUpdHX9WVvK2Bw/ye+b3yzmfoKmJ35WyMplKI45PB5RoVEwLjFbWefM4mHR3y0ikWEnVdSksxd8dDily/H7aFGprmRaTlcXom1g1Nq56h8XhAM44g9tPf8okl8ce4/byy4Gl9gRuN/Dkk9y+8AXg5JOljXXFCs4MXn+dBXqMVFTQUmnxhK5Zw+iiyHdqaIhsO507l4Vq6uo4sYg0qXj+eflyKio4YYmGffsoHrOyKOZnzuT7/9xztBJ/7GOBOYptbXJyXFzMcxMV/6JtBBySSy+VohHg+6lEoyJJMUYaATmBstu5EJP0ojGINRUIFE1izB0ZkTnTNpsc6lwuUEGecQbw73/LO27eHFfRKCbqRmdKV9f4bKN5eWxrIbo86ToXMIWA3LuXQk+M3+HQdY7ZdXXAM8/wmMPB65Zw0JSVyQquosicIvVxOvm9Hxmh4EnVvNTx5DQaranhCuD09HBzOKR7SqThdHfz+5qTw5+dnXz/jIVyBNOl7UaKfnQUisnhdPKiKC6MoiqrsLKKC7uIXImCO/395nzI4WFeuPfu5X5RkRSQc+eOIzF64ULgxhu5dXayAsxjj8lKMKF44w1uP/gB/aQXXEBBafShZGUxeTHILGDePDnhcLlovejpCT8YFxdTgPX20spUVRX6ti0tZofZ+98fvSNM2INzcuQqvODwYaZmfvSjfK8FVmuqeOvy8ibeJmSMjRuBb31L7iuLqiKJsYoTMSTYbBzHfD5pl0xKQohGY94iIF+nMQqRny/t8GNj8Pr1gaLxi1+M3flaEM8rbLKaFnju40XTGAmZMYMaGJD5WEYhefCgOUgbipERjs+vvcbHcTi42LB0qbli68yZcXXyKhJAVhbnOUNDqVn91uuVRgFAFkYMhqhCDPAzfeKJoR9XRBnnzJFjYWkpx8j2djm+VFbKBav6+kDRKJ4v3VGiUaGAtLJWVDBS1dtrbu0hBoihIXNVVp/PvGrX1SVzSzSNA5EQkcZBKSzFxcCmTdw8HvqLHn2UIvLIkdD3O3YMuP32wON/+lNQQ39rK19DeTlfZ1YWJw7btzN1Mhxz53KyUV8f/nU995zMpVq4MHrPv66bCz04HKwPdPgwA7G6zkH973+nVfXsszkZtopGYVcrLIzuecNy3HGM5gpPiq4D//xnXCeeCsVEEa0oxPfP45EiUUy+PB4ZEUsqdD3qSKOY1BkFWUGBFMkm0WhkyxbpP4sDxrYbbW1cbJysaAyGpnHhrqoK+OAHeczrZV6isdBOba38LAB86YODsjZbVhY3j4eGCqOporBQWloXLeLvqdy6YTricqW2aBQuAkG4heA33pC/L10aeowbGKCLyWYzO6xKSlioytgjuqJCLoDX17NWoRFlT1Uopik2Gy+ShYUUR6I/jxCRYgASOSodHdLKarfLgUzXafdsaGBQKjOTokkU1SkpiWL11ulkEuA55wC//jWTDoWA3LbNPAsIxle+Alx5ZcDhtjauSOs6Ux2NC/Bbt0YWjUVFnDT09PD1VVcH3uboUbpuAb7Oc84J/5hGamvNq4p5eXzv5s7l9tBDFI26Tk199ChwySVxKoJj5NJLzYkMDzygRKMiKRGWfCGyRK9GYb3y+5NYNB45Ym5qmJMz1m8nVKTRmM+Ym8uxQdjyADAnubBQ3rCri7PAcFW5Jolou9HUFD/RGAyHg+Ju0SJWpAYoEN97jwJy1y7a90ZG+DnJzg7fq7i7m5ebbdvksbIyczRy4cLp0XIgVUn1vMZoi+B4PNEXwBFRxrIy8+c/N1cWCezvl23eBPX1dE0ZaW6Oe22tpECJRoUiAhkZZivr4KA5CimiWD6fjEJ2dTFaabSyut3MPxE1bwoKpICcN0/2FQuJpslKCd/8JidVTzxBEfnMM4GJLevXM1/SQns7taeuU+zNmGEWjTt2RGdbq6kxRxuNEVddZ/sSwfLlsqptNAhrqmDpUvk+zpsHXHst8K9/cTUd4Dn88Y9mS9bMmcy9BGKQzyjYuJHvveDFFzkjVPXoFUmIyMMBKA7EgpbNxpX7pM1rtEYZV60CHA7oeuicxpDtNgR2O3v9GPsMbd6cENEoqrgmSjQGIzubNr2CAi6ofehDfEt0neOoKLgTLhvCSFMTty1buK9pXNAzCsm5c1M3fy7dSDfRGOqa/s475krtodJnhoc5hRIFooxoGhf1Gxs5X8rN5XzC6eSY2dfHMdRYfX5wkMdjNtdIUtTXWaEYJ9nZ3ObM4QW3t9fc2kOkuA0PU0T29nLgGRkxX0B7emRKoqZRd4iqrJWVUVxsZ85k+4yrr+aTbd7MCORbb9E78bOfBTxIezttn7rOwXTuXEYcRNQQ4Mranj3hG+ECnKyJhfuGBrP1dN8+HgM4MTn77IhvqwmraBwNMoyRmwt84hO0qgqXmfFiKAS4KBsfbhV9XMyfz6VLkWUvLKrXXRejJ1AoYkdJCfPbBGI4SPpiOCGsqf395oUhl0sKQ2OkUXz/A6Ko69cHisYbb4zNOQchO5uiUZzzVIpGt5uLhWKcr6nhoqGm0W0CcDhrbjbbWvfvlxPjcOg60wcOH2ZaPcBJ9oIF5h6Sc+akfzQmnug6v7dDQ9yKiqKL8KZ6241oi+BEWwCnoYHv5axZwd8/IRo7OvhdERbWw4fl/WfPlv0bAS6iKNGoUChComkcvAoKpJVV9H8UeYKAFJdGm6umma2sInn75ZcpcqqrZT7kjBkRLrRZWcD553MLQUeHFIyibQjAczj1VFlRD+CcLZJoBDiYvvmmrLrqcMgKp4LVq8efU2jMZwTMRXAEmsZiENXV1G3GlciyMnmRiXnuzcaN5ivTAw8o0ahISqzFcMR4k6qiMVSUEQhst+H1BpkMWvMaX3yRg3bMVpXMCFErsgiGhznRT7QluL2d4m9khO/N0qXBx2SxeFlWJhf6fD5OjI1C8vDh0AW+jXg8HMuN43luLsWjUUhaK/1OB/x++XmIZjPeVlQIB3jN/dznIjt50i3SGOy63twse5va7aEL4IyMyBzEUK04ioqkI8PtZlTRKBrr61kPwioaFy2K/jWlIko0KhQxJCNDVrcDOECLKGRGBge6mhoO+iIXsquLP41W1pERRghElCAvT0Yh580LXTUsFJ2dUjBWVJgrjgJsvWEVjZ//fOTHLSzk4NrVJaONu3ZxkgJwoBVV/qJFRC4FNlv4Nh1VVbSrPvIIJzQAa9bEPJ9RsHEj8I1vyP2XXqLaF40/FYokwToZF+IlqUXjwIC55DLAAQqh8xmtf8vMpGgMEGdLl9KhIfIl+/tZtWxUlMYal4tjulG8dncnTjT6/ZzkivG0uJhCbTwa2W6XKRRiTdLt5rXJ2EPSOGaHQ7zlO3fKY6WlZlvrokXjv8ZNFV5vcGEXjQiM1fO/8QbtxuHIzORn0e1O8qrJIYhGNBrXcpcuDZ3yc+wY34Pi4tCfM5uNf29v51ytvNwsMBsaArNSpkMxHCUaFYo44nLRjmO0sopIo90uxaXHY27tMTxsdpb29XEeJeZSs2ZJAVldHX4S0NlJu6ffT8F43HGBt1m1ioOkWMGsrY0+Va+mhq+pvp7zseefl387/fQocjUtvPuuef+44yLbb1wu4LLLgAMHeEFetkwWbYhJ5VQj8+axcZqY9QiL6pe+FOMnUigmh1U0er0cK+x2jjVJKRp37jSHsY47bmygDBVpdLtlBMXYVzdg3NA0Rhvvu08e27w5rqJRnIcoktHVlZgU6KEhjqV9fXzeefM4/sfCGipaIBkdIH19MmdfCElj9clwtLfTYfPyy/JYRYWMRC5ezI+B0zn5cw+GrnOhNlrBZ9xE9dlE4nTyeyyKxYmCLuEQixdCsIre1KlCpJxGj0fWMABCF8Dx+WQ0Mly7MIDjp1E0GiusNjXRem1EiUaFQhEzjFbWmhpO4ETV1a4uXgjmzOFt+/s5UInWHoC5vHRLC7dXX+UkqapKRiJnz5YTg64uKRjnzAkuGAGutp1wgnmBf9s29kGMREGBLLjx+OOyNHZeHm2v48Waz2gtbR0KTWMFP0DmPmVlmZtrx4yNG81L5fffr0SjIumwika3m0ImqUVjCGsqEDrSaG23EdCj0Ugw0fjtb0/8fMMgimXk5srCaInIa2xpYaVUn4/vwdKl8c+1ysvjWtrKlfJYe7u5f+T+/eaq2OEQlcf/7/+4LyKeRiFZXW2+Lup65GhfqL9HY7eNNVlZ/D6OZ8vK4jXf7QZuuUXmoQoLZThcLvl6U000Rspp3LNHfu9LSoJXdAco7EZG+H2ItKAsxs+uLn4+XC6uX7W1cU5lXaxXolGhUMQNh4O2nNJS7g8NyZxHh0PaJvx+CrLubmlndTikMPR6ZQGCZ5/lxeDkk1kUUAjG8vLQglGwdq1ZNG7dGp1oBCiCGxuZyzhzpixUOJGV4YmKRiNxs6YKNm4Evv51uf/KK1y+FKpfoUgCrDmN/f08Zrez2l+0E/iEEkY0hurRaMxnLCigGNa0EJNoa17jK69QScSpX4SooNrZGX/R6PPRNiomrzNm0Oo5VRVMS0vpNjn9dO6LNlTG/MgDBwKjdbrO65bPx59i27mTfYTFvt3O5ygpYdQ5L4/Xv0QW2rHZJib8srIm1yI0M5MLxE1NfL/q6yNf41M5rzGSPTWaAjji8wdEjjICnL/k50uHWGkpo41tbfy7daHB2PIrXVGiUaFIEsQFpbycg1tfn4xCiosjwAusqMra0cHJn3HFa2CAq7MvvMCqeJWVtFFEupCuWcO2FYI334x+LpWfT0ur283zOu64iVWy93plXqIgWBGcSMRdNM6dS0/vjh3cFxbV//zPOD2hQjF+rKKxu5vtCsWkp709yXqL6fqEIo3BKqdmZYV4XfPmccYoPH1uN59zvCWeoyQ7mwuA8W67MTDAaMvgIMXIccdNbZq1rpvz/YxbdjYjhjU1FJR1dey129DAxcfOzsgtiAU9PbL1EsD/u1iMFVs017CMDLOoi1YAOp1T9/2pqpILBLW16S0arZFGY+S8pcVcrf2kk4I/Rmsr5zTZ2dEXXyopkRXwS0s5nxKtmq15qc3NXMyYzGJAspMw0ahp2nkAfgPADuAOXddvsfw9E8DfAJwCoAPAZbquH9U0bROArxluuhzAybquv6lpmhPAbwGcBcAP4Nu6rv8z7i9GoYgzmsZBMT/fbGUV+ZAZGXJCMDAgI5Ht7bL/kK7TYrp2bXQXtaoq5tqIi9DICBPs162LfN/ubq50AhxgzzhjYgOnddV5xgzZH3M8xF00Aow2CtEI0KKqRKMiibBOjLq6GJFpb+f3s6dH5jkmBYcPS0ULMGxksBpE06MxZLsNgchrvOsueWzz5riJRhFpjGfbjcZGRhj9fr7+ZctiZz/UdZkzGq3VU2zG9iiRmDWL2ymn8BrQ2cmPQns7t/7+6B7H4+Hkva1N9iYtKeE63/z5jLwuWsTPjlEkJs13YBxUVQGvvcbfo8lrTGXRGC7SaIwyLlkSvI6Crsv3qKoqeqFfWgocOcIFelF5XtDayu+2SMnxevlZncicJVVIiGjUNM0O4HcAPgCgAcAOTdMe1XXdWPLiMwC6dF0/TtO0ywHcCgrHewDcM/o4JwB4RNf1N0fv820ArbquL9Q0zQbAsq6qUKQHVivr8LAUkA4HJwiVlRzU3niDK29idfvOO9nTMJJ/X9MoMB96SB7bujU60fj88zwPl4vnMtHKd1Zr6kSijENDnORkZMQ5b2PjRuCmm+S+sqgqkgynk99FMeH2+2XUxZjXmDQTZmuUcfVqk7cyGnuqsKSGjS4FE4033zzu040Gl0uOxQDf81hFI7xe5gkKnV1WxmhTsMqY423xYBSGxhYPicDppMhbulQKO7+fE/KWFg6zx45xwVQIQ7udP0OJgbo6blu28DbV1eb8yHnzps7GO1GMeXvHjvHzEO41pHKvxlCRxpGR6ArgdHby85KZOT5Rl5PDsWR4mOJQRK6Hh/l4xcVSNAJcsFCicfKsBnBQ1/XDAKBp2n0APgzAKBo/DOC/Rn9/EMBvNU3TdN1kUvg4gP817H8awGIA0HXdD6A9LmevUCQZWVmyn5awsop8yJUr6bvfu5e3bW8H/vxnCsdIvZzWrAkUjZHsay0tMheyqIj9HRsbuZo33pxGq2g84YTx3R9IUJQRYAh49Wom2QgefBD48pfj/MQKRfSUlJijNEJQGNtuJE1RjDDWVCC0aIyq3YYRa1Rx+3YOonl54zvfKHC5KMrFZN7n41NNdHwSls/WVk6W+/o4cZ41ixY9UUHaGgmcCqFgt48/38/lku0hwqHrvPYYC+289150r1PXaYc9ehR46ikey8ig4F6yRPaQjFW12XiRmyuL0Hm98robCrGQMjycZLb0CIjvjEDT5Fd1zx75Py8u5mU5GCLKWFEx/gWb0lJ+t9rbKVYrKmQ7NOs409QUXY/rVCVRonEOgHrDfgMAa13Fsdvouu7VNK0HQAnMQvAyUFxC0zQRN7lZ07SzABwCcJ2u6y3WJ9c07RoA1wBAVTTZrwpFCmG0slZXy4vBnj0UgGLAvfNO4PLLQw+qAJvhilU0gJaMgwcDS0sbee45mX+ybBndZO3tHKQj5VgY0fXYRBoTJhoBRhuNovH++5VoVCQVxcXmBtRJ3asxjGjU9UBrZ1ERjxsjjU6nrJwckooKllp+7z3ue73s9xCp2d0ECNV2Q7Q/GG9/P49HFuYQr3fGDJkeEA+czvEXehGvfcNIAAAgAElEQVRiOV7CRNNYCGb2bBZdAxiNrK01F9o5dCi6yqgjIxSeYrEV4GLKokUyGrlkiXT7JAvV1XIxpa4uvGi02eT1fXg4cf1CJ4tRMAIUy2LxK5oCOD093ByOieX5lpRQNHZ0MCJdWSlFozWqn+4VVBMlGoMNG9Y057C30TTtVACDuq6LaaUDQAWAV3Rdv0HTtBsA/BzAlQEPout/AvAnAFi5cmWU6dUKRWoiBs1ly+jtv+8+2jWHh4G77wYuvpi2n2A4nRx4X3lFHtu2LbRorK2V8y5NA97/fl5o29u56llZGX3Li5YWcyQhM5M5KOMl4aLxa4aU61df5dXF2NBJoZhCrMVwjKKxp0eWqZ9yBgbMPjOA1odRenvNNsmcHI5Xg4NS+BqdDRGLn6xfLwcvgBbVOIhGY9sNYQX+61+jL/RixOfj2Cpy0vLzKZyjEWaiT99Eir2kSiN4m4221rlz5b/S46FwFC0/9u6NXmAPDDDd44035LGSEikiFy+mqIxDgDpqqqpkYZbaWlmpNhRCNA4NpY5oDJXP2Noq/5fhCuCIKOOcORP7LBcUUHAODPB9M+Y1Wq3bSjTGhgYAhrcZFQAaQ9ymQdM0B4ACAEYzyuUwW1M7AAwC+Nfo/gNgXqRCoRhl7lzg6quBe+7hap3PBzzwAC+oq1cHv8+aNWbRuHUrcGXAUgwnPaKHFkBLhrC/il5GdXXho5RG9uwx7y9ePP4cE49HVg9MyIW8uprNKEU1AoAW1euvT8CTKxSRsRbDEXk9djujK93d0ZWfjzs7dphDQgsWmMI60RTBKSqSIjjihHj9enO56M2bx3/OUeJyUeA1N1M0TkQwDg/L/nDZ2fyfzZgxPstnOld1DIXTyQjhkiXyWH8/1wuMQrI9yuSmjg5eH43XyIoKaWldvJgOm7j0Bw6C8btbXx85X9bl4nc+lYrhWEWjyGc0ivnFi4Pb7AcG+D+z2SZebsBm4+Jbaysfa84cLsLoumwLI95zJRpjww4ACzRNmwvgGCgAr7Dc5lEAnwSwFcAlADaLfMbRIjcbAZwpbqzruq5p2mNg5dTNAM6BOUdSoVCA9p3PfAb4+99lif0nn6SIXL8+cJXasLgPgBaf7u7AQjr795vLXBvThGpqOMFpauJFLZoLaCz6M4pk+fz8BOZrXHqpWTTef78SjYqkIVgF1ZISTn4Ac7HSKSVCPmM07TZyczmJcziiWHASnkbBrl1UptbQbAzIzqYGNgoTY4uHcBbPrCz+r9raeJ/SUrpIEiVK0pHcXPYyPvlkeayjQ+ZG7t/P36Ot2NrQwO2557hvt3PBVlhaFy/m+mI8IrbFxbLYlVhYCFe7IBUrqFqL4BQUcMHL2Fc6VAEcEYksK5tY32hBSYkUjRUVLHbT0kKh6nbL91WJxhgwmqN4HYCnwZYbf9F1fY+maT8EsFPX9UcB/BnA3ZqmHQQjjJcbHuJMAA2ikI6Br4/e59cA2gBcHe/XolCkIoWFwKc/Ddx7rxR6L73EC80FF5gvZqWlnOAcOMB9XWfa3gc/KG/j9wPPPiv3V682i8qcHA6qra20zCxcGPkcYyEaxQQyUqXYmHLJJcCNN8r9rVt5pTJ6WBSKKcIqGjs66AoQ3/lUEY3RFMGJ2G7DyIwZfCOEJVbX2dz2ox+N/pyjRFRQ/eQnOXmNtsWD200RMzjIMbW6mgtyqVLAJJUoKQFOO40bwI/DsWPm/MgDB6LLAfb5mPN28CDw+OM8lpnJ66AxP3L27Mn/LzWNC7PvjoZMamvTTzQGizTu3StfQ1ERRbqV4WEKO02b/OW4uJiP093NFOjKSikaPR75vnZ0UNAmTUXqGJOwAsO6rj8J4EnLse8Zfh8Go4nB7vs8gDVBjtfCEH1UKBShyc4GrrqK7kmRyrNrF4Xjxo3mVbi1a6VoBDifM4rGN9+Uq+aZmezLaMUabQyXYzQ4aG7QDITOuwz3GOKcEpLPKKiqYnh22zZ57MEHga98JYEnoVAExxo46+jgBFmIxmhteXFF12MSaTQWnYmK9evNeZSbN8dNNAKcxEZrmxeRr5ERabEUllxF/NE0RpQqKpirD1AsHDkiLa379nE/Grux2w28/TY3QX6+OT9y8eKJ/Y+NorGuLnTqCZCaojFYpPH1nTow7AayskIWwGlo4P9m5sxxjAkhyMjg83Z3cwGrshLYuZPjqFEgiqq+6VrWIMW60igUisngdLKC6mOPyeT5AwdYmGHTJrlSv2YN8Le/yftt3y57QI2MsNeV4LTTgjfTzc6WFo7aWuZ8hGLfPvOFt6pK5i1Ewu+XPbj8fpk/lFAuvdQsGu+/X4lGRVJgFY2dnUkoGg8dMp9Ibm6A1SBUTqPxblH1aDSyfj3w61/L/TjlNY5nou73U4gIW11xMcXEZKx1itjgcNCFs2ABHToAFwLee88sJKO1KPb28tpqLMA9c6a5f+TChcGvr0aM/Rpra8O30zAuYKRK2w1rpDFjqAe13/s70N4G2+JFOOnLlwAwh/ZGRuT/IVY52yUlFI0dHebIpcNhfi+bmpRoVCgUaYLNBlx0EVe8X3yRx44dk70ci4p4sSoslKv4g4NcIV2xgul7ogR2bm5gDqSR6mpaVJubOXCHso1NtNVGdzcv2IOD3C8rY0nshFf7u+QS4IYb5P62bZHrnysUCSCYPbWkRBZu6OqKXbP5CWONMq5eHfAlDmZP7e+XlREBRgL6+8dRFfLMM/nCRQnEd9/lYDV79vjOPwLRTtSHhngKfX28zdy5nJymwsR+upKVRZezsTdfT4/MixR5ksaIeDhaW7m98AL3hf1UWFoXLWJVcWN0a9YsLpi43fzsdHeHjlja7VyA8Hi4pUJurDXSOPTos0A7ffU1+55C7t/bgS99yXSbY8doExY5n7GgtJTrWx0d/D9kZ3Pu4XLxuysWdhqtZT7TCCUaFYppiKZxkT0vj0VxdJ0DoRCOs2ezKOjTT8v7bNvGC9fLL8tjZ50VfgU8O5sXtOZmroAuXhz8dlbReMIJ4c9/ZISDd3OzfJ6FCxOcy2ikspJ2OuPk98EHzUJSoZgCRGsKkYvl8XCiq2mcQPb1mQs5TAkRrKlA8EjjO+9IvVddLYvfRB1pLCgAVq40h3q2bAE+/vEoHyA6RNsN0foo2Hvd2soFMNFjcunSKXBMKGJCQQHXPYRNVNeZqiEikfv2UVRGE3nWdV47a2vl9djhYIXWxYv5HGvWmHsH1tWFt7m6XBwHhoZSQzRaI439+2XPlBocBX7+AnDttWNK2uejaARiu27rckmh2NvL93z/forS7m45FxLzknRkGhZgVigUglWrmM8oJlv9/cCdd9IeZY0gbtvG4jnDw9wvKWHkMRLV1ZygtrQEv0jqemC7jXCRxuZmzvGam2VfrpUrp1AwCi691Lx///1Tcx4KhQFNC4w29vdzwchup+hqbZ2acxsjCtEYLKfRWD1x+XI5No1LAK9fb96Pk0VVWAytY6DPx4nnu+9SMM6YwfFMCcb0QdNoO33f+4DPfx741a9YIOeuu4Cvfx348IcZuYq2xZTXS+H58MPAt77FzjFGcVRbG/7+YlElVfIaTaLR50V3s2wuW4OjVMn33jt2rLmZC8v5+bGfF4ixtL1dWlTz8sz9btO5gqoSjQrFNGfpUvZhFBcSt5vtObKzzQ6xw4fNFVPPOSc6G6jLxcilrgNHjwb+/ehRaS8FOAAHq3Q2NMRJoigOUVjIyVV1dZL0H7vkEvP+a69FvnorFAkgUjGclpbEn9MY/f3mYjRAUM+7NdLo88nJmd1O656YuI0renLOOeb9BOY1iubxTU0cwxYs4ILZePvTKlIPm43XrvPOY4emP/6Rrp8//AH48pdZeK6qKjpr8iOPmK+ZRst2MFKtGI5RNHq7euEGQ3oOeFEuWr7feivg90PXZT5wPLJDROtYY15jbq4SjQqFYhpRXQ1cfbWs7OfzAU89xVVvQVeXFH1z5pibJUfz+JrGiIZRIALBo4zGC6XfT+21YwfPISODtpyTTopcICChVFQA69aZjz344NSci0JhwBpptBbDmdJI444d0mMK0GduOWG/PzAnrF461LBokWy2nZU1zkWkdevMHvvDh4Ovbk0SMVEX419TE/D66xSO2dnsGTjR5uOK9EBc2z7yEeCb32SBusceA375S+Caa1ilfObMwPu53eZF3vZ2fq5CkcqicbhzEJmgQqtEPRzw8Q979wKPPILWVjoOsrMDx71YkJ/P/9PQEBeubTaKRo+H8yZAiUaFQjENmDUL+Oxn5UqartMG09nJAXFggJOcd99lCfLxFGfIymKRGpGfYSRcf8aeHpa1PnKEE8fZs5nDEeM6FbFDWVQVSUikSOOUisYorKk9Pebqyrm5ssUAQGuqmACPu7R+dnbgcxrLQ8cIMVHv7+e5798vx7RTToldsQ5FepGTwzSQj38c+OEPgX/8A/jnPwOrc3Z1AeXlcj9ctDGVRKPfz++MYLjHg0wwQbsalsnEj3+MuloOFPEqIGW0+/f08PubnU3xKKKNfX3hRXsqo0SjQqEYo6AA+PSnpe2ispIJ383NnLT5fBR5t902/ommsNq0tJgH1GCi0evlpGrXLlmd7KSTuAqb1E1zL77YvL99e1yiFgrFeAhWQbW0VIrGtrbEn9MYE8hnzMyU0YfsbNo6RT7jhPqxJSCvUUzUe3o4dtrtsq1Cwqs9K1Ka4uJA0djaam69kS6isa/PsGCk69CHhmEDnQk1OGq6bcfOwxh46Q1kZnIRPF6I8VRYVDVt+lhUlWhUKBQmsrOBq66i5Ss/n6v4wnahabRk7NpFO+vTT0fX2BjgZE6shIpoY3e3rHIGcLWuuJhaq6mJz1dTw4I9U17oJhoqKti40oiyqCqmmEj21I6OxJ8TAA4exv6mQFSVU0UlWIB2drt9gkVwBMFEY7QDW5S4XDLykZvL6GLSOiYUSY/VptraGn0xnIwM5s36fKwPkMwY2214+wag+b0AADt8mFM0BFx44djf61AF3HMPKiriW+eguJiP39srv8PTpRiOEo0KhSKAjAzgssuYZ3PSScylcDpplRFFJgYHgVtuAb73veh7UFVVcbBtbWW00ZjPKHoqHTrESWFBAcViTU2SFLqJFmVRVSQZweyphYWy4EpPj1mIJYyDB5mAJcjLC1o62Sga/X5zhOTEE/lzwvZUgJ53Y4J0YyP7X8QQm41tEmpqOK4mVT62IuUIJhqNlszm5vDf6VSJNhrzGd0dMp9xDo4hY90qlo8F0Is89KAAjl3bUd6wPdhDxQy7neOnrsvvsYg0irWmdG27kUpTMYVCkUBsNi7inX027V+bNrEvo5WXX2bU0di/MRSZmcxtBOjafOcdDrLDw7ShlJVxIrtoURIWuomWiy82J1Ps2MGkTIViiggWabTbZf6yzxcYzUsIVmvq6tVBvZrGcxsakhb14mJZPGZSkUankytjRp57bgIPFJ45c1JwEUyRlAQTjS6XPO73m4tFWUmVthvGSONw7zCyRkVjDY6yiNWaNcBZZzHKCIpJ+09/EvfzEmPqyAjXuvLy+J6LyK2KNCoUimmHplEo3ngjF/Ruuw34z/8MLGnf3Q1897uMPEZKABfRxrY2dqXo6+OFS9dp2Vq9muIxHknsCWHOHGVRVSQVwSKNgJxg+nxTZFGNIp8RMIvG/n4pDJcvl+PEpCKNQML6NSoUscAqGkXbHKNFNR3yGk2VU/t9yARXh6pRO1atfOAr30E7SmGDH3NwjA0sjZWy4oAQjV1dvOSLQlZi8aqxMa5PP2Uo0ahQKCKSk8PVfU0DPvpR4I47grfcePppFtLZtSv0Y2VmMrfR62V7Np+PwYXcXOD8883V71MWZVFVJBGFheZFmP5+WqlmzZKtKqakV2OUolEUwvH5OMk1ikaAY4nXy3FkwuOHVTRu2WJuBaJQJBFW0SiKWaVbMRwhGn3DHnhGgCy4YYMflbZG5q8AqF+wHli4CLPRDCdGQ3233hrX88rK4pzF5wOKimS7MpHXqOypCoVCMUpFBaOOn/lMoJustRW44Qbgt781J4YbqaqifcPn4+Cbl8fVOmNfyJTGalHduZP93xSKKUAUmDLS1WUuhpNw0djXB7z9tvnYmjVBbyoijQMDshdjZaV8TZOqnCpYsYKJ1MYn3b17Eg+oUMSPGTPMl5jOTi6cGCONDQ2yiJ2VVBONw+20MGViGOVohHPFMiAnB2430NKqQdt0BSph8OPee2/4akAxQEQbXS4ZaTSKxhjX0koKlGhUKBQTwm4HPvEJ4A9/YJ6OlX/+kw2J9+0L/JvTyYlsfr6sKhik/kXqUl4OnH66+ZiyqCqmkKTr1bhjhzmSt2hR4EmOIiKNol+byyUL4AAxsKYCfCOsSdvKoqpIUhwO89dF1xltzM+XlcZHRkLn1qWcaOzhiWbCLfMZwbxNXQdmfOwMuBYZwqxeL/Dzn8f13EROuKbx/czI4Hvu81E8WlsFpQNKNCoUikmxYAFw++2stmrNQ6yrA774ReCuuziGG3nvPXNBiOOPj/upJhZlUVUkEZHabrS1JXhlPEprKsBz9XjkKn5urnmRaVJFcIyovEZFCmF15gSzqIYKtomF25GRwGtzMiEK4Qz38SSz4GY+49q1JlFcVWMDvv51853vuCOuq2G5uXwffT6OpVaLajoWw1GiUaFQTBqnE7j2WuDXvw7sPeb3A3/9K8WjuIDpOiunGkk70Wi1qL7+urKoKqYMq2js6OCkR0TnBge5JYwoRaPXy4mjKLClaRwrjAIxJpFGIFA0vvBC8jeyU0xbrA3sx1MMR0THALnokoz09AA+rx8jbroSsjCMKtQB69ahsVG26srNBUu8V1TIOw8PA7/5TdzOTdNktFFUUBVPCyjRqFAoFGFZvhz485+BDRsC//bee8DnPkeXZlOTuSJiZiYwb17izjMhlJUFlvF/4IGpORfFtCeYPVXTpJj0+cwtE+OKrgPbtpmPhRCNPT1ceBLW1MxM9jk0ErNI47Jl5vBNfz8XexSKJCRY2w0gsBhOKAdBKlhUe3sBd+cg9NEXUYl6ZJaXwjenCg0NvM2YSHY6ga9+1fwAv/2tuQRrjAmX16hEo0KhUEQgO5vj9o9/zKpiRkZGgN/9Drj+evPxJUtko/G0QllUFUlCsEgjMEVtNw4cMD9ZXh6wdGnQm3Z2chImLHS5ubTEG4lZpFHTlEVVkTJY7alCNJaUyB7HQ0PStmolFXo19vQAw93SArEQ+4F169Dcoo31SBQ5nACAz37WPNj19rLwQpwoKqLNNydHinCPh0JdiUaFQqGIkrVrgTvvBN73vsC/WS9iaVUEx4jVovrGG8ChQ1N3PoppizXSKCL9UyIardbUU08NLMM8SmenjDICrJpqXGDSdbmyP2nRCASKxueei8GDKhSxx2pPFaJR06KzqCZ7pFHXqfmGe2XS5RLsBdatG5tDVFZa7pSTA3z5y+Zjv/pV3F6kqEydnS3Fq99P4ZiObTfCikZN0+7WNO1vkbZEnaxCoUgtCgqA738f+M53pHUjGCeckLhzSiizZwNnnmk+piyqiikgWCEcIElEY5giOO3tMp8RAI47zvx3j4eTNKczpO4cH1bRuG1bclcKUUxbQtlTgeiK4SS7aOzv57jkGeL3z4kRzMdh+E5dh54eiuOgBZevu8484Wht5Qp2nBBja1mZPOZ2T89I40EAh0a3HgAfAWAH0DB63w8D6I7nCSoUitRG04BzzgH+8hdg5crgtwnhTEsPlEVVkQSEsqeKaEWyisY9e2RnjoyMwPY+IgoZkygjAMyfb56NDw4G9pNUKJKAcKIxHSKNPT3AcK8b+mizyQJ0IysT6J67ArpOa2rQtJaiIuALXzAf+9nP4rb4I8bWWbNk/ujwMAsTpdt6U1jRqOv6D8QGYCGADbqub9J1/Vu6rn8CwAYAixJxogqFIrWZMQP46U+Zz5iZKY+vWyerjqUlH/uYubfIrl3AwYNTdz6KaYk1v7iri2JMrI4L0WhsnRgX+voCSyevWRPy5nv3yt9zcgLFr1jNtx6fMJoWKGKtIlehSAKKisyiqb9fVkAuK2P0HaD4ClYLJiuLH3e3OwHf+wnQ0wMMd8p8xploBVatQtcAX5h1TDPxla+YJxpHjwL33ReX83Q62R9z9mz5nrvdsndmOjGenMY1ACzlzvAagNBLhAqFQmFA04APf5gVVjdtAq68Evja16b6rOLM7NmBiZ3KoqpIME6neXFG1ykcXS6KMYCFqrrj7R3avt08Q128OOTsb3DQHCXJzTXb0YaHZRVYozVs0ijRqEgBNC10r0abzdx9IphFVdNkhD4Z22709gLuXs/YfhmagXXrxqz1Qa2pYzcuAz71KfOxW26JmzouKeEwJgoQeb3c0s2iOh7RuAvAjzVNcwHA6M8fAXgzHiemUCjSlzlzWOTs05+2VD5LVzZuNO8ri6piCghVDMfYdiPuFtVxWlNFPmNmJu2pxtfQ1ETxO2OGXOGPCUo0KlKEdLaotrcD7kHp76xAPdyrTsfgIPOX8/MjPMDXvmZ2+ezZAzz+eFzOtbSU51RYKC2q6ZjXOB7R+CkApwHo0TStBcxxPB3AVXE4L4VCoUgfrBbVN99k2wGFIoGEKoaTrKLxrbdkBETUtRBBSb9fTsjmzInxOa5cafb9HTpkno0rFEmCtYJqS4v8PdWL4RzZ74buGQEAOOFBKTrQuYjjRVGRuTB5UObPBy67zHzsJz8J3bhyEuTkMGo7e7YSjQAAXdeP6rq+DsBxAC4CcJyu6+t0XT8ar5NTKBSKtGDWLOCss8zHlEVVkWBCFcMpLeXPuItGXWc1UiMhRGNnJ9DQwMmspkkLrYg0trezcmpODqs0x5TsbODEE83HrOetUCQBoeypAO2pYq2yrU3mOxpJ5l6NtTtaAVCBZcGNgplZ6LJzsAqbz2jkG98w72/bBrzwQuxO0kBJCVuAjNbtmd6iUaDreh2A7QAaNE2zaZqmej0qFApFJJRFVTHFhBKNYuIZd9H43nsyvAnQXxaidPLu3Twft5vRELud4lEIxGPH+DPmUUaBsqgqUgCrPdUYaczIAMrL5X59feD9kznS2LBXNmjNwhDyllSgq4v7YfMZjSxfDmzYYD72k5/E5gQtlJayurNIm3S75TiVLkQt+DRNK9c07V+apnUA8AIYMWwKhUKhCIfVovrWW8D+/VN3Popph3WiJQRiwno1WoXXqaeavxOj6DpFo9WaWljIm/f3s7Ki3R5oz4sZSjQqUgDr59/qojbmNQazqCaraBwZAVqOyXzGTLjhPH4BRkYYHRXnHRXf/KZ5/5lngNdfj82JGigoYKTRZuMYpuvAkSMxf5opZTxRwtsBeACcA6AfwMkAHgVwbRzOS6FQKNKLmTOBs882H1MWVUUCCVUIZ8YMRvF8PooxjyfwvjEhynzGhgae29AQJ2BigijOv7GRP2fPpnCMC9Zz27Ej/ZquKVIeqz01nGgMVgxHfLeGh+OS6jdhjjXoGOpxAwAyMAIHfMCSJQDGYU0VnHYacMYZ5mNxiDbabMC8eSzaJSyqbW3JWZl2ooxHNK4D8Gld198EoOu6/haAzwC4MS5nplAoFOmG1aL6j39wSVWhSAChCuG4XGzHISY6RgdpTIlSNL71Fn8ODTFnUQQji4qo24QFz2i9izk1NeYwzuAgw58KRRIRLNJoFH9G0djYGLggZLNR5Oh6comboy83wO1jMaosDAN2O/wVrOwTtTXViDXa+NBDwL59kzzLQGbO5DhrtKg2N8f8aaaM8YhGH2hLBYBuTdNmABgAEK+MAoVCoUgvrBbVd95hIsTGjcBdd5kTUhSKGBMqp1E0pxaiMeYW1b4+4N//5ufdyKmnBtzU52NlfICTWGFNBThZbGnhbQoLZXGcuKBpyqKqSHpycmRvQIBrkD09cj87W0Yj/f7gOXbGaGOyUPvCUQwjEwBFo56XD79GW8GE2nSddx5w0klyX9eBn/40BmdqpriYLSL9fj5FuuU1jkc0vgbg/NHfnwbwDwAPAdgZ65NSKBSKtGTGDGD9evOx3l7gwQeBq6+m327VKuC//ot2uDg1IlZMT4KJRl2Pg2isrwfuuw/40peAFSs4y/vQh8whkCVLgvrMDhyQ+VWaxiiIoLg4AQVwjCjRqEgBwvVqBMytN8JZVJMlr9HrBere7IDbIBq9eUXIzuY4lZExgQfVtMBo4913B68ONAkyMmhR1TRevr1e4PDhmD7FlDIe0XglAFGn9noAWwC8A+CKWJ+UQqFQpC033BD+7zt3Aj/4AbB6Nf13V19NUWlcPlYoJoDLZRZhIyMsKpORwSIOYnV8XKLR52Pf0d/9DrjiCvrhqqqAj38c+O1v+bdgix+nnRb04YQ1FaDWNPZiy8ykS9TplG1C4sqaNeZ91XZDkYREEo2pVgynsREYOtoKHdpYPqO9MBcOxwTyGY1cfDGwYIHc93qBX/xi0udrZd48jqli2Nu7N+ZPMWWMp09jt67rnaO/D+m6frOu61/XdT3NupAoFApFHPnQh4AXXwQ+9anAq72VlhbaVjdu5Cx5/Xpe5PbuTa6qBYqUQNOCF8PRNBmFjFhBdWAA2LwZ+OEPgXPP5SxuxQrguuuA//3f6FbuCwuDLp4MDbErh8BouwNkHZry8igae8eClSsBh0PuHzoUOCNXKKaY8YjGhobANZxk69V49O0+DHf0AWB/RgBwluQDmKRotNuBm24yH/uf/2HT1xgyfz4Xtnw+XqanZaRR07QMTdN+oGnaEU3ThjVNOzy674znCSoUCkXaccYZwJ13svPv9u3A97/PCWo4vF5gyxbgq19lb7v582n/+/e/kysZRZHURNOrsb3dsCbR2Mgqv9dfz89oQQFwzjn8zD7zDPMVo2HxYuAzn+Hn/tChsUqIRvbskRbZ8nLmAwlEFFTTmDOUELKzgfsl8z4AACAASURBVBNPNB9T0UZFkhFJNBYWyv6mHk9gYZZkizTWPn/EYE0dgj87F1m5GbDbaU+dFFdeaa6gNTgI/L//N8kHNTN3rnR06HrMHbBTynjsqT8F8H4AnwdwIthqYz2AW+NwXgqFQpH+2GzmHMamJk6qL7kk8tXxyBHa/z70ISqBiy4Cbr89va5QipgTqoJqYSGDaj4fMDykY/B7t9BnNWcOcOmlwG9+w95mQtWFw+mk/fSmm4BHH2Xd+b17gTvuYIQ9RPlDY3HSE080V3F1u1kUp7TUbLGNOyqvUZHkWEVjsHpq4SyqySQafT6g/vVWuMHwZybcGMllPqPo0zopMjOBGy1NH267LfrFrygoL5cuCZ+Pw59xASyVGc/bvxHARbquP6Pr+n5d158B8FEAl8bn1BQKhWKaMXs2J9UPPMArzebNvMAtXhz+foODwGOPAddey9nB8uVM+n/5ZdVbTmEiVKQxM9NQDGf7dnT89++j70wtFi1uvZWfuZ4e/rz1VuDCC6NKQOzqkkU6bDbg+ON5DOBqvcfDtiBxbbMRDCUaFUmOVTS2tQXeJlwxHIdD5uBNtbhpagI8RxvhRiYc8CIDXnhzCpCbO8FWG8G45hrzg3V3c8E1RhQUcCx1OPieejzA/v0xe/gpZTyiMVQGQSIyCxQKhWJ64XQCZ58N/PznjNIcOkQbzbnnRg61vP02cMsttMHOnMkCJffcE4deCopUwzrxMrbdKCgAfAPDwPPPowMlgXcWLFjAxY077uBns60NeOQRRhZPO00mSY0DY5Rx/nxOuAYGuC9amc6YMcmcpolgFY07dqiFGEVSMd5IY11dYEp8skQbjx7yAQ0NGEYmW20A8LrykJMTw+9+bi5TO4z84hcxS/PQNKCykmOqrlM4vv12TB56yhmPaHwAwGOapp2radoSTdPOA/Dw6HGFQqFQxJN582QOY0cHbX+f/zxQURH+fl1dLFDyiU9wdnHaacCPf8wylaqYzrQjWCEcwNB2Y9sOYHhIika7nVVEb7yRDbGbm1mt5s47maO4ePGkq9LoeqA1VUQZAUY/8vIif9TjQk2NuYP64KD5ZBWKKUbkIws6OgLXNWbMkMJwYCBw/TBZejXWvtIAjHjgRhayMAyfIxP+jEwUFgYWxpoUX/qSudFrczPw17/G7OHLy+V76vcD+/bF7KGnlPGIxpsAPAvgdwBeB3Ab2Hbja3E4L4VCoVCEIieHtr8//pHLxm+9RSF42mnhkz78fuDVV4Fvf5uNjisradV55BH2XlCkPaHsqU4nkD/UDN/uPQCAdoxaSm++mZbMn/8c+OhHzQIqRhw7ZrbJLlokRaPPxwlwfn5cnjoymqYsqoqkxuk0N7wP1jZH0wKjjUaSIdLo9wN1r7Ehg4g0enOLAWix/+6XlPDaZ+SnP42Zi6CsTGpSv59GoXRYow0rGjVNWy82AKcDeB7ANQAuBAvibBk9rlAoFIqpQNPMOYytrbSiXnFF5CSQY8dYcvwjH+FF9NxzaYE9dCgx565IOOFEY8HDf4NvdGLTgRLOMq+/Pu7nZAzcLV3K/CoRARU5VmVl5u4XCUWJRkWSYxVVwTrDhCuGkwxtN5qbAfeRxrF9B7zw5rAgXFwqJt94IwcbweHDrCcQA8rKKMRtNorGtjZzYa9UJdIQ/OcQx4Ve1kZ/nxezM1IoFArFxCkpoWC84gqGaV57DXjiCW7GzulWPB62UHjmGeDLX2a4Z8MGbqefTlWhSHlCVU91vvYS8nc8Cx828jiK4f/xLbCJEESc8PmAd96R+8uX82dXlyyAAzAoPmUo0ahIcmbONBdbiSQakzHSWFuLserfGRiBB054s/MAsIhzzJkzB/jkJ5mbLbjlFuDyyydtuS8ro2DMyqKjvbub/xPr+JtqhI006ro+N8Q2b3Sbq+u6EowKhUKRjNjtwLp1wI9+BLz5Ji/It9/OSpeREkT27wd++Uv25CstBS6+GPjLXwKbfClSioICfiwEAwOAe8gP5zdvhBMeOEGV5ptTjZ4PXR738zl4kJMqgBbUmhr+3tlJwajrjDDOnh33UwnNypXmMOfhw8Fn5QrFFBGpVyPAPDsRWOvqAnp75d+SQTQefasH6KYv3Q4fvMiA7sqGwxFHsXXTTeaUjt27gSefnPTDishoURENPzNmBG1Nm3JMtuOJQqFQKFKFigqZw9jRwaI6X/oSi+yEo6+PRVA+8xleDVeuZHP37dvpvVGkDJoWWIWw83/+CceuHbDBjzz0wQ8NOPdcdHTGvzi60Zq6fLlc4O/slNbUzMwYltufCC4Xc4CNqGijIomIRjTa7eaInTHa6HTy716vrFacSHQdqHu1YWzfDxu8OQWAZofDwcWuuLBgAfsiG/nJTyb9sGKRKyeH28BAelwqlWhUKBSK6UhWlsxhPHiQ5d1+8Qtg/frIyWOvvw788IfAqadSRH7qU8D999ODo0h6TALM50PHj/4IAHDCg3z0wrf4eKCyMu4dWoaHzZY6YU0FgMZGWldtNkZHEt5qw4qyqCqSmGhEIxC+X+NURhtbWoChQ8xnzEE/huDCSA6VosNBF0Lc+MY3zPuvvMJtEmRn85ztdnb4sNmA9vZJPWRSoESjQqFQTHc0jTmMN9wAPPcco5APPghcfXXkkpWtrSxVftll9OCcdRbws58B776bHuXi0hCT1au+Hh2trBjohAf5tkH43rceQPwnOe++K4sVlpWZJ77HjvGn08mP55RGGgElGhVJTbSiMVwxnKlsu2HMZyxDE7xwwOfKg6YxUhepNfGkWLECOO8887GHH570w1qL96RDZocSjQqFQqEwk58vcxgbG9nQ/L/+C1i1Kvz9vF7ghReYJ7JsGW2v110HPPXU1HeNVowxJhrdbqCuFp2gInPCg4Lz1sCXx7BevCONVmuqwONh5EHTZP2lKY80rllj3t+xI2bl+RWKyWIVjS0twW9XUSEt4K2tZoE4lZHGowdGeK0BUIxOjCADyMmBw2FuJxI3PvlJ834MerFaRWNjY/DbpRJKNCoUCoUiNDabOYexuRm46y5g48bInqGjR4Hf/Q44/3wqlQsvBP7wh0BflCKhjEXtjhwG/H621wDgLMpBwaYL4fPxz/EUjd3d/HgAnMSecIL8W1MT02gdDlm0Z8ojjTU15qj70FBMJpYKRSwoLjYXuOrrCx4xzMyUYkbXzUPxVIlGXQdqtzUCPi7CFKAH3oxsIMMZ33xGIyeeaN6Pg2hsapr0Q045SjQqFAqFInpmzeKq7P3307+4ZQvw1a9GLg03NAQ8/jjwH//BxJoTTmAuyUsvqYhNgikpAdDfN+aXGos03vAl5M7KGXMV9/TEryjG22/L3+fPZ94PwAnkkSMMggpLmsMh/z5laJqyqCqSFpuNRa6NRGNRNYrGqerV2NYGDB5gGC4bg7DBB28ulWJGRpzzGQULFpg9sM3Nk66QrOypCoVCoVAIMjLMOYyHDwO33cb8kEhJKO+8A9x6K3DmmfRWffzjwN//nh7VApKc4iKdxY9G6UAJsGQJnFddDrvd3I0lHg2pdd3cMtRoTe3o4Ga3y3pMxcWTbpsWG5RoVCQx1vTz8RbDmapIozGfsRq1aMNM+F15sNk4DiQk0uhwMKXCyCSjjSrSqFAoFApFKObOlTmMnZ3AY48B114buTN7Vxdw333AlVdSQK5bB/z3fwO7dqliOnGgZPcWU6XbDpQAv/gFnNlUaXl58rbxsKg2Ncm1AacTWLxY/q2xkf3jRAEcIAnyGQVKNCqSmBkzzPttbcFvZ4w0HjsmjR6ZmYxYejyJNX8cPSJ9stWoRSPKgdzssUWjhIhGwLx6BUxaNM6ezfe0poaXtFNOmdTDJQUR6qorFAqFQjEBsrOBCy7gpuuMLD7xBLdXXw3dtErXORnfuhX47nfZkfr884ENG4D3vz8JfIopjseDkl98C4DsRdYxcwlw3jo4+7ifmyvzoeIR+DXOxZYskcVuhoa41tDfL48BSZDPKFi5khEJMaM+fJjhHGsVEoViCrBGGkMVw8nJoUW9o4NtbY4dY/RR07hg1NNDK2VFRfzPWdeB2je7gIF+AEANjuJpxwbA5UJGBm+TEHsqEHPROGcO10+TwiURI1SkUaFQKBTxRVQ6ETmMbW3AvfcCmzZZ+j8EobERuOMO4KMf5W0/+EHgN78x2SsV4+CPf0TxkZ2mQz3VJ8Ln18aEmtGeGutIo99vzmc01p8Q1QVtNm6CpIk0ulzASSeZj6looyJJiLbtBmC2qBpbbwhTSH19YprRd3QA/fvZXycLwyhFG9pmLgNgS/lIo6all2AElGhUKBQKRaIpLpY5jC0tbKT87W8HTsiteDzA//0fcP31LFywaBHwla8Azz7LvynC09UF/OAHcMCHfPTyWFk59JxcdHcjqGhsaAD27WPEUVRVnQyHDgEDA/w9L4/WLYCPLXJ+rBOtpIk0AsqiqkharPbUcKIxVDGckhJ+/93uSdeBiQprPmMf8tFfXA27XS4cTVmkcc8eVaTNgrKnKhQKhWLqsNuZ8CHyGI8dA558kjbWZ5+VCiMY773H7de/pqfyAx+gjfX88wOrECj4/o5WtilBB3rtRcxDBVf8S0rovszLo4iz23n8vvt4d5uNAq601LyVlMgiGpEwFsA54QQ5MWxr4/wsPz+wVUDSRBoBisbbbpP7SjQqkoRoC+EA5kijiCrabFywqariQlFdHR8zntGyo0dhEo2dKMZAwWxkGLRawiKNM2YwEVGUOfV4eH1ZujRBJ5D8KNGoUCgUiuRhzhzgc5/j5nYDL7wgcyEPHQp9v/5+4F//4gYAJ59MAblhA7BqldnvOB05eNAkdorRiSNV1WPhRWFDdToZaSgtZWDSiN/PiGOwPMfcXLOIFL8XFMi33u3mZFRgtKYeo0MN5eWBFVuTOtK4Ywf7kogELIViighmT9X14KKvsJCLQ319/F62tMh1tlmzKOYGBzkuWFt5xApdB2rfc48lX9bgKBpRjoGsGXAYFo4SJhoBRhuNvTF271ai0YASjQqFQqFITjIzmcP4wQ8ymvjeezIK+eKL4ZsIvvEGt5tv5gryeedRQJ57LmdM042vf930fpUU+IBKWenCKBoHB4FLL6WQa2uTQrGnJ/TD9/dzO3rUfNzhkCISkG6vWbNkZKS3l5PXjAxOfK1iNalEY3W1ORoxNMSJZTqURlSkNLm57LUoIvVuN79XweydIqK4Zw/36+qkaNQ05jYeOED7aLxEY1cX0LuvEdD9yIQbxejAexXnYGDEOZbPCEyBaHzmGbm/ezdw+eUJPIHkRolGhUKhUCQ/msYcRpHH2NtL++oTT1BIhuuc3NYG3H03N7sdOO00GYVcujT9qhVYefFF4KGHTIeKLzkbOGQf2xfRPZHXaLOxWKgRj4fiUohIsXV0hE798XoZSLBWcgxWAGf2bD5vUkcaNY3RRhHRBmhRVaJRMcVoGhddjDmKLS2hcwKrq6VorK0FTj1V/m32bC4A9fVR3MXDIm7MZ6xEPXpQCH3ZMri7zXb3hOU0AjEvhpNuKNGoUCgUitQjPx/42Me4+f3s6ShsrDt2hO7v6PNRRL34IqNv1dVSQJ59dvTJeamC3w/ccIP52OrVKDl3FfB7ecgqGoPVFXI6GY2wpov6/YxCWsVke3vwlFSHg/mMAIOfIveqvJz/tqSONALBReN1103d+SgUo1hFY1sba4YFw1oMx2hltdvZcuPIEf4tHqLRmM9Yg6PoQhE8i5ZD2y5v43TScJIwlGgMS8JEo6Zp5wH4DQA7gDt0Xb/F8vdMAH8DcAqADgCX6bp+VNO0TQC+ZrjpcgAn67r+puG+jwKYp+v68XF+GQqFQqFINmw2RnpOOQX43veoQv79bwrIp58O76usrQV+/3tuWVnA+vVSRBqrRaQq99wDvP66+dj/b+++4+Qqy/6Pf67dTTaV9LZpEAiBBAghERIg9CCgFAWEUEUUBBERlR+oiCL4CIooSpHyACoCCshDL6FJpEjoCQQS0knb9F42e//+uM9k5kzZOjNnyvf9es0re86cc+Zszp7duea67uv+3e/osSWcXU0sT4XmNaOtqPBvKrt1S32DunFjOCO5bh2MGOHHU4FPENfX+8CwfXtfGrt5c3z/tm0LMI5XB1UpUM2ZdqN373g567p1/sOaxA9o+vf3AePKlT7jGLtns2XunHrfnhnfBGc+A1m/0x60eTe+zQ475LkQZLfdwnOxzp+fu1RrEcpLZwAzqwRuBo4GhgMTzSx5ZOm5wErn3C7AjcB1AM65+5xzezvn9gbOBOYkBYxfBdbl4dsQEZFi0Ls3nHUWPPig/6j95ZfhRz9qvKHBpk2+1PU73/FzQeyxh89G/vvfxdl6fcMGuOKK8LqTT4YDDkjJ3rUmaGxI+/Z+fNSoUXDEEXDCCfHA0rl4A5z+/f2/6UpTC656ePRoQoOuZs/OPJO6SB4ld1Bt6MeyoiI+LyOE52sE/yNeU+O/TsxeZsOqVbBq5nLYtJE2bKULq9jSsTtb+gygMl41n9/xjODTmrvvHl6XOLFsmctXO7l9gZnOuVnOuS3AA8DxSdscD9wbfP0QcLhZyp+KicD9sQUz6wRcClyTk7MWEZHi1qYNHHwwXH+9H8Azezb86U9w9NH+Y/aGTJvm9zv4YN9M59RT/bjI2tr8nHtr3XBDPCoDHxH+2hf59OgR3jRXQWNDVq70cXq7dvEMR3LQWJAf8Ldvnzqn6BtvRHMuIgmS52ps7FdVpvkaYwYM8MFlba3/DCpb5s5le5YxNp6R4cOprAqHJXkPGiG1RDVxnqAyl6+gsT8wP2F5QbAu7TbOuTpgNZD0Z41TSAgagV8CNwBZ/FEWEZGSteOOPpP41FM+UnriCbjggvC7p3RWrfKZy7PO8h/njx3rO7O+807m8ZNRWrhwe4C43fe+B0OGAKnjBFes8N9GPoPGxGk2Yh8RF/x4xhiVqEoBSi5PbSwBnliBn5xpBJ9469vXf53NbGNiE5zYeEb22COlqiCvTXBiNK4xo3wFjemKS5L/yja4jZntB2xwzk0NlvcGdnHO/SvNfuEDm51nZlPMbEptsXxCLCIiudWhgx+7eMstvivDhx/6QGv8eEI1Usmcgzff9OMnR4/2tZXf/KZvjrJ2bd5Ov0FXXhlODfTsCT/+8fbFDh3Cida6Oj+uKV9B46ZNPmavqAg31imKTCMoaJSClFye2tCYRvAf2MQqrVes8L8Dkg0c6D/UWbIkPN64NRKb4AxkHqvoCiNGsG1beLuCyDQqaNwuX0HjAiChcpoBwMJM25hZFdAFSPzzcSrhLOM4YLSZzQEmA7ua2cvpXtw5d7tzboxzbkyv5Ny9iIiIWXgMY20t3H8/nHFGai1nskWL4K67fCfXHj1gwoT4vJJReO89uPvu8Lpf/CJlfsp0JaqxOeq3bs1tAjU2zUavXvHXhCLONL71VsPzhorkQfJb3GXLfKOpTKqq4uOJIX02sX17f1zntsd5rbJ2Laz4fCMsq6WKOjqxjnqrotOY3VJKYAsiaJw6lZRotkzlK2h8CxhqZjuZWVt8APhY0jaPAWcHX58EvOic/5NlZhXAyfixkAA45251ztU453YEDgQ+dc4dktPvQkREykO3bvExjEuW+EzST3/qO7o0ZOtWP3/k97/v55QcOhQuuQSefz57H9M3xDn4wQ/CEd/uu8N556Vsmi5oNMt9trG+3sfZEH7DCgU+R2OiwYPjdXvg28QqIyERq64OB1r19fHxypkkVuanK1FN3GbRotZ/NjJnDtvHMw5gAWvZAYYModuAjimNriMJGvv1C/9y3LABZs2K4EQKT16CxmCM4kXAs8DHwD+cc9PM7GozOy7Y7C6gh5nNxDe3uTzhEAcBC5xzumoiIpJflZXhMYyffw533OFbgXbs2PC+M2fCH/4ARx7pS0S/8hW48854qi3bnngCXnwxvO63vw13+wzkq4Nqstpa/8azU6fUMUtFk2k0U4mqFKTmTLsBjTfDAX+vdu/uE26JvbVaInk84wq6w4gRdO+eOjtSJGMazVSimkG+Mo04555yzu3qnNvZOXdtsO5nzrnHgq83OedOds7t4pzbNzFAdM697Jwb28Cx52iORhERyYuamvgYxuXL4bnnfJOZXXZpeL916+DRR+Fb3/Iptn328WMPX389O+VPW7fCD38YXjdhgu8Um0a6ZjgQDxpzVW2ZPM1GunOIKdgxjaCgUQpSc4PG2JhF8POmZiqIiDXNWbCgdb+uEscz9mMh6+hExZ4j6NIF1qwJbxtJphEUNGaQt6BRRESk5FRXx8cwzpjhxzHeeKOflDBxsF46774L11wD++/vSx3PPBMeeCA13dZUf/5zeBxlRYWfdiPDRIdRTLuxdq1/Y1hVlfrmFoqoPBUUNEpBam7Q2K5dvIFOQ+MWu3Txj7q6lhdKrFsHy5bWw4IFVLKNjqwHoOv+w6moKJBMIyhozEBBo4iISLYkjmFcvhweeQTOPTfcIjSdZcvgb3+DiRN914mDDoLrrvNNGJrSkWblSvj5z8Przj0X9twz4y7JQWNypjEXQWPszWbfvqkNap0rskzj6NHhst/Zsxuf40Akx5obNEJ46o2GptaIlbIuWNBwg51M5s7F3yN1W+nP56yjM3TrTrc9fNmBMo2FTUGjiIhILnTuHB/D+Pnn8PbbcPXVfnxkhuwf4Gu/Xn0VLr/cB3077ggXXujHK2aaYfvaa8MdLzp18q/VgHxnGrdsicdUNTWpz69b57MYMe3a+c6NBat9+9TGSMo2SsRaEjQ2pRkO+N8ZHTv6EtaWfD6Sdn7GESPo1t3/PiyYTOPw4b5SI2bWrMKZTilCChpFRERyzSw8hnHJEvjLX+CUU1Kmwkgxbx7ceisce6x/13bMMXDzzcHgIOCzz+Cmm8L7XHFFuLtnGvlqhLNmDUyfDm+84bMT3br5eSKTJVflFnSWMUYlqlJgkoPGpgR3iUHj55+HP7zJtO28ec2flidxPGMvlrKZatqO3J1OnXwgmvg7p6oqwg+NOnTwVSOJpk6N5lwKiIJGERGRfOvVKz6GsbYWXnkFLrsMRoxoeL9Nm+Dpp+Gii2Cnnfz2J54Y7lozcKCf8qMRjTXCaU3QuG2bL0WdMsU3nF28OB4w7rpr+n2KajxjjIJGKTDJQWNtbeP7dO4cv98aG7PYu7evAti40VfVN9WGDUHWc/58KqjfPp6x2/67A6lZxi5dGi7IyDmVqKZQ0CgiIhKlqqrwGMY5c3wm8Utf8u/OGvLRR/D+++F1v/51kz6i79IlPK5wwwYfk7YmaFy3zvfiee01/++6db4f0KBBsN9+MHJk5lMrmuk2EiUHjVOm5K7trEgT9OgRDrZWrWraFLGJ2caXXsocEJrFt22olDXZ3Ln4Es/Vq6hhoR/PWNWG7gdkDhojNXJkeFlBo4JGERGRgjJ4cHwM44oV8OSTfjmxW0Um++4Lp57apJcxS59tbG7QWF/vM4nvvONjpoULfaaxSxfYfXcfVw0Z0ngcW5SZxkGDwmXAGzfqzaVEqrLSFzIkakq2caed4l/Png233AKPP57anAb8j3zbtv5DoeT7NpPE8YwDmcdqusDQoXTr5z8YK7igUZnGFAoaRUREClX79vExjLNn+0zkddf5zGRy+9GKCvjd78INHBqRrhlOVZU/RF1dw/OxbdgAM2f6rOL06fGpNPr3hy98wfeI6dOn6adTlJlGM5WoSsFJDhqb0gxnxIhwcq2+3vfu+uMfYdIkX4UQU1EBAwb4rxvqtpoocTxjT5axjUo6jtp1+4dUBdMEJyZd0NjcQZwlRkGjiIhIMTDz7+wuu8yPgayt9WMizz4bvvpVeOwxOOCAZh2ysWY4yZWW9fX+Zd9/H/77X996v67Oj4kaNszHT0OH+g6LzVVU020kUtAoBSY272JMU4LGqirf7Pm883xlQMzWrTB5MvzhD/4DoliTnJoav8+qVemzkYk2bgwa8syfj+HogO8C3X3/3bZvUzDTbcQMGhSOXNesaV49bgmqanwTERERKTjduvnuq6ec0uJDZGqG06aNzyxs2eKHVW7aBIsW+UesbLWy0jfFqKnxQWNrFWV5KiholILTkmk3Ympq4KyzfFPmSZP8PQ8+8HvuOXjzTTj0UJ+Iq6nxmcZ582CPPTIfc948cFvrYNEi+rGIDfhPlbodEk9tFlym0cx/k5Mnx9d98IGfAqlMKWgUEREpU43N1bh0qf9wPXEKyI4d/ZvFPn3Cc9u3VtFmGkeP9v8RsRTM7Nk+rZKc7hHJk9YEjTE77+wzjtOmwQsvxMvHV6+GRx/1WceDDvKx1bJlsH595gqDuXPxg53rt9GfBaylMxV9+9Blt37btym4TCOkDxqPOy6684mYylNFRETKVGNB44IFfl1FhY+BRo3y4xX7989uwAhFOqYR/LjTUaPC65RtlAhlI2gEHxDusYef4eeYY8JB4dKl8NBD/kd96dKGxzbOmYP/ZQJ0w9/oXUYNCQ3LLrhMI6gZThIFjSIiImUqOWiMZftin/K3b+8zDuPG+U6oufr037nUoLFoMo2gElUpKNkKGmMqK31j5osvhkMOiX+oBL6D6lNP+eHVQVwYsnlzUOIajGfcPj/juN1C2xVspjGRgkYREREpR5ka4fTtC/vv798oDhzoxzjm0tq14U6tHTpAdXVuXzOrFDRKAUkXNGaj8Wd1tQ8aL77Y/26oqPAVBx07+hLU66/3/bgSA8D588HVO5g/nz4s2T6esfthe4eOXXBTbkDqQM0ZM3zb6DKloFFERKRMZco0gs8mJE4SnktF2wQnJjlonDIltfWsSJ7ssEM4G7hxo88IZkunTr5c9aKLYM894wHe2rXw1ltw002+ic7GjUFp6qpVsH4dfVjMZqpp066KjvuOCB2zIIPGzp3DrWTr6+Gjj6I7n4gpaBQRESlTySWgCd8pRAAAIABJREFUq1bF+7nkU9EHjYMGQb94Uw82bvTzkohEwCw121hbm/3X6d4dTjwRvvtd2HVXn81cs8b/Dpk82QeP77/P9vkZu7LK7zdyINYmPCi6IINGUIlqAgWNIiIiZaqqKvXNWfLYwnwo6vGM4N+lq0RVCkhy0LhkSe5eq18/+M534MgjfQlrfb1fv3Gjzz7GgsZM4xk3b/aPmMpKP566ICho3E5Bo4iISBnL1EE1n4o+0wgKGqWgJM/40tpmOI3p0gWGD4ejj4bx45Pu4fnz6Uktm2kHhOdnhPRNcPJVGt+okeFzVdAoIiIiZSk5QEsO4PKhaOdoTKSgUQpIr17h5VwHjeCrtM18Y5wLLoAvfQk6VW3Cli5mLz5gG5V0ZD3V4/cN7VeQ023EpMs0ZqOrUBHK8ixLIiIiUkwaaoaTLyWRaRw92reZjTXAmTMHFi/2rWhF8izfmUbw922nTr7pTm2tn9N19MrJbHbXsZi+zGUw3XbukXKDF+R0GzFDhvh2zrGuqcuX+zlEamqiPa8IKNMoIiJSxjJNu5FPRT+mEaBdOxg1KrzujTeiORcpe9meq7GpBg3y/86f7xNyFW++Tns2sQL/i6b7uGEp+xR0prGiwreITVSmTa4UNIqIiJQxjWnMorFjw8sqUZWIRFGeGnvd9u19E5zaWuC119hKFWvpTAX1dDl475R9CjrTCGqGE1DQKCIiUsYKIWhMzjQWbdCocY1SIJIzjcuWxbua5pIZDBzov543px5ef51VdAVgB9ZQeeC4lH0KdrqNGAWNgIJGERGRshZ1I5z6+hIpT4XUoHHKlPgYR5E8at/ez00fU1eXv+l0+vaFtm1h3bS5LF9dGS9N7VLvJ3RMoqCxOChoFBERKWNRN8JZsybcjLBzZ99PpigNGuQnrYvZuLFsxz9J9KIa11hREWQbp01jHoNYif8UqNvYYf7JJAUfNCaPaZw+PTyxZJlQ0CgiIlLG0mUa89lRviSm24gxU4mqFIzkoHHJkvy9dk0/R9Xkl1lNFzbRjjZspdP4UWm3TR7TWFCNcMD/UorV3IJP206fHt35RERBo4iISBlr394/YurqUt/E5VLJNMGJUdAoBSI5aKytzd9rV95wPf3ffHj7cjdWYgcekHbbgs80gkpUUdAoIiJS9qJshlNSmUZQ0CgFI7JM4zPPwBVXMIAFVOC773TbcyCMH59284KeciNGQaOCRhERkXIXZTOckumcGjN6dHhQ5pw5sHhxZKcj5SuSMY0zZ8LEieAcbahjF2bSfYdt9Hr4trTjGaEIptwABY0oaBQRESl7UTbDKblMY7t2MCpp7JayjRKBvJenrl0LJ5wAq1ZtX1VTsYS9Hr6KqqE7pd1l61bfLyqmogI6dszxebaEgkYFjSIiIuWukMpTiz7TCCpRlYKQ1/JU5+DrX4dp08Lrr78ejjgi427pmuCYZf/0Wm3XXaG6Or68eHH+2tEWCAWNIiIiZS7KoLHkylNBQaMUhJ49wwHYypU5nDb0V7+CRx4JrzvtNLj00gZ3K4omOABVVTBiRHjdhx9Gcy4RUdAoIiJS5pIDNWUaWyk5aJwyBbZsieZcpGxVVaXeTzkpUX3ySbjyyvC6vfeGO+5oNG1YNEEjlH2JqoJGERGRMldIjXCKfkwj+Dndamriy5s2wfvvR3c+Urb69AkvZ72i8tNP4fTTw5O79ugB//oXdOjQ6O5F0Tk1JjloLLN7WkGjiIhImYuqEc62balvGrt2zc9r55RZarbxjTeiORcpazntoLpmjW98k3gTV1bCP/4BO+7Y5EMkUqaxcCloFBERKXNRjWlctSqcoNhhB19SVxLGjg0va1yjRCBnQWN9PZx9Nnz8cXj9b38Lhx3W5MMUdXnqtGlQVxfNuURAQaOIiEiZ69w5HKxt3Bhug58rJTmeMUbNcKQA5KyD6jXXwKOPhtedcQZ873vNOky67qkFq1cv6Ns3vrxliy/PLRMKGkVERMqcWTTNcEpyPGPM6NHQpk18ec4c36ZfJI9yMlfj44/DVVeF1+2zD9x+e7PnyyiqTCOUdYmqgkYRERGJpBlOSWca27WDUaPC65RtlDzLennq9Ok+o5ioVy/f+KZ9+2Yfrqga4YCCRhERESlvUYxrLMk5GhOpRFUiltXy1NWrfeObxJrSWOObQYNadMiiaoQDChpFRESkvEXRQbWkM42goFEi17VruEp6wwZYv74FB6qvhzPPhE8+Ca//3e/gkENafH4qTy0eChpFREREmcZcSA4ap0zxzTNE8sTMV48malGJ6i9+4ccyJjr7bPjud1t8blCEQeNuu4W7hs2fn/qLrEQpaBQREZFIGuEkZxpLqhEOwMCBUFMTX960qewmBJfo9ekTXm520Pjoo3D11eF1Y8bAbbc1u/FNoro6n/mMMYNOnVp8uPyorvaBY6IPP4zmXPJMQaOIiIioEU4umKlEVSLXqkzjRx/5stREvXvDI4/4Zk+tkG66jVbEoPkzcmR4uUxKVBU0ioiISCTlqSWfaYTUoPGpp2DRomjORcpSizONq1b5xjfr1sXXVVXBQw/5LHorFV1pakyZjmusanwTERERKXX5Dhrr6mDt2viymW/aUXKSg8Znn/Ulq0OGwPjx/nHggbDrrkWSZpFi06JpN7Ztg9NPhxkzwut//3v/M5sF6TKNRUFBo4iIiJSrbt18zOKcX16zxgd2VTl6p5DcO6JLF6goxfqnffbx46A2bw6vnzXLP+691y/36uWDx1gguffeufvPl7LSovLUq67yWfFE3/gGXHhh1s6rZDKNH37og+zKymjOJ09K8deziIiINFNlZeqbtlyOayz58Ywx7drB97/f+Ha1tX6C9EsvhS98waddJ0zwXStffLGF8ySItKA89eGH4dprw+v23Rduvjmr2fDkoLFoMo39+oVLMzZs8B8AlTh9hCUiIiKAD9xWrYovr1iRWtqWLSU/3UaiX/3KB4BPPw2TJ/upN+rqGt5n/XqYNMk/wGcd99knXs564IHQs2fuz12KXrryVOcyxH9Tp/qpNBL16ZOVxjfJkstTiybTaOazjS+9FF/3wQcwdGh055QHyjSKiIgIkN9xjWWTaQT/JvOww+A3v/HdU1ev9tnDq6/2wWRT5hmoq4P//hduuAG+8hVfc7j77nDeefDXv8Ls2fHaYpEEHTpAx47x5bq68IdD261c6RvfJGa127Txmcf+/bN+XkVbngplOa5RmUYREREB8hs0JmcaS7JzaiYdOsChh/oH+Hfx778Pr77qM5Gvvtq0gWfTp/vHHXf45f79w+MiR4wo+XFW0jS9e/vPFWKWLEm657Ztg9NOg88+C+94001wwAE5OScFjcVFQaOIiIgAqUGjxjTmSVUVjB7tH5dc4jOGM2f64DEWSM6c2fhxPv8cHnzQP8C/Cz/ggHggOWZM1ksMpTgkB421tUlz1F91FTzzTHinb30Lzj8/Z+dUtGMaQUGjiIiIlK/kwC2f5alllWlsjJkfHzV0qO9YCX5ux//8Jx5Evvce1Nc3fJzVq30HzFgXzOpq32QnNi5y//1LdJ4TSZY8rnHJkoSF55/3424TjRsHf/xjTqeBKdoxjQDDh/t2z7F7cNYsP4dQ587RnlcOKWgUERERQJnGgtavH5x0kn+Af8f9+uvxctY334RNmxo+xubNfvvJk/2yGey5Z3i+yByMXZPoZeygumQJnHlmeDxs377w0EP+Q4YcKupMY4cO/kOdTz6Jr5s6NXVe1hKioFFERESA/GYay3pMYzbssAN88Yv+AbBlC7z9djyInDw59T85mXO+rO6DD/x0CgA77RQvZz3wQF/DmMNsk+RH2rka6+vhrLPCaceKCrj/fqipyfk5FXWmEXyJamLQ+MEHChpFRESk9Kl7ahFr29a/YR03Dn70Ix8QfPxxuLnOvHmNH2f2bP/461/9cs+e8Sk+xo+HUaN8R00pKumm3eA3v4Hnngs/ceWVcMghOT+fbdtg3br4slkRVnbutRf885/x5RIf16igUURERIDUoHHlygbmc2uFLVvCXf0rKoowy1DoKip899QRI+Db3/br5s0LZyKnTm38OMuWwaOP+gf4sryxY+OZyLFjmzZliEQqpTz14+Vw+0/CKw86CH7607ycz9q14eVOnfyPbFEZOTK8rKBRREREykF1tY8JNmzwy9u2+XFH2e6Vkq40VRWQeTBokJ9W4bTT/PKKFfDaa/EurVOmwNatDR9jwwY/x+SLL/rlykqffYyNizzggNS0lkSuZ8+EhbqtrHjrE+q2JQQCPXrAfff5Tr55UNTTbcSk66Cai0/ZCoSCRhEREdmuR4940Ag+rsh10KjS1Ih07w5f/rJ/AGzcCP/9bzwb+dprqSmhZNu2+WBzyhS48Ua/btiw8LjIIUNK9o10sWjTxl/uFSscfPIJbvNmltGTvgTjGe+5BwYMyNv5lETQOGiQH1scG5y5Zo3P5g8eHO155YiCRhEREdmue3eYPz++vHy5f8+fTZpuo0C1bw8HH+wf4APCDz4Ij4tcvLjx43zyiX/cdZdf7tcvHkCOH+87tlZW5u77kLR694YVHy70kzQCS+ntg8ZLLol/cJAnJRE0mvlsY6wbMfj7pUSDxrxVD5vZUWb2iZnNNLPL0zxfbWYPBs+/aWY7ButPN7P3Eh71Zra3mXUwsyfNbLqZTTOzX+frexERESlV+WiGoyY4RSJWenrxxfCPf8DChTBzJtx9N5x7Luy6a9OOs2iR3//ii/3xuneHo4/2cwP++9+NTxUiWdHbLfHXL7CU3jB6NPw6/2+hi3q6jUTJJarvvx/NeeRBXjKNZlYJ3AxMABYAb5nZY865jxI2OxdY6ZzbxcxOBa4DTnHO3QfcFxxnT+D/nHPvmVkH4LfOuZfMrC3wgpkd7Zx7Oh/fk4iISClKDhqnTIGjjsruayjTWKTMYOed/ePrX/frliyB//wnno18912foWzImjXwzDP+Ab7z65gx8WzkAQfohyLb1q+n96O3gztw+6ol1YPhgdtyPh9jOkU/3UZMunGNJSpfmcZ9gZnOuVnOuS3AA8DxSdscD9wbfP0QcLhZSgH8ROB+AOfcBufcS8HXW4B3gPwVY4uIiJSgvfcOL7/4YtNmamgOZRpLSJ8+8NWv+vGMb73lB6w+9xz87Gdw6KG+5LUxW7b48ZPXXQfHHus/udhrL7jwQj9nYGK9tLTMd79L7yXhgKb25Athl10iOZ2SzTSWcNCYrzGN/YHEO34BsF+mbZxzdWa2GugBLEvY5hRSg03MrCtwLPCHLJ6ziIhI2Rk3zs/vPnu2X3bOT9n3k580vF9zpOueKiWic2eYMME/wHdjfeed8FQfjdU8Owcffugft97q1w0eHB4XudtuRThHQ0Tuuw/uvpveHBRf17cfSwbsFtkplUymcY89wsszZvhOYh06RHM+OZSvuy1dyyzXnG3MbD9gg3MuNKmQmVXhs483OedmpX1xs/PMbIqZTakNBv+KiIhIKjM466zwuhdeyG6yR5nGMtKmDey3H/zgB36ux6VL4aOP4M9/hjPPhB13bNpx5s6Fv/3Nzzk5YoTv6nL88fDb38Ibb/hspaSaMWP7PJ29WerXdegAQ4eydGl0p1UymcbOncOdwurr/c93CcpX0LgAGJiwPABYmGmbIBDsAiT+WTmVoDQ1ye3ADOfc7zO9uHPudufcGOfcmF69erXg9EVERMrHwQeH38vHso3Zoik3ylhFBey+O5x3HvzlLz6lPX++L0O98EJf7teU6TmWL4fHHoMf/cinx7t29eWwP/uZL49tbKqQcrB5M5x6KqxbBwRBo1X4oLuykijzKCWTaYSyKVHNV9D4FjDUzHYKmtacCjyWtM1jwNnB1ycBLzrnHICZVQAn48dCbmdm1+CDy0tyeO4iIiJlxQzOPju8btIkWLAgO8dXplFCBgzwwc3NN/vuk8uXw5NPwuWX+6Y4bds2foyNG+Hll+GXv4QvftEHkWPG+OkkHn7YN+wpN5df7kuDA91ZQdWwnaFjJ8DH1Rs3RnNqJTHlRkyZBI15GdMYjFG8CHgWqAT+1zk3zcyuBqY45x4D7gL+amYz8RnGUxMOcRCwILH81MwGAD8BpgPvBD1z/uScuzMf35OIiEgpO/hgP4xs7ly/HMs2XnFF6467aVP4jWpVFXTq1LpjSonp1g2OOcY/wP/QvPVWfFzkf/6TmqpKVl8Pb7/tH38IWl4MHRoeF7nzzk3Lahajxx+H34eL8OzEE+nVtj+LFsXXLV0azbSCChqLjwXJvLIxZswYN2XKlKhPQ0REpOC99BJcfXV82cxXFA5oRa/yRYvgtNPiy716+Sn8RJps2zaYOjXeWOfVV/0cks3Vt68PIGNB5MiRfm7KYrdggf9eElP6gwfDe+/xvau6hmKa66+HL3whv6e3bZvvk5QYgkyaVMT/9TNmhOcs7dEDamsL/gMJM3vbOTemqdvnq3uqiIiIFJlcZBs1R6O0WmWlD4pGjoSLLvI/mHPm+OAxFkhOn974cRYvhoce8g/wTU3GjYtnI/fbr2lThhSSujr/qUzijVZZCQ88AF270qdPePMomuGsXRsOGDt1KuKAEXzGukMH3zUVfHn1okVQUxPteWWZehWLiIhIWhUVqZ1UJ02Czz9v+TE1nlGyzszPE3PWWXDHHfDxxz4a+te/4NJLYd99mxaVrF3rm+hceaVvqtOlC+y/P1x2mS/3TP7hLUS//KUPnBNdey2MHQv4prOJoggaS6oJDvhflHvuGV5XgiWqChpFREQko0MOgUGD4sv19a3rpKo5GiUvevWCE06AG26AN9/0g+gmTYKf/xwOP7xp8+ht3Qqvvw6/+Q0cd5wvO9xjD7jgAj/34bx5Of82muWll3zQmOjII32H2UAhBI0lM91GojIY16jyVBEREckolm285pr4uuef91Ps9e/f/OMp0yiR6NjRB4uHH+6Xt26F996Ll7NOnkyT5qCYNs0/brvNLw8cGG6uM3y4v2nyrbYWTj89XPfZt68fhJxwPslBYxRNZUsu0whlETQq0ygiIiINOvRQ/944pr7ez7PeEpqjUQpCmza+A8yll8Ijj/joafp0X9569tnhCdsbMn8+/P3vfo7JPfeEnj3h2GN9h5nXXoMtW3L7fYC/Ic8+m1BbVDNfEpA0iDE5aIxirsayyDS+/34055FDyjSKiIhIg2LZxmuvja977jmfbWxurwdlGqUgmcGwYf7xzW/6dQsXxruzTp7sA4HGZh1YuRKeeMI/ANq18w11YpnIceOyHyX9/vfw9NPhdVdcAUcckbJpuvJU5/Lb6LMkM43JYxqnT4fNm6G6OprzyQFlGkVERKRRhx0WnmqjpdlGBY1SNGpq4Gtfgz/+Ed591weETz8NP/6xDwCbEhBs2gSvvOI/cTnqKD+Id5994OKL4Z//DGcHW+Ktt+Dyy8Pr9t8ffvGLtJt37BhuCLtlS2rmL9dKao7GmG7dwuUYdXVN6+BbRBQ0ioiISKPSdVJ99tnmv+dVIxwpWl26+MDv2mvh3//20c/kyfDrX8OXvgRduzZ+jPp6H4D+8Y8+IK2pgV12gXPOgbvugk8/bTybGbN6NZx6qh+fGdO1K9x/P1SlLyY0i75EtSSDRij5cY0KGkVERKRJWpttdE6ZRikh1dVwwAHw//6fL0ddvtwHCjffDBMnhm+Whnz2Gdxzjy+LHTbMN7A58URfdjplis9aJXMOzj8fZs0Kr7/77nC74zSiboajoLE4aUyjiIiINEllpR/H+D//E1/37LNwxhnQr1/j+2/c6If5xLRp07SZD0SKQmy+vj339I1xnPPTcrz6anxc5EcfNX6cpUt9c55HHvHLHTv6sZDjx/vHfvv55jsPPhje76KL/DQjjUjqjZP3aTdKshEOKGgUERERiTn8cN+UccECv7xtm5+y7oc/bHzfdFnGfDbgEMkrMxg82D/OOMOvW7bMd1WNBZGZMomJ1q/3c0xOmuSXq6pSb5yRI/18kk3Qq1d4Od9BY0k2woGSDxpVnioiIiJNFss2JnrmGVi8uPF9NZ5Ryl7PnnDccT7Ae/11n3Z78UW4+mqYMAE6dWr8GHV14XGMHTv6rGO7dk06BWUac2TXXaFt2/jy4sX5/8/NIQWNIiIi0iyHHw79+8eXY9nGxmg8o0iSDh38RKhXXunnsVm50mcfb7zRj2tMHoCYzi23+LGQTZRu2o18qa+HtWvD60omaKyqghEjwus+/DCac8kBBY0iIiLSLOmyjU8/3Xi2MTnTqKBRJElVFYweDZdcAg895G+qTz/1nVW//nXfaTXRd76T2ta4EVGWp65bF24O26FDxkavxWnkyPByCZWoltJlEhERkTw54gj4y1/8/OcQzzb+4AeZ91GmUaSZzGDoUP/4xjf8ukWL4O23/fQaBxzQ7EMmB43Llvn7t7IyC+fbiJIdzxhTwuMalWkUERGRZsuUbWyofb/GNIpkQb9+8OUvw4EHtqiTVHV1eEpJ5/xsIflQstNtxChoFBEREQmbMMHPTR7T2NhGZRpFCkNU4xrLLmicNq3x7rhFQkGjiIiItEhlZXwmgZinn878BjQ5aFSmUSQayUFjQxUC2VTyQWOvXtC3b3x582Y/JrUEKGgUERGRFpswwVfLxdTVZc42KtMoUhiSg8ba2vy8bvKYxpLpnJqoREtUFTSKiIhIi1VVpY5tfOqp1GyjcxrTKFIolGnMIQWNIiIiIqnSZRv//vfwNuvXh+cjr66G9u3zc34iEtanT3g5qjGNyjQWDwWNIiIi0ipVValjG598MvxGNF1pagsaP4pIFkQ1V2PJT7kBChpFREREMjnyyHD/h+RsY3JpqsYzikSnULqnlmSmcbfd/CdpMfPnp/4CLEIKGkVERKTVMmUbYw021ARHpHD06AEVCVHAmjW+0WeulUWmsbraB46JPvwwmnPJIgWNIiIikhVf/GJ4rFRitlFNcEQKR0UF9OwZXpePbGNZNMKBkixRVdAoIiIiWZEu2/jEE7BsmTKNIoUm381wnCuTKTdAQaOIiIhIQ446KjxeKpZtTA4alWkUiVa+xzWuWwf19fHl9u2hTZvcvmZkRo4MLytoFBEREYlLN2/jE0/AzJnhdco0ikQr30FjWYxnjIllGisrYfhwGDEi2vPJgqrGNxERERFpuqOOgr/+Nf4mdOtWmDEjvI0yjSLRSg4alyzJ7euVzXhG8BPXvvMO7L47tGsX9dlkhTKNIiIiklXpxjYmU6ZRJFrJQWOs03GulM14RvCT0I4aVTIBIyhoFBERkRw4+ujUN6WJFDSKREuZRmkOBY0iIiKSdVVVcPrp6Z/r0MFPZSYi0Uk3ptG53L1ectBY0pnGEqSgUURERHLi6KOhV6/U9RrPKBK9zp3DH95s3gxr1+bu9cqqEU4JUtAoIiIiOdGmTfpso0pTRaJnlt8OqipPLW4KGkVERCRnjjkmNduoTKNIYejTJ7ysoFEyUdAoIiIiOZMu27jLLtGci4iEJX+gk8+gUWMai4uCRhEREcmpY4/14xvbtIGRI+G446I+IxGB/GYaNaaxuFVFfQIiIiJS2ioq4LLL4Ec/8stm0Z6PiHhRjmlUprG4KGgUERGRvFCwKFJY8hU0OpeaaVTQWFxUnioiIiIiUoaSg8YlS3LzOhs2wLZt8eV27TRXa7FR0CgiIiIiUoaSg8Zly6C+Pvuvo9LU4qegUURERESkDFVXhwO4+npYvjz7r6MmOMVPQaOIiIiISJnKx7hGzdFY/BQ0ioiIiIiUqeSgsbY2+6+hoLH4KWgUERERESlT+WiGozGNxU9Bo4iIiIhImerTJ7yci/JUjWksfgoaRURERETKVK9e4eV8jGlUprH4KGgUERERESlTyjRKUyhoFBEREREpU+qeKk2hoFFEREREpEz16AFm8eVVq2DDhuy+RnKmUeWpxUdBo4iIiIhImaqshL59w+sefDC7r6FMY/FT0CgiIiIiUsYOPzy8/OCDsGxZdo7tnILGUqCgUURERESkjE2cCF27xpc3b4Y778zOsTduhLq6+HJ1tX9IcVHQKCIiIiJSxjp0gHPOCa977jmYMaP1x9Z0G6VBQaOIiIiISJn70pdg8OD4snNwyy3+39bQdBulQUGjiIiIiEiZq6yECy4Ir3vvPXj99dYdV5nG0qCgUURERERE2HdfGDMmvO7WW8NjEptLmcbSoKBRREREREQw89nGxHkbFyyAxx5r+TGVaSwNChpFRERERASAIUP8+MZE994La9e27HjKNJYGBY0iIiIiIrLdOedA+/bx5TVr4G9/a9mxNEdjaVDQKCIiIiIi23XvDqedFl73yCOwcGHzj6WgsTTkLWg0s6PM7BMzm2lml6d5vtrMHgyef9PMdgzWn25m7yU86s1s7+C50Wb2YbDPTWaJFdgiIiIiItISJ58MvXrFl+vq4Pbbm38cBY2lIS9Bo5lVAjcDRwPDgYlmNjxps3OBlc65XYAbgesAnHP3Oef2ds7tDZwJzHHOvRfscytwHjA0eByV829GRERERKTEVVfDt74VXvfKKzB1avOOo0Y4pSFfmcZ9gZnOuVnOuS3AA8DxSdscD9wbfP0QcHiazOFE4H4AM+sH7OCce90554C/ACfk6hsQERERESknRxwBw4aF1918MzjX9GOoEU5pyFfQ2B+Yn7C8IFiXdhvnXB2wGuiRtM0pBEFjsP2CRo4pIiIiIiItYAYXXhheN306vPhi0/Z3TpnGUpGvoDHdWMPkzyga3MbM9gM2OOemNmX70IHNzjOzKWY2pba2tinnKyIiIiJS9vbaC8aPD6+7/XbYvLnxfTdvhq1b48tt2kC7dtk9P8mPfAWNC4CBCcsDgOT+S9u3MbMqoAuwIuH5U4lnGWPbD2jkmAA45253zo1xzo3plTiiV0REREREGnTeeVBZGV9euhQefrjx/dJlGdXKJVuvAAANlklEQVS2sjjlK2h8CxhqZjuZWVt8APhY0jaPAWcHX58EvBiMVcTMKoCT8WMhAXDOLQLWmtnYYOzjWcD/5fbbEBEREREpLwMGwFe+El53332walXD+2k8Y+nIS9AYjFG8CHgW+Bj4h3NumpldbWbHBZvdBfQws5nApUDitBwHAQucc7OSDn0BcCcwE/gMeDqH34aIiIiISFk66yzo3Dm+vGED3H13w/touo3SUZWvF3LOPQU8lbTuZwlfb8JnE9Pt+zIwNs36KcAeWT1REREREREJ6dzZB4433xxf9/jjPgO5447p91HQWDryVZ4qIiIiIiJF7IQToH/CXAXOwW23Zd5eQWPpUNAoIiIiIiKNqqqC888Pr3vzTZgyJf32mm6jdChoFBERERGRJjnwQD8NR6Jbb4X6+tRt1QindChoFBERERGRJjGDCy8Mr5s1C55O045SmcbSoaBRRERERESabNgwmDAhvO6uu3xH1UTKNJYOBY0iIiIiItIs3/wmtG0bX165Eh54ILyNMo2lQ0GjiIiIiIg0S+/e8LWvhdc9+CAsXRpfVqaxdChoFBERERGRZps4Ebp1iy9v2eLLVGM05UbpUNAoIiIiIiLN1qEDfOMb4XXPPQeffAKbN/tHTFUVtG+f3/OT7FHQKCIiIiIiLXLMMTBkSHjdLbekzzKa5e+8JLsUNIqIiIiISItUVMAFF4TXffABPPlkeJ2a4BQ3BY0iIiIiItJiY8bAfvuF1/397+FljWcsbgoaRURERESkVb797XD5aV1d+HllGoubgkYREREREWmVHXeEL3858/PKNBY3BY0iIiIiItJq55zjO6qmo0xjcVPQKCIiIiIirdatG5x2WvrnlGksbgoaRUREREQkK046CXr3Tl2voLG4KWgUEREREZGsqK6G885LXa+gsbgpaBQRERERkaw57DDYbbf4shnstFN05yOtp6BRRERERESyxgx+/nMYNgw6d4bzz09fsirFoyrqExARERERkdLSpw/cdlvUZyHZokyjiIiIiIiIZKSgUURERERERDJS0CgiIiIiIiIZKWgUERERERGRjBQ0ioiIiIiISEYKGkVERERERCQjBY0iIiIiIiKSkYJGERERERERyUhBo4iIiIiIiGSkoFFEREREREQyUtAoIiIiIiIiGSloFBERERERkYwUNIqIiIiIiEhGChpFREREREQkIwWNIiIiIiIikpGCRhEREREREclIQaOIiIiIiIhkZM65qM8hr8ysFpgb9XnkSU9gWdQnIZHQtS9fuvblSde9fOnaly9d+/KVjWs/2DnXq6kbl13QWE7MbIpzbkzU5yH5p2tfvnTty5Oue/nStS9fuvblK4prr/JUERERERERyUhBo4iIiIiIiGSkoLG03R71CUhkdO3Ll659edJ1L1+69uVL17585f3aa0yjiIiIiIiIZKRMo4iIiIiIiGSkoLFAmNn/mtlSM5uatP5kM5tmZvVmlrFLkpn9xsymm9kHZvYvM+ua8NxeZvZ6cJwPzaxdmv13MrM3zWyGmT1oZm2D9dXB8szg+R2z911LAVz3i4Jr68ysZ8L604NjfmBmr5nZyGx9z+Ll6toH1+69hEe9me2dZn/d8xEpgGuv+z4iObz2bczs3uB3/cdmdkWG/UcH28w0s5vMzIL13c3s+eD3wfNm1i2b37cUxLW/1szmm9m6pPWXmtlHwXFfMLPB2fh+xcvhdW9rZncH1/19Mzskw/5Z+1uvoLFw3AMclWb9VOCrwL8b2f95YA/n3F7Ap8AVAGZWBfwN+LZzbgRwCLA1zf7XATc654YCK4Fzg/XnAiudc7sANwbbSfbcQ7TX/T/AEaTOXTobODg47i/RuIlcuIccXHvn3H3Oub2dc3sDZwJznHPvpdlf93x07iHaa6/7Pjr3kINrD5wMVDvn9gRGA+dneBN4K3AeMDR4xM7lcuCF4PfBC8GyZNc9RHvtHwf2TbP+XWBMcNyHgOsbOQ9pnnvIzXX/FkBw3ScAN5hZurgua3/rFTQWCOfcv4EVadZ/7Jz7pAn7P+ecqwsW3wAGBF8fCXzgnHs/2G65c25b4r7BJ42H4X9ZANwLnBB8fXywTPD84bFPJqX1orzuwfp3nXNz0qx/zTm3Ms1xJUtyeO0TTQTuT16pez5aUV77YH/d9xHJ4bV3QMfgA8P2wBZgTeK+ZtYP2ME597rzDS3+Qvr7PvH3gWRJlNc+2P8N59yiNOtfcs5tSHNcyYIcXvfh+A94cM4tBVYBoYxltv/WK2gsTd8Ang6+3hVwZvasmb1jZpel2b4HsCrhh3IB0D/4uj8wHyB4fnWwvRSe5l73pjo34bhSmBKvfaJTSB846J4vHc299k2l+77wJV77h4D1wCJgHvBb51zyG9X++Hs9JvG+7xMLKIJ/e+fqpCUrmnvtm0r3fWFLvO7vA8ebWZWZ7YTPMg9M2j6rf+urWn36UlDM7CdAHXBfsKoKOBD4ArABeMHM3nbOvZC4W5pDuSY8JwWihde9Kcc9FP9H5MAsnq5kUZprH1u/H7DBOTc13W5p1umeLzItvPZNOa7u+wKX5trvC2wDaoBuwKtmNsk5NytxtzSH0r1dZFp47Zty3DPwmaqDs3i6kiVprvv/ArsDU/BDDV4Lng/tluZQLf5br0xjkQoGv75nZk8lrDsb+DJwuovPpbIAeMU5tywoP3gK2CfpcMuArkFpA/jU98KE/QcGx68CupAmzS75keXr3thr7QXcCRzvnFuene9AWqoZ1z7mVDJnmnTPF5EsX/vGXkv3fQFpxrU/DXjGObc1KFX7D0mlavh7O7H0MPG+XxKUr8bKWJdm/7uR5sjytW/stY4AfgIc55zbnJ3vQFqiqdfdOVfnnPt+MI79eKArMCPpcFn9W6+gsUg5584JflCOATCzo4D/h7/hNyRs+iywl5l1CH4oDgY+SjqWA14CTgpWnQ38X/D1Y8EywfMvpnmDInmSzeveEDMbBDwCnOmc+zR734G0VDOuPcFg+JOBBzIcS/d8EcnmtW+I7vvC04xrPw84zLyOwFhgetKxFgFrzWxsMHbpLNLf94m/DyQi2bz2DTGzUcCfg+Pqw4KINfW6B+/vOgZfTwDqnHO5fX/vnNOjAB74T4UX4TtcLgDODdZ/JVjeDCwBns2w/0x8bfJ7weO2hOfOAKbhOzVdn2H/IcB/g+P8E9+JC6BdsDwzeH5I1P9XpfQogOt+cfA6dfhPn+4M1t+J77IVO+6UqP+vSu2R42t/CPBGI6+ve758r73u+xK79kCn4L6dhv+A8EcZ9h8T/E34DPgTYMH6HvimGjOCf7tH/X9Vao8CuPbXB69TH/z782D9pOB1Y8d9LOr/q1J65PC67wh8AnwcXMPBGfbP2t/62C8LERERERERkRQqTxUREREREZGMFDSKiIiIiIhIRgoaRUREREREJCMFjSIiIiIiIpKRgkYRERERERHJSEGjiIiUNTMbZGbrzKwy6nMREREpRAoaRUSk7JjZHDM7AsA5N88518k5ty2Pr3+ImS3I1+uJiIi0hoJGERERERERyUhBo4iIlBUz+yswCHg8KEu9zMycmVUFz79sZteY2WvB84+bWQ8zu8/M1pjZW2a2Y8LxdjOz581shZl9YmZfS3juGDP7yMzWmtnnZvZDM+sIPA3UBMdfZ2Y1Zravmb1uZqvMbJGZ/cnM2iYcy5nZhWY2IzjeL81s52CfNWb2j9j2sUymmf3YzJYFmdXT8/M/LCIipUZBo4iIlBXn3JnAPOBY51wn4B9pNjsVOBPoD+wMvA7cDXQHPgauAggCwOeBvwO9gYnALWY2IjjOXcD5zrnOwB7Ai8659cDRwMKgLLaTc24hsA34PtATGAccDlyYdF5HAaOBscBlwO3A6cDA4PgTE7btGxyrP3A2cLuZDWvWf5aIiAgKGkVERNK52zn3mXNuNT4r+JlzbpJzrg74JzAq2O7LwBzn3N3OuTrn3DvAw8BJwfNbgeFmtoNzbmXwfFrOubedc28Ex5kD/Bk4OGmz65xza5xz04CpwHPOuVkJ5zkqafsrnXObnXOvAE8CX0NERKSZFDSKiIikWpLw9cY0y52CrwcD+wUlpavMbBU+89c3eP5E4Bhgrpm9YmbjMr2gme1qZk+Y2WIzWwP8Cp8pbMl5AawMspoxc4GaTK8vIiKSiYJGEREpRy5Lx5kPvOKc65rw6OScuwDAOfeWc+54fOnqo8RLYdO9/q3AdGCoc24H4MeAteLcugXlszGDgIWtOJ6IiJQpBY0iIlKOlgBDsnCcJ4BdzexMM2sTPL5gZrubWVszO93MujjntgJr8OMWY6/fw8y6JByrc7DNOjPbDbggC+f3i+A8xuNLaf+ZhWOKiEiZUdAoIiLl6H+AnwblpCc1tnEmzrm1wJH4xjkLgcXAdUB1sMmZwJyg3PTbwBnBftOB+4FZQVlrDfBD4DRgLXAH8GBLzyuwGFgZnNd9wLeD1xUREWkWcy5bFToiIiJSCMzsEOBvzrkBUZ+LiIgUP2UaRUREREREJCMFjSIiIiIiIpKRylNFREREREQkI2UaRUREREREJCMFjSIiIiIiIpKRgkYRERERERHJSEGjiIiIiIiIZKSgUURERERERDJS0CgiIiIiIiIZ/X+3Rq+Lzg5+BwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df = eval_df[(eval_df.h=='t+1')][['timestamp', 'actual']]\n",
    "for t in range(1, HORIZON+1):\n",
    "    plot_df['t+'+str(t)] = eval_df[ (eval_df.h=='t+'+str(t))]['prediction'].values\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.plot(plot_df['timestamp'], plot_df['actual'], color='red', linewidth=4.0)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(plot_df['timestamp'], plot_df['t+1'], color='blue', linewidth=4.0, alpha=0.75)\n",
    "ax.plot(plot_df['timestamp'], plot_df['t+2'], color='blue', linewidth=3.0, alpha=0.5)\n",
    "ax.plot(plot_df['timestamp'], plot_df['t+3'], color='blue', linewidth=2.0, alpha=0.25)\n",
    "plt.xlabel('timestamp', fontsize=12)\n",
    "plt.ylabel('load', fontsize=12)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take input here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-28 12:00:00</th>\n",
       "      <td>5,153.672513960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 14:00:00</th>\n",
       "      <td>5,153.671504970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 16:00:00</th>\n",
       "      <td>5,153.672332760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 18:00:00</th>\n",
       "      <td>5,153.673091890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 20:00:00</th>\n",
       "      <td>5,153.675785060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:00:00</th>\n",
       "      <td>5,153.674497600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                sqrt_A\n",
       "Epoch_Time_of_Clock                   \n",
       "2017-11-28 12:00:00 5,153.672513960000\n",
       "2017-11-28 14:00:00 5,153.671504970000\n",
       "2017-11-28 16:00:00 5,153.672332760000\n",
       "2017-11-28 18:00:00 5,153.673091890000\n",
       "2017-11-28 20:00:00 5,153.675785060000\n",
       "2017-11-28 22:00:00 5,153.674497600000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df.iloc[156:162  , :]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sqrt_A'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key , value in enumerate(columns):\n",
    "    new_df[value] = a[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.dropna( how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-28 12:00:00</th>\n",
       "      <td>5,153.672513960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 14:00:00</th>\n",
       "      <td>5,153.671504970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 16:00:00</th>\n",
       "      <td>5,153.672332760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 18:00:00</th>\n",
       "      <td>5,153.673091890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 20:00:00</th>\n",
       "      <td>5,153.675785060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:00:00</th>\n",
       "      <td>5,153.674497600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                sqrt_A\n",
       "Epoch_Time_of_Clock                   \n",
       "2017-11-28 12:00:00 5,153.672513960000\n",
       "2017-11-28 14:00:00 5,153.671504970000\n",
       "2017-11-28 16:00:00 5,153.672332760000\n",
       "2017-11-28 18:00:00 5,153.673091890000\n",
       "2017-11-28 20:00:00 5,153.675785060000\n",
       "2017-11-28 22:00:00 5,153.674497600000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 11, 29)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating index for output\n",
    "import datetime\n",
    "date = new_df.index.date[0]\n",
    "date + datetime.timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = new_df.index + datetime.timedelta(days =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2017-11-29 12:00:00', '2017-11-29 14:00:00',\n",
       "               '2017-11-29 16:00:00', '2017-11-29 18:00:00',\n",
       "               '2017-11-29 20:00:00', '2017-11-29 22:00:00'],\n",
       "              dtype='datetime64[ns]', name='Epoch_Time_of_Clock', freq='2H')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.index= date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sqrt_A'], dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                sqrt_A\n",
      "Epoch_Time_of_Clock                   \n",
      "2017-11-29 12:00:00 5,153.672513960000\n",
      "2017-11-29 14:00:00 5,153.671504970000\n",
      "2017-11-29 16:00:00 5,153.672332760000\n",
      "2017-11-29 18:00:00 5,153.673091890000\n",
      "2017-11-29 20:00:00 5,153.675785060000\n",
      "2017-11-29 22:00:00 5,153.674497600000\n",
      "Index(['sqrt_A'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(new_df)\n",
    "print(new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             sqrt_A\n",
      "Epoch_Time_of_Clock                \n",
      "2017-11-29 12:00:00 -0.903708056080\n",
      "2017-11-29 14:00:00 -1.147318784535\n",
      "2017-11-29 16:00:00 -0.947457016901\n",
      "2017-11-29 18:00:00 -0.764172532240\n",
      "2017-11-29 20:00:00 -0.113933079899\n"
     ]
    }
   ],
   "source": [
    "freq = None\n",
    "idx_tuples = []\n",
    "drop_incomplete  = True\n",
    "new_df[['sqrt_A']] = X_scaler.transform(new_df)\n",
    "new_new_df = new_df.copy()\n",
    "tensor_structure={'X':(range(-T+1, 1), [ 'sqrt_A'])}\n",
    "for name, structure in tensor_structure.items():\n",
    "        rng = structure[0]\n",
    "        dataset_cols = structure[1]\n",
    "        for col in dataset_cols:\n",
    "        # do not shift non-sequential 'static' features\n",
    "            if rng is None:\n",
    "                new_df['context_'+col] = new_df[col]\n",
    "                idx_tuples.append((name, col, 'static'))\n",
    "            else:\n",
    "                for t in rng:\n",
    "                    sign = '+' if t > 0 else ''\n",
    "                    shift = str(t) if t != 0 else ''\n",
    "                    period = 't'+sign+shift\n",
    "                    shifted_col = name+'_'+col+'_'+ period\n",
    "                    new_new_df[shifted_col] = new_new_df[col].shift(t*-1, freq=freq)\n",
    "                    idx_tuples.append((name, col, period))\n",
    "        new_new_df = new_new_df.drop(new_df.columns, axis=1)\n",
    "        idx = pd.MultiIndex.from_tuples(idx_tuples, names=['tensor', 'feature', 'time step'])\n",
    "        print(new_df.head())\n",
    "        new_new_df.columns = idx\n",
    "        if drop_incomplete:\n",
    "            new_new_df = new_new_df.dropna(how='any')\n",
    "            \n",
    "inputs = {}           \n",
    "for name, structure in tensor_structure.items():\n",
    "    rng = structure[0]\n",
    "    cols = structure[1]\n",
    "    tensor = new_new_df[name][cols].as_matrix()\n",
    "    if rng is None:\n",
    "        tensor = tensor.reshape(tensor.shape[0], len(cols))\n",
    "    else:\n",
    "        tensor = tensor.reshape(tensor.shape[0], len(cols), len(rng))\n",
    "        tensor = np.transpose(tensor, axes=[0, 2, 1])\n",
    "    inputs[name] = tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor                            X                                  \\\n",
      "feature                      sqrt_A                                   \n",
      "time step                       t-5             t-4             t-3   \n",
      "Epoch_Time_of_Clock                                                   \n",
      "2017-11-29 22:00:00 -0.903708056080 -1.147318784535 -0.947457016901   \n",
      "\n",
      "tensor                                                               \n",
      "feature                                                              \n",
      "time step                       t-2             t-1               t  \n",
      "Epoch_Time_of_Clock                                                  \n",
      "2017-11-29 22:00:00 -0.764172532240 -0.113933079899 -0.424777655362  \n",
      "[[[-0.90370805608 ]\n",
      "  [-1.147318784535]\n",
      "  [-0.947457016901]\n",
      "  [-0.76417253224 ]\n",
      "  [-0.113933079899]\n",
      "  [-0.424777655362]]]\n"
     ]
    }
   ],
   "source": [
    "print(new_new_df)\n",
    "print(inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44741413 , -0.39113715 , -0.40431628 , -0.120824456,\n",
       "         0.37140098 ,  0.27459154 ]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictions[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44741413 , -0.39113715 , -0.40431628 , -0.120824456,\n",
       "        0.37140098 ,  0.27459154 ], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.447414129972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.391137152910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.404316276312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.120824456215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.371400982141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.274591535330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sqrt_A\n",
       "0 -0.447414129972\n",
       "1 -0.391137152910\n",
       "2 -0.404316276312\n",
       "3 -0.120824456215\n",
       "4  0.371400982141\n",
       "5  0.274591535330"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(results , columns = [var_name])\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-29 12:00:00</th>\n",
       "      <td>-0.447414129972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 14:00:00</th>\n",
       "      <td>-0.391137152910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 16:00:00</th>\n",
       "      <td>-0.404316276312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 18:00:00</th>\n",
       "      <td>-0.120824456215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 20:00:00</th>\n",
       "      <td>0.371400982141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 22:00:00</th>\n",
       "      <td>0.274591535330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             sqrt_A\n",
       "Epoch_Time_of_Clock                \n",
       "2017-11-29 12:00:00 -0.447414129972\n",
       "2017-11-29 14:00:00 -0.391137152910\n",
       "2017-11-29 16:00:00 -0.404316276312\n",
       "2017-11-29 18:00:00 -0.120824456215\n",
       "2017-11-29 20:00:00  0.371400982141\n",
       "2017-11-29 22:00:00  0.274591535330"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.index = date\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-29 12:00:00</th>\n",
       "      <td>-0.447414129972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 14:00:00</th>\n",
       "      <td>-0.391137152910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 16:00:00</th>\n",
       "      <td>-0.404316276312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 18:00:00</th>\n",
       "      <td>-0.120824456215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 20:00:00</th>\n",
       "      <td>0.371400982141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 22:00:00</th>\n",
       "      <td>0.274591535330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             sqrt_A\n",
       "Epoch_Time_of_Clock                \n",
       "2017-11-29 12:00:00 -0.447414129972\n",
       "2017-11-29 14:00:00 -0.391137152910\n",
       "2017-11-29 16:00:00 -0.404316276312\n",
       "2017-11-29 18:00:00 -0.120824456215\n",
       "2017-11-29 20:00:00  0.371400982141\n",
       "2017-11-29 22:00:00  0.274591535330"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[[var_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[[var_name]] = y_scalar.inverse_transform(res_df[[var_name]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_df[[var_name]] = y_scalar.inverse_transform(res_df[[var_name]]) \n",
    "#pd.set_option(\"display.precision\", 8)\n",
    "#a = pd.Series( dtype = float)\n",
    "#list_b = []\n",
    "\n",
    "#for i in range(res_df.shape[0]):\n",
    "#    list_b.append( \"%.20f\"%res_df.iloc[i ,0])\n",
    "    \n",
    "\n",
    "#print(list_b)\n",
    "#for i in range(res_df.shape[0]):\n",
    "#    res_df.iloc[i , 1] = y_scalar.inverse_transform(np.array(res_df.iloc[i ,0]).reshape(1,-1))[0]\n",
    "#    c[0 , j] = y_scalar.inverse_transform(np.array(res_df.iloc[i ,0]).reshape(1,-1))[0]\n",
    "#    j +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print (y_scalar.inverse_transform(np.array(list_b).reshape(-1,1)))    \n",
    "\n",
    "\n",
    "#print (y_scalar.inverse_transform(list_b))    \n",
    "#a['prediction']\n",
    "#res_df['inverted'] = 0\n",
    "#c = np.array()\n",
    "#a\n",
    "#j=0\n",
    "\n",
    "#for i in range(res_df.shape[0]):\n",
    "    #res_df.iloc[i , 1] = y_scalar.inverse_transform(np.array(res_df.iloc[i ,0]).reshape(1,-1))[0]\n",
    "    #c[0 , j] = y_scalar.inverse_transform(np.array(res_df.iloc[i ,0]).reshape(1,-1))[0]\n",
    "    #j +=1\n",
    "\n",
    "#b = np.array(-1.08850443363189697266).reshape(1,-1)\n",
    "\n",
    "#y_scalar.inverse_transform(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final generated output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-29 12:00:00</th>\n",
       "      <td>5,153.674316406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 14:00:00</th>\n",
       "      <td>5,153.674804687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 16:00:00</th>\n",
       "      <td>5,153.674804687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 18:00:00</th>\n",
       "      <td>5,153.675781250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 20:00:00</th>\n",
       "      <td>5,153.677734375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 22:00:00</th>\n",
       "      <td>5,153.677246093750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                sqrt_A\n",
       "Epoch_Time_of_Clock                   \n",
       "2017-11-29 12:00:00 5,153.674316406250\n",
       "2017-11-29 14:00:00 5,153.674804687500\n",
       "2017-11-29 16:00:00 5,153.674804687500\n",
       "2017-11-29 18:00:00 5,153.675781250000\n",
       "2017-11-29 20:00:00 5,153.677734375000\n",
       "2017-11-29 22:00:00 5,153.677246093750"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final generated ouput\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('SA1SqrtAWithoutC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqrt_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-29 12:00:00</th>\n",
       "      <td>5,153.671113970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 14:00:00</th>\n",
       "      <td>5,153.670415880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 16:00:00</th>\n",
       "      <td>5,153.670558930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 18:00:00</th>\n",
       "      <td>5,153.671985630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 20:00:00</th>\n",
       "      <td>5,153.674263000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 22:00:00</th>\n",
       "      <td>5,153.674684520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                sqrt_A\n",
       "Epoch_Time_of_Clock                   \n",
       "2017-11-29 12:00:00 5,153.671113970000\n",
       "2017-11-29 14:00:00 5,153.670415880000\n",
       "2017-11-29 16:00:00 5,153.670558930000\n",
       "2017-11-29 18:00:00 5,153.671985630000\n",
       "2017-11-29 20:00:00 5,153.674263000000\n",
       "2017-11-29 22:00:00 5,153.674684520000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
