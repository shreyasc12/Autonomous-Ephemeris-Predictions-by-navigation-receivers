{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi step model (simple encoder-decoder)\n",
    "\n",
    "In this notebook, we demonstrate how to:\n",
    "- prepare time series data for training a RNN forecasting model\n",
    "- get data in the required shape for the keras API\n",
    "- implement a RNN model in keras to predict the next 3 steps ahead (time *t+1* to *t+3*) in the time series. This model uses a simple encoder decoder approach in which the final hidden state of the encoder is replicated across each time step of the decoder. \n",
    "- enable early stopping to reduce the likelihood of model overfitting\n",
    "- evaluate the model on a test dataset\n",
    "\n",
    "The data in this example is taken from the GEFCom2014 forecasting competition<sup>1</sup>. It consists of 3 years of hourly electricity load and temperature values between 2012 and 2014. The task is to forecast future values of electricity load.\n",
    "\n",
    "<sup>1</sup>Tao Hong, Pierre Pinson, Shu Fan, Hamidreza Zareipour, Alberto Troccoli and Rob J. Hyndman, \"Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond\", International Journal of Forecasting, vol.32, no.3, pp 896-913, July-September, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from collections import UserDict\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "from common.utils import load_data, mape, TimeSeriesTensor, create_evaluation_df\n",
    "\n",
    "pd.options.display.float_format = '{:,.20f}'.format\n",
    "np.set_printoptions(precision=20)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paras = {\n",
    "    'M0':['M0', 'e','Del_n' , 'sqrt_A'],\n",
    "    'e':['OMEGA','i0','omega','I_dot','Cus','Crs','e' , 'M0'],\n",
    "    'sqrt_A':['Cuc','Crc','Del_n','Crs','sqrt_A','OMEGA_dot','Cus'],\n",
    "    'OMEGA':['OMEGA','e','i0','omega'],\n",
    "    'i0':['e','i0','omega','OMEGA' ,'I_dot'],\n",
    "    'omega':['omega','e','OMEGA','i0'],\n",
    "    'I_dot':['I_dot','e','Crs','Cuc'],\n",
    "    'Cic':['M0','Cic'],\n",
    "    'Cis':['Cis'],\n",
    "    'OMEGA_dot':['OMEGA_dot','sqrt_A','Crc','Del_n','Cus' , 'omega' ],\n",
    "    'Cuc':['Cuc','e','sqrt_A','I_dot','Crs'],\n",
    "    'Cus':['Cus','sqrt_A','OMEGA_dot','Crc','Del_n','Cus'],\n",
    "    'Crc':['Crc','sqrt_A','OMEGA_dot','Cus','Del_n'],\n",
    "    'Crs':['Crs','e','sqrt_A','I_dot','Cuc'],\n",
    "    'Del_n':['Crc','sqrt_A','OMEGA_dot','Cus','Del_n'],\n",
    "    'Codes' : ['Codes']    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>Crc</th>\n",
       "      <th>I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-01 14:00:00</th>\n",
       "      <td>0.00709403527435000045</td>\n",
       "      <td>309.37500000000000000000</td>\n",
       "      <td>0.00000000022393789934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 16:00:00</th>\n",
       "      <td>0.00709456193727000042</td>\n",
       "      <td>310.12500000000000000000</td>\n",
       "      <td>0.00000000030465554727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 18:00:00</th>\n",
       "      <td>0.00709454016760000059</td>\n",
       "      <td>308.62500000000000000000</td>\n",
       "      <td>0.00000000029715523485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 20:00:00</th>\n",
       "      <td>0.00709462445228999962</td>\n",
       "      <td>309.37500000000000000000</td>\n",
       "      <td>0.00000000025822504182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 22:00:00</th>\n",
       "      <td>0.00709612155333000025</td>\n",
       "      <td>307.43750000000000000000</td>\n",
       "      <td>0.00000000020000833115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         e                      Crc  \\\n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-01 14:00:00 0.00709403527435000045 309.37500000000000000000   \n",
       "2017-11-01 16:00:00 0.00709456193727000042 310.12500000000000000000   \n",
       "2017-11-01 18:00:00 0.00709454016760000059 308.62500000000000000000   \n",
       "2017-11-01 20:00:00 0.00709462445228999962 309.37500000000000000000   \n",
       "2017-11-01 22:00:00 0.00709612155333000025 307.43750000000000000000   \n",
       "\n",
       "                                     I_dot  \n",
       "Epoch_Time_of_Clock                         \n",
       "2017-11-01 14:00:00 0.00000000022393789934  \n",
       "2017-11-01 16:00:00 0.00000000030465554727  \n",
       "2017-11-01 18:00:00 0.00000000029715523485  \n",
       "2017-11-01 20:00:00 0.00000000025822504182  \n",
       "2017-11-01 22:00:00 0.00000000020000833115  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hourinterpol_21.csv\" , parse_dates = True)\n",
    "a = pd.to_datetime(df['Epoch_Time_of_Clock'])\n",
    "print(type(a[0]))\n",
    "#df = df.drop(['Unnamed: 0', 'Unnamed: 0.1' ,'sqrt_A'  ,'PRN','SV_Clock_Bias', 'SV_Clock_Drift', 'SV_Clock_Drift_Rate', 'IODE', 'Crs',\n",
    "#       'Del_n', 'Cuc','Cus','Toe', 'Cic', \n",
    "#       'Cis', 'Crc', 'M0', 'OMEGA_dot', 'I_dot', 'Codes', 'GPS_week',\n",
    "#       'L2_P_Data_flag', 'SV_accuracy', 'SV_health', 'Tgd', 'IODC', 'T_Tx',\n",
    "#       'Fit_Interval' ,'Epoch_Time_of_Clock' ],axis =1 )\n",
    "df = df.loc[:,Paras[var_name]]\n",
    "#df.head()\n",
    "#df = df.set_index(['Epoch_Time_of_Clock'])\n",
    "df = df.set_index(a)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = 'I_dot'\n",
    "sat_var = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.iloc[5 : , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         e                      Crc  \\\n",
      "Epoch_Time_of_Clock                                                   \n",
      "2017-11-02 12:00:00 0.00709870201535999987 297.03125000000000000000   \n",
      "2017-11-02 14:00:00 0.00709954684135000007 288.65625000000000000000   \n",
      "2017-11-02 16:00:00 0.00709992146584999956 289.53125000000000000000   \n",
      "2017-11-02 18:00:00 0.00710008526221000014 290.93750000000000000000   \n",
      "2017-11-02 20:00:00 0.00710032973439000091 293.37500000000000000000   \n",
      "2017-11-02 22:00:00 0.00710155733395000013 289.59375000000000000000   \n",
      "2017-11-03 12:00:00 0.00710492231883000022 266.28125000000000000000   \n",
      "\n",
      "                                     I_dot  \n",
      "Epoch_Time_of_Clock                         \n",
      "2017-11-02 12:00:00 0.00000000034822879084  \n",
      "2017-11-02 14:00:00 0.00000000041644591807  \n",
      "2017-11-02 16:00:00 0.00000000051287850631  \n",
      "2017-11-02 18:00:00 0.00000000049680640827  \n",
      "2017-11-02 20:00:00 0.00000000045430463790  \n",
      "2017-11-02 22:00:00 0.00000000037501562091  \n",
      "2017-11-03 12:00:00 0.00000000052323608060  \n",
      "Index(['e', 'Crc', 'I_dot'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.head(7))\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter number of entries per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "2017-11-25 12:00:00 2017-11-21 12:00:00\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "entry = 6\n",
    "print(df.shape[0])\n",
    "no_of_entries = df.shape[0]//entry\n",
    "valid = (no_of_entries * 70)//100\n",
    "test = (no_of_entries * 85)//100\n",
    "indexes = df.index\n",
    "#print(valid , test , indexes)\n",
    "valid_start_dt = indexes[int(valid)*int(entry)] \n",
    "test_start_dt = indexes [int(test)*int(entry)] \n",
    "test_start_dt = str(test_start_dt)\n",
    "valid_start_dt = str(valid_start_dt)\n",
    "print(test_start_dt,valid_start_dt)\n",
    "print(type(test_start_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load data into Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter lag and no. of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"total = len(df)\n",
    "t = total*70/100\n",
    "t = round(t)\n",
    "indexes = df.index\n",
    "valid_start_dt = str(indexes[t])\n",
    "t = total*85/100\n",
    "t = round(t)\n",
    "test_start_dt = str(indexes[t])\n",
    "print(valid_start_dt , test_start_dt)\n",
    "\"\"\"\n",
    "T = 6\n",
    "HORIZON = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training set containing only the model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>Crc</th>\n",
       "      <th>I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 12:00:00</th>\n",
       "      <td>0.00709870201535999987</td>\n",
       "      <td>297.03125000000000000000</td>\n",
       "      <td>0.00000000034822879084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 14:00:00</th>\n",
       "      <td>0.00709954684135000007</td>\n",
       "      <td>288.65625000000000000000</td>\n",
       "      <td>0.00000000041644591807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 16:00:00</th>\n",
       "      <td>0.00709992146584999956</td>\n",
       "      <td>289.53125000000000000000</td>\n",
       "      <td>0.00000000051287850631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 18:00:00</th>\n",
       "      <td>0.00710008526221000014</td>\n",
       "      <td>290.93750000000000000000</td>\n",
       "      <td>0.00000000049680640827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 20:00:00</th>\n",
       "      <td>0.00710032973439000091</td>\n",
       "      <td>293.37500000000000000000</td>\n",
       "      <td>0.00000000045430463790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         e                      Crc  \\\n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 12:00:00 0.00709870201535999987 297.03125000000000000000   \n",
       "2017-11-02 14:00:00 0.00709954684135000007 288.65625000000000000000   \n",
       "2017-11-02 16:00:00 0.00709992146584999956 289.53125000000000000000   \n",
       "2017-11-02 18:00:00 0.00710008526221000014 290.93750000000000000000   \n",
       "2017-11-02 20:00:00 0.00710032973439000091 293.37500000000000000000   \n",
       "\n",
       "                                     I_dot  \n",
       "Epoch_Time_of_Clock                         \n",
       "2017-11-02 12:00:00 0.00000000034822879084  \n",
       "2017-11-02 14:00:00 0.00000000041644591807  \n",
       "2017-11-02 16:00:00 0.00000000051287850631  \n",
       "2017-11-02 18:00:00 0.00000000049680640827  \n",
       "2017-11-02 20:00:00 0.00000000045430463790  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.copy()[df.index < valid_start_dt][['e', 'Crc', 'I_dot' ]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>Crc</th>\n",
       "      <th>I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-20 12:00:00</th>\n",
       "      <td>0.00711646664422000037</td>\n",
       "      <td>169.65625000000000000000</td>\n",
       "      <td>0.00000000053395081262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 14:00:00</th>\n",
       "      <td>0.00711591739672999980</td>\n",
       "      <td>168.31250000000000000000</td>\n",
       "      <td>0.00000000055502311894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 16:00:00</th>\n",
       "      <td>0.00711673183832000050</td>\n",
       "      <td>179.53125000000000000000</td>\n",
       "      <td>0.00000000057180953245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 18:00:00</th>\n",
       "      <td>0.00711753428914000015</td>\n",
       "      <td>187.56250000000000000000</td>\n",
       "      <td>0.00000000046609084313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 20:00:00</th>\n",
       "      <td>0.00711681041867000090</td>\n",
       "      <td>188.25000000000000000000</td>\n",
       "      <td>0.00000000043716106666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20 22:00:00</th>\n",
       "      <td>0.00711733335629000010</td>\n",
       "      <td>178.34375000000000000000</td>\n",
       "      <td>0.00000000039108771895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         e                      Crc  \\\n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-20 12:00:00 0.00711646664422000037 169.65625000000000000000   \n",
       "2017-11-20 14:00:00 0.00711591739672999980 168.31250000000000000000   \n",
       "2017-11-20 16:00:00 0.00711673183832000050 179.53125000000000000000   \n",
       "2017-11-20 18:00:00 0.00711753428914000015 187.56250000000000000000   \n",
       "2017-11-20 20:00:00 0.00711681041867000090 188.25000000000000000000   \n",
       "2017-11-20 22:00:00 0.00711733335629000010 178.34375000000000000000   \n",
       "\n",
       "                                     I_dot  \n",
       "Epoch_Time_of_Clock                         \n",
       "2017-11-20 12:00:00 0.00000000053395081262  \n",
       "2017-11-20 14:00:00 0.00000000055502311894  \n",
       "2017-11-20 16:00:00 0.00000000057180953245  \n",
       "2017-11-20 18:00:00 0.00000000046609084313  \n",
       "2017-11-20 20:00:00 0.00000000043716106666  \n",
       "2017-11-20 22:00:00 0.00000000039108771895  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data to be in range (0, 1). This transformation should be calibrated on the training set only. This is to prevent information from the validation or test sets leaking into the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter variable to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "y_scalar = StandardScaler()\n",
    "y_scalar.fit(train[[var_name]])\n",
    "\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "train[['e', 'Crc', 'I_dot']] = X_scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the TimeSeriesTensor convenience class to:\n",
    "1. Shift the values of the time series to create a Pandas dataframe containing all the data for a single training example\n",
    "2. Discard any samples with missing values\n",
    "3. Transform this Pandas dataframe into a numpy array of shape (samples, time steps, features) for input into Keras\n",
    "\n",
    "The class takes the following parameters:\n",
    "\n",
    "- **dataset**: original time series\n",
    "- **H**: the forecast horizon\n",
    "- **tensor_structure**: a dictionary discribing the tensor structure in the form { 'tensor_name' : (range(max_backward_shift, max_forward_shift), [feature, feature, ...] ) }\n",
    "- **freq**: time series frequency\n",
    "- **drop_incomplete**: (Boolean) whether to drop incomplete samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_structure = {'X':(range(-T+1, 1), ['e', 'Crc', 'I_dot'])}\n",
    "train_inputs = TimeSeriesTensor(train, var_name, HORIZON, {'X':(range(-T+1, 1), ['e', 'Crc', 'I_dot'])} ,freq = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"15\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"4\" halign=\"left\">e</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Crc</th>\n",
       "      <th colspan=\"6\" halign=\"left\">I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>...</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 22:00:00</th>\n",
       "      <td>0.47058568812334411469</td>\n",
       "      <td>0.73589273044273850743</td>\n",
       "      <td>1.16262998369912717855</td>\n",
       "      <td>1.12051775475904102919</td>\n",
       "      <td>0.95066509803481602514</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>-2.92617496977098268118</td>\n",
       "      <td>-2.79400608189676713522</td>\n",
       "      <td>-2.73539791391728170922</td>\n",
       "      <td>-2.70977277572862673694</td>\n",
       "      <td>...</td>\n",
       "      <td>1.25202275607788715561</td>\n",
       "      <td>1.28225133017616377273</td>\n",
       "      <td>1.33464752527984376052</td>\n",
       "      <td>1.25336624826003273370</td>\n",
       "      <td>-0.21724738455685779770</td>\n",
       "      <td>0.05086713969423365539</td>\n",
       "      <td>0.42987720015107228066</td>\n",
       "      <td>0.36670885674094411133</td>\n",
       "      <td>0.19966368194448591566</td>\n",
       "      <td>-0.11196681220664428391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 12:00:00</th>\n",
       "      <td>0.73589273044273850743</td>\n",
       "      <td>1.16262998369912717855</td>\n",
       "      <td>1.12051775475904102919</td>\n",
       "      <td>0.95066509803481602514</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>-2.79400608189676713522</td>\n",
       "      <td>-2.73539791391728170922</td>\n",
       "      <td>-2.70977277572862673694</td>\n",
       "      <td>-2.67152630068010399356</td>\n",
       "      <td>...</td>\n",
       "      <td>1.28225133017616377273</td>\n",
       "      <td>1.33464752527984376052</td>\n",
       "      <td>1.25336624826003273370</td>\n",
       "      <td>0.75224366431971045888</td>\n",
       "      <td>0.05086713969423365539</td>\n",
       "      <td>0.42987720015107228066</td>\n",
       "      <td>0.36670885674094411133</td>\n",
       "      <td>0.19966368194448591566</td>\n",
       "      <td>-0.11196681220664428391</td>\n",
       "      <td>0.47058568812334411469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 14:00:00</th>\n",
       "      <td>1.16262998369912717855</td>\n",
       "      <td>1.12051775475904102919</td>\n",
       "      <td>0.95066509803481602514</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>-2.73539791391728170922</td>\n",
       "      <td>-2.70977277572862673694</td>\n",
       "      <td>-2.67152630068010399356</td>\n",
       "      <td>-2.47947436274205035289</td>\n",
       "      <td>...</td>\n",
       "      <td>1.33464752527984376052</td>\n",
       "      <td>1.25336624826003273370</td>\n",
       "      <td>0.75224366431971045888</td>\n",
       "      <td>0.51847602462636976917</td>\n",
       "      <td>0.42987720015107228066</td>\n",
       "      <td>0.36670885674094411133</td>\n",
       "      <td>0.19966368194448591566</td>\n",
       "      <td>-0.11196681220664428391</td>\n",
       "      <td>0.47058568812334411469</td>\n",
       "      <td>0.73589273044273850743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 16:00:00</th>\n",
       "      <td>1.12051775475904102919</td>\n",
       "      <td>0.95066509803481602514</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>-2.70977277572862673694</td>\n",
       "      <td>-2.67152630068010399356</td>\n",
       "      <td>-2.47947436274205035289</td>\n",
       "      <td>-1.95303896117129727195</td>\n",
       "      <td>...</td>\n",
       "      <td>1.25336624826003273370</td>\n",
       "      <td>0.75224366431971045888</td>\n",
       "      <td>0.51847602462636976917</td>\n",
       "      <td>0.50705634107813191136</td>\n",
       "      <td>0.36670885674094411133</td>\n",
       "      <td>0.19966368194448591566</td>\n",
       "      <td>-0.11196681220664428391</td>\n",
       "      <td>0.47058568812334411469</td>\n",
       "      <td>0.73589273044273850743</td>\n",
       "      <td>1.16262998369912717855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 18:00:00</th>\n",
       "      <td>0.95066509803481602514</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>-2.67152630068010399356</td>\n",
       "      <td>-2.47947436274205035289</td>\n",
       "      <td>-1.95303896117129727195</td>\n",
       "      <td>-1.77839827418765583644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75224366431971045888</td>\n",
       "      <td>0.51847602462636976917</td>\n",
       "      <td>0.50705634107813191136</td>\n",
       "      <td>0.56012428227288457716</td>\n",
       "      <td>0.19966368194448591566</td>\n",
       "      <td>-0.11196681220664428391</td>\n",
       "      <td>0.47058568812334411469</td>\n",
       "      <td>0.73589273044273850743</td>\n",
       "      <td>1.16262998369912717855</td>\n",
       "      <td>1.12051775475904102919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 20:00:00</th>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>-2.47947436274205035289</td>\n",
       "      <td>-1.95303896117129727195</td>\n",
       "      <td>-1.77839827418765583644</td>\n",
       "      <td>-1.72634664376002766595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51847602462636976917</td>\n",
       "      <td>0.50705634107813191136</td>\n",
       "      <td>0.56012428227288457716</td>\n",
       "      <td>0.64745127411235081638</td>\n",
       "      <td>-0.11196681220664428391</td>\n",
       "      <td>0.47058568812334411469</td>\n",
       "      <td>0.73589273044273850743</td>\n",
       "      <td>1.16262998369912717855</td>\n",
       "      <td>1.12051775475904102919</td>\n",
       "      <td>0.95066509803481602514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 22:00:00</th>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>-1.95303896117129727195</td>\n",
       "      <td>-1.77839827418765583644</td>\n",
       "      <td>-1.72634664376002766595</td>\n",
       "      <td>-1.69933734814780645905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50705634107813191136</td>\n",
       "      <td>0.56012428227288457716</td>\n",
       "      <td>0.64745127411235081638</td>\n",
       "      <td>0.57087221973004964592</td>\n",
       "      <td>0.47058568812334411469</td>\n",
       "      <td>0.73589273044273850743</td>\n",
       "      <td>1.16262998369912717855</td>\n",
       "      <td>1.12051775475904102919</td>\n",
       "      <td>0.95066509803481602514</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 12:00:00</th>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>-1.77839827418765583644</td>\n",
       "      <td>-1.72634664376002766595</td>\n",
       "      <td>-1.69933734814780645905</td>\n",
       "      <td>-1.63533824623316559332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56012428227288457716</td>\n",
       "      <td>0.64745127411235081638</td>\n",
       "      <td>0.57087221973004964592</td>\n",
       "      <td>-0.07266053551771001207</td>\n",
       "      <td>0.73589273044273850743</td>\n",
       "      <td>1.16262998369912717855</td>\n",
       "      <td>1.12051775475904102919</td>\n",
       "      <td>0.95066509803481602514</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 14:00:00</th>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>-1.72634664376002766595</td>\n",
       "      <td>-1.69933734814780645905</td>\n",
       "      <td>-1.63533824623316559332</td>\n",
       "      <td>-1.50637477769096883584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.64745127411235081638</td>\n",
       "      <td>0.57087221973004964592</td>\n",
       "      <td>-0.07266053551771001207</td>\n",
       "      <td>-0.29030626902530304312</td>\n",
       "      <td>1.16262998369912717855</td>\n",
       "      <td>1.12051775475904102919</td>\n",
       "      <td>0.95066509803481602514</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 16:00:00</th>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>-1.69933734814780645905</td>\n",
       "      <td>-1.63533824623316559332</td>\n",
       "      <td>-1.50637477769096883584</td>\n",
       "      <td>-0.99163187025148802345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.57087221973004964592</td>\n",
       "      <td>-0.07266053551771001207</td>\n",
       "      <td>-0.29030626902530304312</td>\n",
       "      <td>-0.34001547976469159718</td>\n",
       "      <td>1.12051775475904102919</td>\n",
       "      <td>0.95066509803481602514</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 18:00:00</th>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>-1.63533824623316559332</td>\n",
       "      <td>-1.50637477769096883584</td>\n",
       "      <td>-0.99163187025148802345</td>\n",
       "      <td>-0.82061549218194562716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07266053551771001207</td>\n",
       "      <td>-0.29030626902530304312</td>\n",
       "      <td>-0.34001547976469159718</td>\n",
       "      <td>-0.34135897194683723077</td>\n",
       "      <td>0.95066509803481602514</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 20:00:00</th>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>-1.50637477769096883584</td>\n",
       "      <td>-0.99163187025148802345</td>\n",
       "      <td>-0.82061549218194562716</td>\n",
       "      <td>-0.73887731173082793479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.29030626902530304312</td>\n",
       "      <td>-0.34001547976469159718</td>\n",
       "      <td>-0.34135897194683723077</td>\n",
       "      <td>-0.25268848792522524693</td>\n",
       "      <td>0.53796525442905296366</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 22:00:00</th>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>-0.99163187025148802345</td>\n",
       "      <td>-0.82061549218194562716</td>\n",
       "      <td>-0.73887731173082793479</td>\n",
       "      <td>-0.74412254364959273811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.34001547976469159718</td>\n",
       "      <td>-0.34135897194683723077</td>\n",
       "      <td>-0.25268848792522524693</td>\n",
       "      <td>-0.28023007765921076340</td>\n",
       "      <td>0.91837905584977530538</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 12:00:00</th>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>-0.82061549218194562716</td>\n",
       "      <td>-0.73887731173082793479</td>\n",
       "      <td>-0.74412254364959273811</td>\n",
       "      <td>-0.67536995275316269449</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.34135897194683723077</td>\n",
       "      <td>-0.25268848792522524693</td>\n",
       "      <td>-0.28023007765921076340</td>\n",
       "      <td>-0.84718377852466919542</td>\n",
       "      <td>1.14859257405243142536</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 14:00:00</th>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>-0.73887731173082793479</td>\n",
       "      <td>-0.74412254364959273811</td>\n",
       "      <td>-0.67536995275316269449</td>\n",
       "      <td>-0.59774782258829062265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25268848792522524693</td>\n",
       "      <td>-0.28023007765921076340</td>\n",
       "      <td>-0.84718377852466919542</td>\n",
       "      <td>-0.97280029755528607538</td>\n",
       "      <td>1.59498220081104902235</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 16:00:00</th>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>-0.74412254364959273811</td>\n",
       "      <td>-0.67536995275316269449</td>\n",
       "      <td>-0.59774782258829062265</td>\n",
       "      <td>-0.22919750908055919192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.28023007765921076340</td>\n",
       "      <td>-0.84718377852466919542</td>\n",
       "      <td>-0.97280029755528607538</td>\n",
       "      <td>-1.03392919184291254275</td>\n",
       "      <td>1.60059716467051305067</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 18:00:00</th>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>-0.67536995275316269449</td>\n",
       "      <td>-0.59774782258829062265</td>\n",
       "      <td>-0.22919750908055919192</td>\n",
       "      <td>-0.11678930055519548548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.84718377852466919542</td>\n",
       "      <td>-0.97280029755528607538</td>\n",
       "      <td>-1.03392919184291254275</td>\n",
       "      <td>-1.14476729686992761970</td>\n",
       "      <td>1.44057069470133258804</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 20:00:00</th>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>-0.59774782258829062265</td>\n",
       "      <td>-0.22919750908055919192</td>\n",
       "      <td>-0.11678930055519548548</td>\n",
       "      <td>0.03078745392909986237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.97280029755528607538</td>\n",
       "      <td>-1.03392919184291254275</td>\n",
       "      <td>-1.14476729686992761970</td>\n",
       "      <td>-1.10513427749663128985</td>\n",
       "      <td>0.94505013417928218633</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 22:00:00</th>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>-0.22919750908055919192</td>\n",
       "      <td>-0.11678930055519548548</td>\n",
       "      <td>0.03078745392909986237</td>\n",
       "      <td>-0.02026248354428677664</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.03392919184291254275</td>\n",
       "      <td>-1.14476729686992761970</td>\n",
       "      <td>-1.10513427749663128985</td>\n",
       "      <td>-1.07087522685191749439</td>\n",
       "      <td>0.98716236311543736903</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 12:00:00</th>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>0.03682973004753833934</td>\n",
       "      <td>-0.11678930055519548548</td>\n",
       "      <td>0.03078745392909986237</td>\n",
       "      <td>-0.02026248354428677664</td>\n",
       "      <td>0.01840288108604457018</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.14476729686992761970</td>\n",
       "      <td>-1.10513427749663128985</td>\n",
       "      <td>-1.07087522685191749439</td>\n",
       "      <td>-1.35771080774001084990</td>\n",
       "      <td>1.16403372466301013866</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 14:00:00</th>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>0.03682973004753833934</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>0.03078745392909986237</td>\n",
       "      <td>-0.02026248354428677664</td>\n",
       "      <td>0.01840288108604457018</td>\n",
       "      <td>0.08735581088639735037</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.10513427749663128985</td>\n",
       "      <td>-1.07087522685191749439</td>\n",
       "      <td>-1.35771080774001084990</td>\n",
       "      <td>-1.34494763200962719196</td>\n",
       "      <td>1.58796349598770136780</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 16:00:00</th>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>0.03682973004753833934</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>0.45654827847664902762</td>\n",
       "      <td>-0.02026248354428677664</td>\n",
       "      <td>0.01840288108604457018</td>\n",
       "      <td>0.08735581088639735037</td>\n",
       "      <td>0.25296304481097164896</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.07087522685191749439</td>\n",
       "      <td>-1.35771080774001084990</td>\n",
       "      <td>-1.34494763200962719196</td>\n",
       "      <td>-1.37114572956146707483</td>\n",
       "      <td>1.60901961045774410941</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 18:00:00</th>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>0.03682973004753833934</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>0.45654827847664902762</td>\n",
       "      <td>0.27546569403742610094</td>\n",
       "      <td>0.01840288108604457018</td>\n",
       "      <td>0.08735581088639735037</td>\n",
       "      <td>0.25296304481097164896</td>\n",
       "      <td>0.26778810648378881254</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.35771080774001084990</td>\n",
       "      <td>-1.34494763200962719196</td>\n",
       "      <td>-1.37114572956146707483</td>\n",
       "      <td>-1.57670003342974940352</td>\n",
       "      <td>1.47145299592248957055</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 20:00:00</th>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>0.03682973004753833934</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>0.45654827847664902762</td>\n",
       "      <td>0.27546569403742610094</td>\n",
       "      <td>0.17299260361890922422</td>\n",
       "      <td>0.08735581088639735037</td>\n",
       "      <td>0.25296304481097164896</td>\n",
       "      <td>0.26778810648378881254</td>\n",
       "      <td>0.49464433768609061826</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.34494763200962719196</td>\n",
       "      <td>-1.37114572956146707483</td>\n",
       "      <td>-1.57670003342974940352</td>\n",
       "      <td>-1.61767654498519131145</td>\n",
       "      <td>0.93522394742423720082</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 22:00:00</th>\n",
       "      <td>0.03682973004753833934</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>0.45654827847664902762</td>\n",
       "      <td>0.27546569403742610094</td>\n",
       "      <td>0.17299260361890922422</td>\n",
       "      <td>-0.30708680629256251970</td>\n",
       "      <td>0.25296304481097164896</td>\n",
       "      <td>0.26778810648378881254</td>\n",
       "      <td>0.49464433768609061826</td>\n",
       "      <td>0.41673080505272314111</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.37114572956146707483</td>\n",
       "      <td>-1.57670003342974940352</td>\n",
       "      <td>-1.61767654498519131145</td>\n",
       "      <td>-1.55318892024220089887</td>\n",
       "      <td>0.65728323642203179134</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 12:00:00</th>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>0.45654827847664902762</td>\n",
       "      <td>0.27546569403742610094</td>\n",
       "      <td>0.17299260361890922422</td>\n",
       "      <td>-0.30708680629256251970</td>\n",
       "      <td>-0.68469312578551777548</td>\n",
       "      <td>0.26778810648378881254</td>\n",
       "      <td>0.49464433768609061826</td>\n",
       "      <td>0.41673080505272314111</td>\n",
       "      <td>0.39835428469199501977</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.57670003342974940352</td>\n",
       "      <td>-1.61767654498519131145</td>\n",
       "      <td>-1.55318892024220089887</td>\n",
       "      <td>-1.47325113540453545014</td>\n",
       "      <td>0.78361992324228824103</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>0.03682973004753833934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 14:00:00</th>\n",
       "      <td>0.45654827847664902762</td>\n",
       "      <td>0.27546569403742610094</td>\n",
       "      <td>0.17299260361890922422</td>\n",
       "      <td>-0.30708680629256251970</td>\n",
       "      <td>-0.68469312578551777548</td>\n",
       "      <td>-0.59625744501369659645</td>\n",
       "      <td>0.49464433768609061826</td>\n",
       "      <td>0.41673080505272314111</td>\n",
       "      <td>0.39835428469199501977</td>\n",
       "      <td>0.50655538249538389906</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.61767654498519131145</td>\n",
       "      <td>-1.55318892024220089887</td>\n",
       "      <td>-1.47325113540453545014</td>\n",
       "      <td>-1.32748223364173401073</td>\n",
       "      <td>1.15561127887577907991</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>0.03682973004753833934</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 16:00:00</th>\n",
       "      <td>0.27546569403742610094</td>\n",
       "      <td>0.17299260361890922422</td>\n",
       "      <td>-0.30708680629256251970</td>\n",
       "      <td>-0.68469312578551777548</td>\n",
       "      <td>-0.59625744501369659645</td>\n",
       "      <td>-0.28462695085863609634</td>\n",
       "      <td>0.41673080505272314111</td>\n",
       "      <td>0.39835428469199501977</td>\n",
       "      <td>0.50655538249538389906</td>\n",
       "      <td>0.51622627581428048860</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.55318892024220089887</td>\n",
       "      <td>-1.47325113540453545014</td>\n",
       "      <td>-1.32748223364173401073</td>\n",
       "      <td>-1.27978826117556399034</td>\n",
       "      <td>1.11349904993569381872</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>0.03682973004753833934</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>0.45654827847664902762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 18:00:00</th>\n",
       "      <td>0.17299260361890922422</td>\n",
       "      <td>-0.30708680629256251970</td>\n",
       "      <td>-0.68469312578551777548</td>\n",
       "      <td>-0.59625744501369659645</td>\n",
       "      <td>-0.28462695085863609634</td>\n",
       "      <td>-0.62292852333927317687</td>\n",
       "      <td>0.39835428469199501977</td>\n",
       "      <td>0.50655538249538389906</td>\n",
       "      <td>0.51622627581428048860</td>\n",
       "      <td>0.42709377864782482881</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.47325113540453545014</td>\n",
       "      <td>-1.32748223364173401073</td>\n",
       "      <td>-1.27978826117556399034</td>\n",
       "      <td>-1.50146447122959392217</td>\n",
       "      <td>0.99698854987048202148</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>0.03682973004753833934</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>0.45654827847664902762</td>\n",
       "      <td>0.27546569403742610094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 20:00:00</th>\n",
       "      <td>-0.30708680629256251970</td>\n",
       "      <td>-0.68469312578551777548</td>\n",
       "      <td>-0.59625744501369659645</td>\n",
       "      <td>-0.28462695085863609634</td>\n",
       "      <td>-0.62292852333927317687</td>\n",
       "      <td>-0.71978664990225571518</td>\n",
       "      <td>0.50655538249538389906</td>\n",
       "      <td>0.51622627581428048860</td>\n",
       "      <td>0.42709377864782482881</td>\n",
       "      <td>0.71315919454097298491</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.32748223364173401073</td>\n",
       "      <td>-1.27978826117556399034</td>\n",
       "      <td>-1.50146447122959392217</td>\n",
       "      <td>-1.60961559189231762090</td>\n",
       "      <td>0.47339317005504100155</td>\n",
       "      <td>0.03682973004753833934</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>0.45654827847664902762</td>\n",
       "      <td>0.27546569403742610094</td>\n",
       "      <td>0.17299260361890922422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 12:00:00</th>\n",
       "      <td>0.05086713969423365539</td>\n",
       "      <td>0.48321935680615585307</td>\n",
       "      <td>0.30775173622639651061</td>\n",
       "      <td>0.26985073017796129546</td>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.57246680675461392163</td>\n",
       "      <td>0.46566807971331197757</td>\n",
       "      <td>0.45284640394979230882</td>\n",
       "      <td>0.40527507493839998176</td>\n",
       "      <td>...</td>\n",
       "      <td>1.24866402562252298836</td>\n",
       "      <td>1.19089386179026068824</td>\n",
       "      <td>1.18955036960811488811</td>\n",
       "      <td>0.97996558919339582516</td>\n",
       "      <td>-0.52466665581633731286</td>\n",
       "      <td>-0.00247501696477986192</td>\n",
       "      <td>-0.18496134236788674965</td>\n",
       "      <td>-0.22426608938020536899</td>\n",
       "      <td>-0.58923874019034905292</td>\n",
       "      <td>0.01156239268191504473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 14:00:00</th>\n",
       "      <td>0.48321935680615585307</td>\n",
       "      <td>0.30775173622639651061</td>\n",
       "      <td>0.26985073017796129546</td>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.46566807971331197757</td>\n",
       "      <td>0.45284640394979230882</td>\n",
       "      <td>0.40527507493839998176</td>\n",
       "      <td>0.41707684440847908602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.19089386179026068824</td>\n",
       "      <td>1.18955036960811488811</td>\n",
       "      <td>0.97996558919339582516</td>\n",
       "      <td>0.99541574928807052824</td>\n",
       "      <td>-0.00247501696477986192</td>\n",
       "      <td>-0.18496134236788674965</td>\n",
       "      <td>-0.22426608938020536899</td>\n",
       "      <td>-0.58923874019034905292</td>\n",
       "      <td>0.01156239268191504473</td>\n",
       "      <td>0.05086713969423365539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 16:00:00</th>\n",
       "      <td>0.30775173622639651061</td>\n",
       "      <td>0.26985073017796129546</td>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.45284640394979230882</td>\n",
       "      <td>0.40527507493839998176</td>\n",
       "      <td>0.41707684440847908602</td>\n",
       "      <td>0.13906140039398354191</td>\n",
       "      <td>...</td>\n",
       "      <td>1.18955036960811488811</td>\n",
       "      <td>0.97996558919339582516</td>\n",
       "      <td>0.99541574928807052824</td>\n",
       "      <td>0.89062335908071099677</td>\n",
       "      <td>-0.18496134236788674965</td>\n",
       "      <td>-0.22426608938020536899</td>\n",
       "      <td>-0.58923874019034905292</td>\n",
       "      <td>0.01156239268191504473</td>\n",
       "      <td>0.05086713969423365539</td>\n",
       "      <td>0.48321935680615585307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 18:00:00</th>\n",
       "      <td>0.26985073017796129546</td>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.40527507493839998176</td>\n",
       "      <td>0.41707684440847908602</td>\n",
       "      <td>0.13906140039398354191</td>\n",
       "      <td>0.11345447554412074753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.97996558919339582516</td>\n",
       "      <td>0.99541574928807052824</td>\n",
       "      <td>0.89062335908071099677</td>\n",
       "      <td>0.84897510143419629980</td>\n",
       "      <td>-0.22426608938020536899</td>\n",
       "      <td>-0.58923874019034905292</td>\n",
       "      <td>0.01156239268191504473</td>\n",
       "      <td>0.05086713969423365539</td>\n",
       "      <td>0.48321935680615585307</td>\n",
       "      <td>0.30775173622639651061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 20:00:00</th>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.41707684440847908602</td>\n",
       "      <td>0.13906140039398354191</td>\n",
       "      <td>0.11345447554412074753</td>\n",
       "      <td>0.01537958731915608362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.99541574928807052824</td>\n",
       "      <td>0.89062335908071099677</td>\n",
       "      <td>0.84897510143419629980</td>\n",
       "      <td>0.75694588695722020422</td>\n",
       "      <td>-0.58923874019034905292</td>\n",
       "      <td>0.01156239268191504473</td>\n",
       "      <td>0.05086713969423365539</td>\n",
       "      <td>0.48321935680615585307</td>\n",
       "      <td>0.30775173622639651061</td>\n",
       "      <td>0.26985073017796129546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 22:00:00</th>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.13906140039398354191</td>\n",
       "      <td>0.11345447554412074753</td>\n",
       "      <td>0.01537958731915608362</td>\n",
       "      <td>0.00091877834102239194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.89062335908071099677</td>\n",
       "      <td>0.84897510143419629980</td>\n",
       "      <td>0.75694588695722020422</td>\n",
       "      <td>0.74283921904469096820</td>\n",
       "      <td>0.01156239268191504473</td>\n",
       "      <td>0.05086713969423365539</td>\n",
       "      <td>0.48321935680615585307</td>\n",
       "      <td>0.30775173622639651061</td>\n",
       "      <td>0.26985073017796129546</td>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 12:00:00</th>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.11345447554412074753</td>\n",
       "      <td>0.01537958731915608362</td>\n",
       "      <td>0.00091877834102239194</td>\n",
       "      <td>-0.08078297699720485281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84897510143419629980</td>\n",
       "      <td>0.75694588695722020422</td>\n",
       "      <td>0.74283921904469096820</td>\n",
       "      <td>0.46607982952269000343</td>\n",
       "      <td>0.05086713969423365539</td>\n",
       "      <td>0.48321935680615585307</td>\n",
       "      <td>0.30775173622639651061</td>\n",
       "      <td>0.26985073017796129546</td>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 14:00:00</th>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>0.01537958731915608362</td>\n",
       "      <td>0.00091877834102239194</td>\n",
       "      <td>-0.08078297699720485281</td>\n",
       "      <td>-0.04881985193498573905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75694588695722020422</td>\n",
       "      <td>0.74283921904469096820</td>\n",
       "      <td>0.46607982952269000343</td>\n",
       "      <td>0.49429316534774836445</td>\n",
       "      <td>0.48321935680615585307</td>\n",
       "      <td>0.30775173622639651061</td>\n",
       "      <td>0.26985073017796129546</td>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 16:00:00</th>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.00091877834102239194</td>\n",
       "      <td>-0.08078297699720485281</td>\n",
       "      <td>-0.04881985193498573905</td>\n",
       "      <td>-0.28928089961274144892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.74283921904469096820</td>\n",
       "      <td>0.46607982952269000343</td>\n",
       "      <td>0.49429316534774836445</td>\n",
       "      <td>0.40898141178150049235</td>\n",
       "      <td>0.30775173622639651061</td>\n",
       "      <td>0.26985073017796129546</td>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 18:00:00</th>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>-0.08078297699720485281</td>\n",
       "      <td>-0.04881985193498573905</td>\n",
       "      <td>-0.28928089961274144892</td>\n",
       "      <td>-0.32250069498507860644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46607982952269000343</td>\n",
       "      <td>0.49429316534774836445</td>\n",
       "      <td>0.40898141178150049235</td>\n",
       "      <td>0.36531791586176731723</td>\n",
       "      <td>0.26985073017796129546</td>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 20:00:00</th>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>-0.04881985193498573905</td>\n",
       "      <td>-0.28928089961274144892</td>\n",
       "      <td>-0.32250069498507860644</td>\n",
       "      <td>-0.37983398088034736606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49429316534774836445</td>\n",
       "      <td>0.40898141178150049235</td>\n",
       "      <td>0.36531791586176731723</td>\n",
       "      <td>0.25313631865260671772</td>\n",
       "      <td>-0.02774235432647321684</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 22:00:00</th>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>-0.28928089961274144892</td>\n",
       "      <td>-0.32250069498507860644</td>\n",
       "      <td>-0.37983398088034736606</td>\n",
       "      <td>-0.37770310629331527164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40898141178150049235</td>\n",
       "      <td>0.36531791586176731723</td>\n",
       "      <td>0.25313631865260671772</td>\n",
       "      <td>0.19402266263819872849</td>\n",
       "      <td>0.51410165802731322415</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 12:00:00</th>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>-0.32250069498507860644</td>\n",
       "      <td>-0.37983398088034736606</td>\n",
       "      <td>-0.37770310629331527164</td>\n",
       "      <td>-0.49368098684551747768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.36531791586176731723</td>\n",
       "      <td>0.25313631865260671772</td>\n",
       "      <td>0.19402266263819872849</td>\n",
       "      <td>-0.12505673062138980556</td>\n",
       "      <td>0.54779144118016787068</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 14:00:00</th>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>-0.37983398088034736606</td>\n",
       "      <td>-0.37770310629331527164</td>\n",
       "      <td>-0.49368098684551747768</td>\n",
       "      <td>-0.42915372000985402279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25313631865260671772</td>\n",
       "      <td>0.19402266263819872849</td>\n",
       "      <td>-0.12505673062138980556</td>\n",
       "      <td>-0.08609545733916637578</td>\n",
       "      <td>0.86503689919076121040</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 16:00:00</th>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>-0.37770310629331527164</td>\n",
       "      <td>-0.49368098684551747768</td>\n",
       "      <td>-0.42915372000985402279</td>\n",
       "      <td>-0.56574827084875922711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19402266263819872849</td>\n",
       "      <td>-0.12505673062138980556</td>\n",
       "      <td>-0.08609545733916637578</td>\n",
       "      <td>-0.10624784007135092134</td>\n",
       "      <td>0.65447575449426476091</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 18:00:00</th>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>-0.49368098684551747768</td>\n",
       "      <td>-0.42915372000985402279</td>\n",
       "      <td>-0.56574827084875922711</td>\n",
       "      <td>-0.61807309040649816012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.12505673062138980556</td>\n",
       "      <td>-0.08609545733916637578</td>\n",
       "      <td>-0.10624784007135092134</td>\n",
       "      <td>-0.11632403143744318719</td>\n",
       "      <td>0.60815230265859876457</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 20:00:00</th>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>-0.42915372000985402279</td>\n",
       "      <td>-0.56574827084875922711</td>\n",
       "      <td>-0.61807309040649816012</td>\n",
       "      <td>-0.61639753051443624265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08609545733916637578</td>\n",
       "      <td>-0.10624784007135092134</td>\n",
       "      <td>-0.11632403143744318719</td>\n",
       "      <td>-0.22581864428231251951</td>\n",
       "      <td>0.38355374831540633984</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 22:00:00</th>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>-0.56574827084875922711</td>\n",
       "      <td>-0.61807309040649816012</td>\n",
       "      <td>-0.61639753051443624265</td>\n",
       "      <td>-0.58352377606241256913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.10624784007135092134</td>\n",
       "      <td>-0.11632403143744318719</td>\n",
       "      <td>-0.22581864428231251951</td>\n",
       "      <td>-0.34807643285756539875</td>\n",
       "      <td>0.84678826665241602178</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 12:00:00</th>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>-0.61807309040649816012</td>\n",
       "      <td>-0.61639753051443624265</td>\n",
       "      <td>-0.58352377606241256913</td>\n",
       "      <td>-0.72447113964009046683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.11632403143744318719</td>\n",
       "      <td>-0.22581864428231251951</td>\n",
       "      <td>-0.34807643285756539875</td>\n",
       "      <td>-0.68193424012075598561</td>\n",
       "      <td>0.88468927269692076987</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 14:00:00</th>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>0.90293790523919681412</td>\n",
       "      <td>-0.61639753051443624265</td>\n",
       "      <td>-0.58352377606241256913</td>\n",
       "      <td>-0.72447113964009046683</td>\n",
       "      <td>-0.63069442726636659735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.22581864428231251951</td>\n",
       "      <td>-0.34807643285756539875</td>\n",
       "      <td>-0.68193424012075598561</td>\n",
       "      <td>-0.64633169729389661207</td>\n",
       "      <td>1.08963545353788426873</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 16:00:00</th>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>0.90293790523919681412</td>\n",
       "      <td>0.97312495346874228197</td>\n",
       "      <td>-0.58352377606241256913</td>\n",
       "      <td>-0.72447113964009046683</td>\n",
       "      <td>-0.63069442726636659735</td>\n",
       "      <td>-0.62756185972482148028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.34807643285756539875</td>\n",
       "      <td>-0.68193424012075598561</td>\n",
       "      <td>-0.64633169729389661207</td>\n",
       "      <td>-0.57311137336695938682</td>\n",
       "      <td>0.81450222446344522353</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 18:00:00</th>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>0.90293790523919681412</td>\n",
       "      <td>0.97312495346874228197</td>\n",
       "      <td>0.58007748336913866893</td>\n",
       "      <td>-0.72447113964009046683</td>\n",
       "      <td>-0.63069442726636659735</td>\n",
       "      <td>-0.62756185972482148028</td>\n",
       "      <td>-0.70104972782894203753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.68193424012075598561</td>\n",
       "      <td>-0.64633169729389661207</td>\n",
       "      <td>-0.57311137336695938682</td>\n",
       "      <td>-0.52407390871864367732</td>\n",
       "      <td>0.75273762202113092545</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 20:00:00</th>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>0.90293790523919681412</td>\n",
       "      <td>0.97312495346874228197</td>\n",
       "      <td>0.58007748336913866893</td>\n",
       "      <td>0.47900813391057522894</td>\n",
       "      <td>-0.63069442726636659735</td>\n",
       "      <td>-0.62756185972482148028</td>\n",
       "      <td>-0.70104972782894203753</td>\n",
       "      <td>-0.63914507695259314968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.64633169729389661207</td>\n",
       "      <td>-0.57311137336695938682</td>\n",
       "      <td>-0.52407390871864367732</td>\n",
       "      <td>-0.60871391619381876037</td>\n",
       "      <td>0.58569244722467206365</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18 22:00:00</th>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>0.90293790523919681412</td>\n",
       "      <td>0.97312495346874228197</td>\n",
       "      <td>0.58007748336913866893</td>\n",
       "      <td>0.47900813391057522894</td>\n",
       "      <td>0.33442281455197325757</td>\n",
       "      <td>-0.62756185972482148028</td>\n",
       "      <td>-0.70104972782894203753</td>\n",
       "      <td>-0.63914507695259314968</td>\n",
       "      <td>-0.56960934377939997919</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.57311137336695938682</td>\n",
       "      <td>-0.52407390871864367732</td>\n",
       "      <td>-0.60871391619381876037</td>\n",
       "      <td>-0.79008536078347957332</td>\n",
       "      <td>0.95908754382597771748</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 12:00:00</th>\n",
       "      <td>0.90293790523919681412</td>\n",
       "      <td>0.97312495346874228197</td>\n",
       "      <td>0.58007748336913866893</td>\n",
       "      <td>0.47900813391057522894</td>\n",
       "      <td>0.33442281455197325757</td>\n",
       "      <td>0.51269791706342981996</td>\n",
       "      <td>-0.70104972782894203753</td>\n",
       "      <td>-0.63914507695259314968</td>\n",
       "      <td>-0.56960934377939997919</td>\n",
       "      <td>-0.71928054773756555651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.52407390871864367732</td>\n",
       "      <td>-0.60871391619381876037</td>\n",
       "      <td>-0.79008536078347957332</td>\n",
       "      <td>-1.10580602358770407889</td>\n",
       "      <td>1.00821847758547988860</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 14:00:00</th>\n",
       "      <td>0.97312495346874228197</td>\n",
       "      <td>0.58007748336913866893</td>\n",
       "      <td>0.47900813391057522894</td>\n",
       "      <td>0.33442281455197325757</td>\n",
       "      <td>0.51269791706342981996</td>\n",
       "      <td>0.59551863397971716019</td>\n",
       "      <td>-0.63914507695259314968</td>\n",
       "      <td>-0.56960934377939997919</td>\n",
       "      <td>-0.71928054773756555651</td>\n",
       "      <td>-0.60949495360695882251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.60871391619381876037</td>\n",
       "      <td>-0.79008536078347957332</td>\n",
       "      <td>-1.10580602358770407889</td>\n",
       "      <td>-1.09304284785732064300</td>\n",
       "      <td>1.12472897765462209740</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>0.90293790523919681412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 16:00:00</th>\n",
       "      <td>0.58007748336913866893</td>\n",
       "      <td>0.47900813391057522894</td>\n",
       "      <td>0.33442281455197325757</td>\n",
       "      <td>0.51269791706342981996</td>\n",
       "      <td>0.59551863397971716019</td>\n",
       "      <td>0.66149445931761208239</td>\n",
       "      <td>-0.56960934377939997919</td>\n",
       "      <td>-0.71928054773756555651</td>\n",
       "      <td>-0.60949495360695882251</td>\n",
       "      <td>-0.47084237684251628586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.79008536078347957332</td>\n",
       "      <td>-1.10580602358770407889</td>\n",
       "      <td>-1.09304284785732064300</td>\n",
       "      <td>-0.92376283290697036588</td>\n",
       "      <td>0.78221618227840450377</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>0.90293790523919681412</td>\n",
       "      <td>0.97312495346874228197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 18:00:00</th>\n",
       "      <td>0.47900813391057522894</td>\n",
       "      <td>0.33442281455197325757</td>\n",
       "      <td>0.51269791706342981996</td>\n",
       "      <td>0.59551863397971716019</td>\n",
       "      <td>0.66149445931761208239</td>\n",
       "      <td>0.24598713378015207853</td>\n",
       "      <td>-0.71928054773756555651</td>\n",
       "      <td>-0.60949495360695882251</td>\n",
       "      <td>-0.47084237684251628586</td>\n",
       "      <td>-0.55788042609965926566</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.10580602358770407889</td>\n",
       "      <td>-1.09304284785732064300</td>\n",
       "      <td>-0.92376283290697036588</td>\n",
       "      <td>-0.80822250524244576564</td>\n",
       "      <td>0.70220294729381438348</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>0.90293790523919681412</td>\n",
       "      <td>0.97312495346874228197</td>\n",
       "      <td>0.58007748336913866893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 20:00:00</th>\n",
       "      <td>0.33442281455197325757</td>\n",
       "      <td>0.51269791706342981996</td>\n",
       "      <td>0.59551863397971716019</td>\n",
       "      <td>0.66149445931761208239</td>\n",
       "      <td>0.24598713378015207853</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>-0.60949495360695882251</td>\n",
       "      <td>-0.47084237684251628586</td>\n",
       "      <td>-0.55788042609965926566</td>\n",
       "      <td>-0.45044425723410597140</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.09304284785732064300</td>\n",
       "      <td>-0.92376283290697036588</td>\n",
       "      <td>-0.80822250524244576564</td>\n",
       "      <td>-0.84785552461574198446</td>\n",
       "      <td>0.56182885082686295775</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>0.90293790523919681412</td>\n",
       "      <td>0.97312495346874228197</td>\n",
       "      <td>0.58007748336913866893</td>\n",
       "      <td>0.47900813391057522894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19 22:00:00</th>\n",
       "      <td>0.51269791706342981996</td>\n",
       "      <td>0.59551863397971716019</td>\n",
       "      <td>0.66149445931761208239</td>\n",
       "      <td>0.24598713378015207853</td>\n",
       "      <td>0.13228411564270725620</td>\n",
       "      <td>-0.04879846879651587865</td>\n",
       "      <td>-0.47084237684251628586</td>\n",
       "      <td>-0.55788042609965926566</td>\n",
       "      <td>-0.45044425723410597140</td>\n",
       "      <td>-0.34748839134934617068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.92376283290697036588</td>\n",
       "      <td>-0.80822250524244576564</td>\n",
       "      <td>-0.84785552461574198446</td>\n",
       "      <td>-1.06415776594118938192</td>\n",
       "      <td>0.83836582086518485202</td>\n",
       "      <td>0.90293790523919681412</td>\n",
       "      <td>0.97312495346874228197</td>\n",
       "      <td>0.58007748336913866893</td>\n",
       "      <td>0.47900813391057522894</td>\n",
       "      <td>0.33442281455197325757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                               target                          \\\n",
       "feature                                   y                           \n",
       "time step                               t+1                     t+2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  0.47058568812334411469  0.73589273044273850743   \n",
       "2017-11-03 12:00:00  0.73589273044273850743  1.16262998369912717855   \n",
       "2017-11-03 14:00:00  1.16262998369912717855  1.12051775475904102919   \n",
       "2017-11-03 16:00:00  1.12051775475904102919  0.95066509803481602514   \n",
       "2017-11-03 18:00:00  0.95066509803481602514  0.53796525442905296366   \n",
       "2017-11-03 20:00:00  0.53796525442905296366  0.91837905584977530538   \n",
       "2017-11-03 22:00:00  0.91837905584977530538  1.14859257405243142536   \n",
       "2017-11-04 12:00:00  1.14859257405243142536  1.59498220081104902235   \n",
       "2017-11-04 14:00:00  1.59498220081104902235  1.60059716467051305067   \n",
       "2017-11-04 16:00:00  1.60059716467051305067  1.44057069470133258804   \n",
       "2017-11-04 18:00:00  1.44057069470133258804  0.94505013417928218633   \n",
       "2017-11-04 20:00:00  0.94505013417928218633  0.98716236311543736903   \n",
       "2017-11-04 22:00:00  0.98716236311543736903  1.16403372466301013866   \n",
       "2017-11-05 12:00:00  1.16403372466301013866  1.58796349598770136780   \n",
       "2017-11-05 14:00:00  1.58796349598770136780  1.60901961045774410941   \n",
       "2017-11-05 16:00:00  1.60901961045774410941  1.47145299592248957055   \n",
       "2017-11-05 18:00:00  1.47145299592248957055  0.93522394742423720082   \n",
       "2017-11-05 20:00:00  0.93522394742423720082  0.65728323642203179134   \n",
       "2017-11-05 22:00:00  0.65728323642203179134  0.78361992324228824103   \n",
       "2017-11-06 12:00:00  0.78361992324228824103  1.15561127887577907991   \n",
       "2017-11-06 14:00:00  1.15561127887577907991  1.11349904993569381872   \n",
       "2017-11-06 16:00:00  1.11349904993569381872  0.99698854987048202148   \n",
       "2017-11-06 18:00:00  0.99698854987048202148  0.47339317005504100155   \n",
       "2017-11-06 20:00:00  0.47339317005504100155  0.03682973004753833934   \n",
       "2017-11-06 22:00:00  0.03682973004753833934  0.13228411564270725620   \n",
       "2017-11-07 12:00:00  0.13228411564270725620  0.45654827847664902762   \n",
       "2017-11-07 14:00:00  0.45654827847664902762  0.27546569403742610094   \n",
       "2017-11-07 16:00:00  0.27546569403742610094  0.17299260361890922422   \n",
       "2017-11-07 18:00:00  0.17299260361890922422 -0.30708680629256251970   \n",
       "2017-11-07 20:00:00 -0.30708680629256251970 -0.68469312578551777548   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  0.05086713969423365539  0.48321935680615585307   \n",
       "2017-11-15 14:00:00  0.48321935680615585307  0.30775173622639651061   \n",
       "2017-11-15 16:00:00  0.30775173622639651061  0.26985073017796129546   \n",
       "2017-11-15 18:00:00  0.26985073017796129546 -0.02774235432647321684   \n",
       "2017-11-15 20:00:00 -0.02774235432647321684  0.51410165802731322415   \n",
       "2017-11-15 22:00:00  0.51410165802731322415  0.54779144118016787068   \n",
       "2017-11-16 12:00:00  0.54779144118016787068  0.86503689919076121040   \n",
       "2017-11-16 14:00:00  0.86503689919076121040  0.65447575449426476091   \n",
       "2017-11-16 16:00:00  0.65447575449426476091  0.60815230265859876457   \n",
       "2017-11-16 18:00:00  0.60815230265859876457  0.38355374831540633984   \n",
       "2017-11-16 20:00:00  0.38355374831540633984  0.84678826665241602178   \n",
       "2017-11-16 22:00:00  0.84678826665241602178  0.88468927269692076987   \n",
       "2017-11-17 12:00:00  0.88468927269692076987  1.08963545353788426873   \n",
       "2017-11-17 14:00:00  1.08963545353788426873  0.81450222446344522353   \n",
       "2017-11-17 16:00:00  0.81450222446344522353  0.75273762202113092545   \n",
       "2017-11-17 18:00:00  0.75273762202113092545  0.58569244722467206365   \n",
       "2017-11-17 20:00:00  0.58569244722467206365  0.95908754382597771748   \n",
       "2017-11-17 22:00:00  0.95908754382597771748  1.00821847758547988860   \n",
       "2017-11-18 12:00:00  1.00821847758547988860  1.12472897765462209740   \n",
       "2017-11-18 14:00:00  1.12472897765462209740  0.78221618227840450377   \n",
       "2017-11-18 16:00:00  0.78221618227840450377  0.70220294729381438348   \n",
       "2017-11-18 18:00:00  0.70220294729381438348  0.56182885082686295775   \n",
       "2017-11-18 20:00:00  0.56182885082686295775  0.83836582086518485202   \n",
       "2017-11-18 22:00:00  0.83836582086518485202  0.90293790523919681412   \n",
       "2017-11-19 12:00:00  0.90293790523919681412  0.97312495346874228197   \n",
       "2017-11-19 14:00:00  0.97312495346874228197  0.58007748336913866893   \n",
       "2017-11-19 16:00:00  0.58007748336913866893  0.47900813391057522894   \n",
       "2017-11-19 18:00:00  0.47900813391057522894  0.33442281455197325757   \n",
       "2017-11-19 20:00:00  0.33442281455197325757  0.51269791706342981996   \n",
       "2017-11-19 22:00:00  0.51269791706342981996  0.59551863397971716019   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+3                     t+4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  1.16262998369912717855  1.12051775475904102919   \n",
       "2017-11-03 12:00:00  1.12051775475904102919  0.95066509803481602514   \n",
       "2017-11-03 14:00:00  0.95066509803481602514  0.53796525442905296366   \n",
       "2017-11-03 16:00:00  0.53796525442905296366  0.91837905584977530538   \n",
       "2017-11-03 18:00:00  0.91837905584977530538  1.14859257405243142536   \n",
       "2017-11-03 20:00:00  1.14859257405243142536  1.59498220081104902235   \n",
       "2017-11-03 22:00:00  1.59498220081104902235  1.60059716467051305067   \n",
       "2017-11-04 12:00:00  1.60059716467051305067  1.44057069470133258804   \n",
       "2017-11-04 14:00:00  1.44057069470133258804  0.94505013417928218633   \n",
       "2017-11-04 16:00:00  0.94505013417928218633  0.98716236311543736903   \n",
       "2017-11-04 18:00:00  0.98716236311543736903  1.16403372466301013866   \n",
       "2017-11-04 20:00:00  1.16403372466301013866  1.58796349598770136780   \n",
       "2017-11-04 22:00:00  1.58796349598770136780  1.60901961045774410941   \n",
       "2017-11-05 12:00:00  1.60901961045774410941  1.47145299592248957055   \n",
       "2017-11-05 14:00:00  1.47145299592248957055  0.93522394742423720082   \n",
       "2017-11-05 16:00:00  0.93522394742423720082  0.65728323642203179134   \n",
       "2017-11-05 18:00:00  0.65728323642203179134  0.78361992324228824103   \n",
       "2017-11-05 20:00:00  0.78361992324228824103  1.15561127887577907991   \n",
       "2017-11-05 22:00:00  1.15561127887577907991  1.11349904993569381872   \n",
       "2017-11-06 12:00:00  1.11349904993569381872  0.99698854987048202148   \n",
       "2017-11-06 14:00:00  0.99698854987048202148  0.47339317005504100155   \n",
       "2017-11-06 16:00:00  0.47339317005504100155  0.03682973004753833934   \n",
       "2017-11-06 18:00:00  0.03682973004753833934  0.13228411564270725620   \n",
       "2017-11-06 20:00:00  0.13228411564270725620  0.45654827847664902762   \n",
       "2017-11-06 22:00:00  0.45654827847664902762  0.27546569403742610094   \n",
       "2017-11-07 12:00:00  0.27546569403742610094  0.17299260361890922422   \n",
       "2017-11-07 14:00:00  0.17299260361890922422 -0.30708680629256251970   \n",
       "2017-11-07 16:00:00 -0.30708680629256251970 -0.68469312578551777548   \n",
       "2017-11-07 18:00:00 -0.68469312578551777548 -0.59625744501369659645   \n",
       "2017-11-07 20:00:00 -0.59625744501369659645 -0.28462695085863609634   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  0.30775173622639651061  0.26985073017796129546   \n",
       "2017-11-15 14:00:00  0.26985073017796129546 -0.02774235432647321684   \n",
       "2017-11-15 16:00:00 -0.02774235432647321684  0.51410165802731322415   \n",
       "2017-11-15 18:00:00  0.51410165802731322415  0.54779144118016787068   \n",
       "2017-11-15 20:00:00  0.54779144118016787068  0.86503689919076121040   \n",
       "2017-11-15 22:00:00  0.86503689919076121040  0.65447575449426476091   \n",
       "2017-11-16 12:00:00  0.65447575449426476091  0.60815230265859876457   \n",
       "2017-11-16 14:00:00  0.60815230265859876457  0.38355374831540633984   \n",
       "2017-11-16 16:00:00  0.38355374831540633984  0.84678826665241602178   \n",
       "2017-11-16 18:00:00  0.84678826665241602178  0.88468927269692076987   \n",
       "2017-11-16 20:00:00  0.88468927269692076987  1.08963545353788426873   \n",
       "2017-11-16 22:00:00  1.08963545353788426873  0.81450222446344522353   \n",
       "2017-11-17 12:00:00  0.81450222446344522353  0.75273762202113092545   \n",
       "2017-11-17 14:00:00  0.75273762202113092545  0.58569244722467206365   \n",
       "2017-11-17 16:00:00  0.58569244722467206365  0.95908754382597771748   \n",
       "2017-11-17 18:00:00  0.95908754382597771748  1.00821847758547988860   \n",
       "2017-11-17 20:00:00  1.00821847758547988860  1.12472897765462209740   \n",
       "2017-11-17 22:00:00  1.12472897765462209740  0.78221618227840450377   \n",
       "2017-11-18 12:00:00  0.78221618227840450377  0.70220294729381438348   \n",
       "2017-11-18 14:00:00  0.70220294729381438348  0.56182885082686295775   \n",
       "2017-11-18 16:00:00  0.56182885082686295775  0.83836582086518485202   \n",
       "2017-11-18 18:00:00  0.83836582086518485202  0.90293790523919681412   \n",
       "2017-11-18 20:00:00  0.90293790523919681412  0.97312495346874228197   \n",
       "2017-11-18 22:00:00  0.97312495346874228197  0.58007748336913866893   \n",
       "2017-11-19 12:00:00  0.58007748336913866893  0.47900813391057522894   \n",
       "2017-11-19 14:00:00  0.47900813391057522894  0.33442281455197325757   \n",
       "2017-11-19 16:00:00  0.33442281455197325757  0.51269791706342981996   \n",
       "2017-11-19 18:00:00  0.51269791706342981996  0.59551863397971716019   \n",
       "2017-11-19 20:00:00  0.59551863397971716019  0.66149445931761208239   \n",
       "2017-11-19 22:00:00  0.66149445931761208239  0.24598713378015207853   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+5                     t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  0.95066509803481602514  0.53796525442905296366   \n",
       "2017-11-03 12:00:00  0.53796525442905296366  0.91837905584977530538   \n",
       "2017-11-03 14:00:00  0.91837905584977530538  1.14859257405243142536   \n",
       "2017-11-03 16:00:00  1.14859257405243142536  1.59498220081104902235   \n",
       "2017-11-03 18:00:00  1.59498220081104902235  1.60059716467051305067   \n",
       "2017-11-03 20:00:00  1.60059716467051305067  1.44057069470133258804   \n",
       "2017-11-03 22:00:00  1.44057069470133258804  0.94505013417928218633   \n",
       "2017-11-04 12:00:00  0.94505013417928218633  0.98716236311543736903   \n",
       "2017-11-04 14:00:00  0.98716236311543736903  1.16403372466301013866   \n",
       "2017-11-04 16:00:00  1.16403372466301013866  1.58796349598770136780   \n",
       "2017-11-04 18:00:00  1.58796349598770136780  1.60901961045774410941   \n",
       "2017-11-04 20:00:00  1.60901961045774410941  1.47145299592248957055   \n",
       "2017-11-04 22:00:00  1.47145299592248957055  0.93522394742423720082   \n",
       "2017-11-05 12:00:00  0.93522394742423720082  0.65728323642203179134   \n",
       "2017-11-05 14:00:00  0.65728323642203179134  0.78361992324228824103   \n",
       "2017-11-05 16:00:00  0.78361992324228824103  1.15561127887577907991   \n",
       "2017-11-05 18:00:00  1.15561127887577907991  1.11349904993569381872   \n",
       "2017-11-05 20:00:00  1.11349904993569381872  0.99698854987048202148   \n",
       "2017-11-05 22:00:00  0.99698854987048202148  0.47339317005504100155   \n",
       "2017-11-06 12:00:00  0.47339317005504100155  0.03682973004753833934   \n",
       "2017-11-06 14:00:00  0.03682973004753833934  0.13228411564270725620   \n",
       "2017-11-06 16:00:00  0.13228411564270725620  0.45654827847664902762   \n",
       "2017-11-06 18:00:00  0.45654827847664902762  0.27546569403742610094   \n",
       "2017-11-06 20:00:00  0.27546569403742610094  0.17299260361890922422   \n",
       "2017-11-06 22:00:00  0.17299260361890922422 -0.30708680629256251970   \n",
       "2017-11-07 12:00:00 -0.30708680629256251970 -0.68469312578551777548   \n",
       "2017-11-07 14:00:00 -0.68469312578551777548 -0.59625744501369659645   \n",
       "2017-11-07 16:00:00 -0.59625744501369659645 -0.28462695085863609634   \n",
       "2017-11-07 18:00:00 -0.28462695085863609634 -0.62292852333927317687   \n",
       "2017-11-07 20:00:00 -0.62292852333927317687 -0.71978664990225571518   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00 -0.02774235432647321684  0.51410165802731322415   \n",
       "2017-11-15 14:00:00  0.51410165802731322415  0.54779144118016787068   \n",
       "2017-11-15 16:00:00  0.54779144118016787068  0.86503689919076121040   \n",
       "2017-11-15 18:00:00  0.86503689919076121040  0.65447575449426476091   \n",
       "2017-11-15 20:00:00  0.65447575449426476091  0.60815230265859876457   \n",
       "2017-11-15 22:00:00  0.60815230265859876457  0.38355374831540633984   \n",
       "2017-11-16 12:00:00  0.38355374831540633984  0.84678826665241602178   \n",
       "2017-11-16 14:00:00  0.84678826665241602178  0.88468927269692076987   \n",
       "2017-11-16 16:00:00  0.88468927269692076987  1.08963545353788426873   \n",
       "2017-11-16 18:00:00  1.08963545353788426873  0.81450222446344522353   \n",
       "2017-11-16 20:00:00  0.81450222446344522353  0.75273762202113092545   \n",
       "2017-11-16 22:00:00  0.75273762202113092545  0.58569244722467206365   \n",
       "2017-11-17 12:00:00  0.58569244722467206365  0.95908754382597771748   \n",
       "2017-11-17 14:00:00  0.95908754382597771748  1.00821847758547988860   \n",
       "2017-11-17 16:00:00  1.00821847758547988860  1.12472897765462209740   \n",
       "2017-11-17 18:00:00  1.12472897765462209740  0.78221618227840450377   \n",
       "2017-11-17 20:00:00  0.78221618227840450377  0.70220294729381438348   \n",
       "2017-11-17 22:00:00  0.70220294729381438348  0.56182885082686295775   \n",
       "2017-11-18 12:00:00  0.56182885082686295775  0.83836582086518485202   \n",
       "2017-11-18 14:00:00  0.83836582086518485202  0.90293790523919681412   \n",
       "2017-11-18 16:00:00  0.90293790523919681412  0.97312495346874228197   \n",
       "2017-11-18 18:00:00  0.97312495346874228197  0.58007748336913866893   \n",
       "2017-11-18 20:00:00  0.58007748336913866893  0.47900813391057522894   \n",
       "2017-11-18 22:00:00  0.47900813391057522894  0.33442281455197325757   \n",
       "2017-11-19 12:00:00  0.33442281455197325757  0.51269791706342981996   \n",
       "2017-11-19 14:00:00  0.51269791706342981996  0.59551863397971716019   \n",
       "2017-11-19 16:00:00  0.59551863397971716019  0.66149445931761208239   \n",
       "2017-11-19 18:00:00  0.66149445931761208239  0.24598713378015207853   \n",
       "2017-11-19 20:00:00  0.24598713378015207853  0.13228411564270725620   \n",
       "2017-11-19 22:00:00  0.13228411564270725620 -0.04879846879651587865   \n",
       "\n",
       "tensor                                    X                          \\\n",
       "feature                                   e                           \n",
       "time step                               t-5                     t-4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00 -2.92617496977098268118 -2.79400608189676713522   \n",
       "2017-11-03 12:00:00 -2.79400608189676713522 -2.73539791391728170922   \n",
       "2017-11-03 14:00:00 -2.73539791391728170922 -2.70977277572862673694   \n",
       "2017-11-03 16:00:00 -2.70977277572862673694 -2.67152630068010399356   \n",
       "2017-11-03 18:00:00 -2.67152630068010399356 -2.47947436274205035289   \n",
       "2017-11-03 20:00:00 -2.47947436274205035289 -1.95303896117129727195   \n",
       "2017-11-03 22:00:00 -1.95303896117129727195 -1.77839827418765583644   \n",
       "2017-11-04 12:00:00 -1.77839827418765583644 -1.72634664376002766595   \n",
       "2017-11-04 14:00:00 -1.72634664376002766595 -1.69933734814780645905   \n",
       "2017-11-04 16:00:00 -1.69933734814780645905 -1.63533824623316559332   \n",
       "2017-11-04 18:00:00 -1.63533824623316559332 -1.50637477769096883584   \n",
       "2017-11-04 20:00:00 -1.50637477769096883584 -0.99163187025148802345   \n",
       "2017-11-04 22:00:00 -0.99163187025148802345 -0.82061549218194562716   \n",
       "2017-11-05 12:00:00 -0.82061549218194562716 -0.73887731173082793479   \n",
       "2017-11-05 14:00:00 -0.73887731173082793479 -0.74412254364959273811   \n",
       "2017-11-05 16:00:00 -0.74412254364959273811 -0.67536995275316269449   \n",
       "2017-11-05 18:00:00 -0.67536995275316269449 -0.59774782258829062265   \n",
       "2017-11-05 20:00:00 -0.59774782258829062265 -0.22919750908055919192   \n",
       "2017-11-05 22:00:00 -0.22919750908055919192 -0.11678930055519548548   \n",
       "2017-11-06 12:00:00 -0.11678930055519548548  0.03078745392909986237   \n",
       "2017-11-06 14:00:00  0.03078745392909986237 -0.02026248354428677664   \n",
       "2017-11-06 16:00:00 -0.02026248354428677664  0.01840288108604457018   \n",
       "2017-11-06 18:00:00  0.01840288108604457018  0.08735581088639735037   \n",
       "2017-11-06 20:00:00  0.08735581088639735037  0.25296304481097164896   \n",
       "2017-11-06 22:00:00  0.25296304481097164896  0.26778810648378881254   \n",
       "2017-11-07 12:00:00  0.26778810648378881254  0.49464433768609061826   \n",
       "2017-11-07 14:00:00  0.49464433768609061826  0.41673080505272314111   \n",
       "2017-11-07 16:00:00  0.41673080505272314111  0.39835428469199501977   \n",
       "2017-11-07 18:00:00  0.39835428469199501977  0.50655538249538389906   \n",
       "2017-11-07 20:00:00  0.50655538249538389906  0.51622627581428048860   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  0.57246680675461392163  0.46566807971331197757   \n",
       "2017-11-15 14:00:00  0.46566807971331197757  0.45284640394979230882   \n",
       "2017-11-15 16:00:00  0.45284640394979230882  0.40527507493839998176   \n",
       "2017-11-15 18:00:00  0.40527507493839998176  0.41707684440847908602   \n",
       "2017-11-15 20:00:00  0.41707684440847908602  0.13906140039398354191   \n",
       "2017-11-15 22:00:00  0.13906140039398354191  0.11345447554412074753   \n",
       "2017-11-16 12:00:00  0.11345447554412074753  0.01537958731915608362   \n",
       "2017-11-16 14:00:00  0.01537958731915608362  0.00091877834102239194   \n",
       "2017-11-16 16:00:00  0.00091877834102239194 -0.08078297699720485281   \n",
       "2017-11-16 18:00:00 -0.08078297699720485281 -0.04881985193498573905   \n",
       "2017-11-16 20:00:00 -0.04881985193498573905 -0.28928089961274144892   \n",
       "2017-11-16 22:00:00 -0.28928089961274144892 -0.32250069498507860644   \n",
       "2017-11-17 12:00:00 -0.32250069498507860644 -0.37983398088034736606   \n",
       "2017-11-17 14:00:00 -0.37983398088034736606 -0.37770310629331527164   \n",
       "2017-11-17 16:00:00 -0.37770310629331527164 -0.49368098684551747768   \n",
       "2017-11-17 18:00:00 -0.49368098684551747768 -0.42915372000985402279   \n",
       "2017-11-17 20:00:00 -0.42915372000985402279 -0.56574827084875922711   \n",
       "2017-11-17 22:00:00 -0.56574827084875922711 -0.61807309040649816012   \n",
       "2017-11-18 12:00:00 -0.61807309040649816012 -0.61639753051443624265   \n",
       "2017-11-18 14:00:00 -0.61639753051443624265 -0.58352377606241256913   \n",
       "2017-11-18 16:00:00 -0.58352377606241256913 -0.72447113964009046683   \n",
       "2017-11-18 18:00:00 -0.72447113964009046683 -0.63069442726636659735   \n",
       "2017-11-18 20:00:00 -0.63069442726636659735 -0.62756185972482148028   \n",
       "2017-11-18 22:00:00 -0.62756185972482148028 -0.70104972782894203753   \n",
       "2017-11-19 12:00:00 -0.70104972782894203753 -0.63914507695259314968   \n",
       "2017-11-19 14:00:00 -0.63914507695259314968 -0.56960934377939997919   \n",
       "2017-11-19 16:00:00 -0.56960934377939997919 -0.71928054773756555651   \n",
       "2017-11-19 18:00:00 -0.71928054773756555651 -0.60949495360695882251   \n",
       "2017-11-19 20:00:00 -0.60949495360695882251 -0.47084237684251628586   \n",
       "2017-11-19 22:00:00 -0.47084237684251628586 -0.55788042609965926566   \n",
       "\n",
       "tensor                                                               ...  \\\n",
       "feature                                                              ...   \n",
       "time step                               t-3                     t-2  ...   \n",
       "Epoch_Time_of_Clock                                                  ...   \n",
       "2017-11-02 22:00:00 -2.73539791391728170922 -2.70977277572862673694  ...   \n",
       "2017-11-03 12:00:00 -2.70977277572862673694 -2.67152630068010399356  ...   \n",
       "2017-11-03 14:00:00 -2.67152630068010399356 -2.47947436274205035289  ...   \n",
       "2017-11-03 16:00:00 -2.47947436274205035289 -1.95303896117129727195  ...   \n",
       "2017-11-03 18:00:00 -1.95303896117129727195 -1.77839827418765583644  ...   \n",
       "2017-11-03 20:00:00 -1.77839827418765583644 -1.72634664376002766595  ...   \n",
       "2017-11-03 22:00:00 -1.72634664376002766595 -1.69933734814780645905  ...   \n",
       "2017-11-04 12:00:00 -1.69933734814780645905 -1.63533824623316559332  ...   \n",
       "2017-11-04 14:00:00 -1.63533824623316559332 -1.50637477769096883584  ...   \n",
       "2017-11-04 16:00:00 -1.50637477769096883584 -0.99163187025148802345  ...   \n",
       "2017-11-04 18:00:00 -0.99163187025148802345 -0.82061549218194562716  ...   \n",
       "2017-11-04 20:00:00 -0.82061549218194562716 -0.73887731173082793479  ...   \n",
       "2017-11-04 22:00:00 -0.73887731173082793479 -0.74412254364959273811  ...   \n",
       "2017-11-05 12:00:00 -0.74412254364959273811 -0.67536995275316269449  ...   \n",
       "2017-11-05 14:00:00 -0.67536995275316269449 -0.59774782258829062265  ...   \n",
       "2017-11-05 16:00:00 -0.59774782258829062265 -0.22919750908055919192  ...   \n",
       "2017-11-05 18:00:00 -0.22919750908055919192 -0.11678930055519548548  ...   \n",
       "2017-11-05 20:00:00 -0.11678930055519548548  0.03078745392909986237  ...   \n",
       "2017-11-05 22:00:00  0.03078745392909986237 -0.02026248354428677664  ...   \n",
       "2017-11-06 12:00:00 -0.02026248354428677664  0.01840288108604457018  ...   \n",
       "2017-11-06 14:00:00  0.01840288108604457018  0.08735581088639735037  ...   \n",
       "2017-11-06 16:00:00  0.08735581088639735037  0.25296304481097164896  ...   \n",
       "2017-11-06 18:00:00  0.25296304481097164896  0.26778810648378881254  ...   \n",
       "2017-11-06 20:00:00  0.26778810648378881254  0.49464433768609061826  ...   \n",
       "2017-11-06 22:00:00  0.49464433768609061826  0.41673080505272314111  ...   \n",
       "2017-11-07 12:00:00  0.41673080505272314111  0.39835428469199501977  ...   \n",
       "2017-11-07 14:00:00  0.39835428469199501977  0.50655538249538389906  ...   \n",
       "2017-11-07 16:00:00  0.50655538249538389906  0.51622627581428048860  ...   \n",
       "2017-11-07 18:00:00  0.51622627581428048860  0.42709377864782482881  ...   \n",
       "2017-11-07 20:00:00  0.42709377864782482881  0.71315919454097298491  ...   \n",
       "...                                     ...                     ...  ...   \n",
       "2017-11-15 12:00:00  0.45284640394979230882  0.40527507493839998176  ...   \n",
       "2017-11-15 14:00:00  0.40527507493839998176  0.41707684440847908602  ...   \n",
       "2017-11-15 16:00:00  0.41707684440847908602  0.13906140039398354191  ...   \n",
       "2017-11-15 18:00:00  0.13906140039398354191  0.11345447554412074753  ...   \n",
       "2017-11-15 20:00:00  0.11345447554412074753  0.01537958731915608362  ...   \n",
       "2017-11-15 22:00:00  0.01537958731915608362  0.00091877834102239194  ...   \n",
       "2017-11-16 12:00:00  0.00091877834102239194 -0.08078297699720485281  ...   \n",
       "2017-11-16 14:00:00 -0.08078297699720485281 -0.04881985193498573905  ...   \n",
       "2017-11-16 16:00:00 -0.04881985193498573905 -0.28928089961274144892  ...   \n",
       "2017-11-16 18:00:00 -0.28928089961274144892 -0.32250069498507860644  ...   \n",
       "2017-11-16 20:00:00 -0.32250069498507860644 -0.37983398088034736606  ...   \n",
       "2017-11-16 22:00:00 -0.37983398088034736606 -0.37770310629331527164  ...   \n",
       "2017-11-17 12:00:00 -0.37770310629331527164 -0.49368098684551747768  ...   \n",
       "2017-11-17 14:00:00 -0.49368098684551747768 -0.42915372000985402279  ...   \n",
       "2017-11-17 16:00:00 -0.42915372000985402279 -0.56574827084875922711  ...   \n",
       "2017-11-17 18:00:00 -0.56574827084875922711 -0.61807309040649816012  ...   \n",
       "2017-11-17 20:00:00 -0.61807309040649816012 -0.61639753051443624265  ...   \n",
       "2017-11-17 22:00:00 -0.61639753051443624265 -0.58352377606241256913  ...   \n",
       "2017-11-18 12:00:00 -0.58352377606241256913 -0.72447113964009046683  ...   \n",
       "2017-11-18 14:00:00 -0.72447113964009046683 -0.63069442726636659735  ...   \n",
       "2017-11-18 16:00:00 -0.63069442726636659735 -0.62756185972482148028  ...   \n",
       "2017-11-18 18:00:00 -0.62756185972482148028 -0.70104972782894203753  ...   \n",
       "2017-11-18 20:00:00 -0.70104972782894203753 -0.63914507695259314968  ...   \n",
       "2017-11-18 22:00:00 -0.63914507695259314968 -0.56960934377939997919  ...   \n",
       "2017-11-19 12:00:00 -0.56960934377939997919 -0.71928054773756555651  ...   \n",
       "2017-11-19 14:00:00 -0.71928054773756555651 -0.60949495360695882251  ...   \n",
       "2017-11-19 16:00:00 -0.60949495360695882251 -0.47084237684251628586  ...   \n",
       "2017-11-19 18:00:00 -0.47084237684251628586 -0.55788042609965926566  ...   \n",
       "2017-11-19 20:00:00 -0.55788042609965926566 -0.45044425723410597140  ...   \n",
       "2017-11-19 22:00:00 -0.45044425723410597140 -0.34748839134934617068  ...   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                 Crc                           \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  1.25202275607788715561  1.28225133017616377273   \n",
       "2017-11-03 12:00:00  1.28225133017616377273  1.33464752527984376052   \n",
       "2017-11-03 14:00:00  1.33464752527984376052  1.25336624826003273370   \n",
       "2017-11-03 16:00:00  1.25336624826003273370  0.75224366431971045888   \n",
       "2017-11-03 18:00:00  0.75224366431971045888  0.51847602462636976917   \n",
       "2017-11-03 20:00:00  0.51847602462636976917  0.50705634107813191136   \n",
       "2017-11-03 22:00:00  0.50705634107813191136  0.56012428227288457716   \n",
       "2017-11-04 12:00:00  0.56012428227288457716  0.64745127411235081638   \n",
       "2017-11-04 14:00:00  0.64745127411235081638  0.57087221973004964592   \n",
       "2017-11-04 16:00:00  0.57087221973004964592 -0.07266053551771001207   \n",
       "2017-11-04 18:00:00 -0.07266053551771001207 -0.29030626902530304312   \n",
       "2017-11-04 20:00:00 -0.29030626902530304312 -0.34001547976469159718   \n",
       "2017-11-04 22:00:00 -0.34001547976469159718 -0.34135897194683723077   \n",
       "2017-11-05 12:00:00 -0.34135897194683723077 -0.25268848792522524693   \n",
       "2017-11-05 14:00:00 -0.25268848792522524693 -0.28023007765921076340   \n",
       "2017-11-05 16:00:00 -0.28023007765921076340 -0.84718377852466919542   \n",
       "2017-11-05 18:00:00 -0.84718377852466919542 -0.97280029755528607538   \n",
       "2017-11-05 20:00:00 -0.97280029755528607538 -1.03392919184291254275   \n",
       "2017-11-05 22:00:00 -1.03392919184291254275 -1.14476729686992761970   \n",
       "2017-11-06 12:00:00 -1.14476729686992761970 -1.10513427749663128985   \n",
       "2017-11-06 14:00:00 -1.10513427749663128985 -1.07087522685191749439   \n",
       "2017-11-06 16:00:00 -1.07087522685191749439 -1.35771080774001084990   \n",
       "2017-11-06 18:00:00 -1.35771080774001084990 -1.34494763200962719196   \n",
       "2017-11-06 20:00:00 -1.34494763200962719196 -1.37114572956146707483   \n",
       "2017-11-06 22:00:00 -1.37114572956146707483 -1.57670003342974940352   \n",
       "2017-11-07 12:00:00 -1.57670003342974940352 -1.61767654498519131145   \n",
       "2017-11-07 14:00:00 -1.61767654498519131145 -1.55318892024220089887   \n",
       "2017-11-07 16:00:00 -1.55318892024220089887 -1.47325113540453545014   \n",
       "2017-11-07 18:00:00 -1.47325113540453545014 -1.32748223364173401073   \n",
       "2017-11-07 20:00:00 -1.32748223364173401073 -1.27978826117556399034   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  1.24866402562252298836  1.19089386179026068824   \n",
       "2017-11-15 14:00:00  1.19089386179026068824  1.18955036960811488811   \n",
       "2017-11-15 16:00:00  1.18955036960811488811  0.97996558919339582516   \n",
       "2017-11-15 18:00:00  0.97996558919339582516  0.99541574928807052824   \n",
       "2017-11-15 20:00:00  0.99541574928807052824  0.89062335908071099677   \n",
       "2017-11-15 22:00:00  0.89062335908071099677  0.84897510143419629980   \n",
       "2017-11-16 12:00:00  0.84897510143419629980  0.75694588695722020422   \n",
       "2017-11-16 14:00:00  0.75694588695722020422  0.74283921904469096820   \n",
       "2017-11-16 16:00:00  0.74283921904469096820  0.46607982952269000343   \n",
       "2017-11-16 18:00:00  0.46607982952269000343  0.49429316534774836445   \n",
       "2017-11-16 20:00:00  0.49429316534774836445  0.40898141178150049235   \n",
       "2017-11-16 22:00:00  0.40898141178150049235  0.36531791586176731723   \n",
       "2017-11-17 12:00:00  0.36531791586176731723  0.25313631865260671772   \n",
       "2017-11-17 14:00:00  0.25313631865260671772  0.19402266263819872849   \n",
       "2017-11-17 16:00:00  0.19402266263819872849 -0.12505673062138980556   \n",
       "2017-11-17 18:00:00 -0.12505673062138980556 -0.08609545733916637578   \n",
       "2017-11-17 20:00:00 -0.08609545733916637578 -0.10624784007135092134   \n",
       "2017-11-17 22:00:00 -0.10624784007135092134 -0.11632403143744318719   \n",
       "2017-11-18 12:00:00 -0.11632403143744318719 -0.22581864428231251951   \n",
       "2017-11-18 14:00:00 -0.22581864428231251951 -0.34807643285756539875   \n",
       "2017-11-18 16:00:00 -0.34807643285756539875 -0.68193424012075598561   \n",
       "2017-11-18 18:00:00 -0.68193424012075598561 -0.64633169729389661207   \n",
       "2017-11-18 20:00:00 -0.64633169729389661207 -0.57311137336695938682   \n",
       "2017-11-18 22:00:00 -0.57311137336695938682 -0.52407390871864367732   \n",
       "2017-11-19 12:00:00 -0.52407390871864367732 -0.60871391619381876037   \n",
       "2017-11-19 14:00:00 -0.60871391619381876037 -0.79008536078347957332   \n",
       "2017-11-19 16:00:00 -0.79008536078347957332 -1.10580602358770407889   \n",
       "2017-11-19 18:00:00 -1.10580602358770407889 -1.09304284785732064300   \n",
       "2017-11-19 20:00:00 -1.09304284785732064300 -0.92376283290697036588   \n",
       "2017-11-19 22:00:00 -0.92376283290697036588 -0.80822250524244576564   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-1                       t   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  1.33464752527984376052  1.25336624826003273370   \n",
       "2017-11-03 12:00:00  1.25336624826003273370  0.75224366431971045888   \n",
       "2017-11-03 14:00:00  0.75224366431971045888  0.51847602462636976917   \n",
       "2017-11-03 16:00:00  0.51847602462636976917  0.50705634107813191136   \n",
       "2017-11-03 18:00:00  0.50705634107813191136  0.56012428227288457716   \n",
       "2017-11-03 20:00:00  0.56012428227288457716  0.64745127411235081638   \n",
       "2017-11-03 22:00:00  0.64745127411235081638  0.57087221973004964592   \n",
       "2017-11-04 12:00:00  0.57087221973004964592 -0.07266053551771001207   \n",
       "2017-11-04 14:00:00 -0.07266053551771001207 -0.29030626902530304312   \n",
       "2017-11-04 16:00:00 -0.29030626902530304312 -0.34001547976469159718   \n",
       "2017-11-04 18:00:00 -0.34001547976469159718 -0.34135897194683723077   \n",
       "2017-11-04 20:00:00 -0.34135897194683723077 -0.25268848792522524693   \n",
       "2017-11-04 22:00:00 -0.25268848792522524693 -0.28023007765921076340   \n",
       "2017-11-05 12:00:00 -0.28023007765921076340 -0.84718377852466919542   \n",
       "2017-11-05 14:00:00 -0.84718377852466919542 -0.97280029755528607538   \n",
       "2017-11-05 16:00:00 -0.97280029755528607538 -1.03392919184291254275   \n",
       "2017-11-05 18:00:00 -1.03392919184291254275 -1.14476729686992761970   \n",
       "2017-11-05 20:00:00 -1.14476729686992761970 -1.10513427749663128985   \n",
       "2017-11-05 22:00:00 -1.10513427749663128985 -1.07087522685191749439   \n",
       "2017-11-06 12:00:00 -1.07087522685191749439 -1.35771080774001084990   \n",
       "2017-11-06 14:00:00 -1.35771080774001084990 -1.34494763200962719196   \n",
       "2017-11-06 16:00:00 -1.34494763200962719196 -1.37114572956146707483   \n",
       "2017-11-06 18:00:00 -1.37114572956146707483 -1.57670003342974940352   \n",
       "2017-11-06 20:00:00 -1.57670003342974940352 -1.61767654498519131145   \n",
       "2017-11-06 22:00:00 -1.61767654498519131145 -1.55318892024220089887   \n",
       "2017-11-07 12:00:00 -1.55318892024220089887 -1.47325113540453545014   \n",
       "2017-11-07 14:00:00 -1.47325113540453545014 -1.32748223364173401073   \n",
       "2017-11-07 16:00:00 -1.32748223364173401073 -1.27978826117556399034   \n",
       "2017-11-07 18:00:00 -1.27978826117556399034 -1.50146447122959392217   \n",
       "2017-11-07 20:00:00 -1.50146447122959392217 -1.60961559189231762090   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00  1.18955036960811488811  0.97996558919339582516   \n",
       "2017-11-15 14:00:00  0.97996558919339582516  0.99541574928807052824   \n",
       "2017-11-15 16:00:00  0.99541574928807052824  0.89062335908071099677   \n",
       "2017-11-15 18:00:00  0.89062335908071099677  0.84897510143419629980   \n",
       "2017-11-15 20:00:00  0.84897510143419629980  0.75694588695722020422   \n",
       "2017-11-15 22:00:00  0.75694588695722020422  0.74283921904469096820   \n",
       "2017-11-16 12:00:00  0.74283921904469096820  0.46607982952269000343   \n",
       "2017-11-16 14:00:00  0.46607982952269000343  0.49429316534774836445   \n",
       "2017-11-16 16:00:00  0.49429316534774836445  0.40898141178150049235   \n",
       "2017-11-16 18:00:00  0.40898141178150049235  0.36531791586176731723   \n",
       "2017-11-16 20:00:00  0.36531791586176731723  0.25313631865260671772   \n",
       "2017-11-16 22:00:00  0.25313631865260671772  0.19402266263819872849   \n",
       "2017-11-17 12:00:00  0.19402266263819872849 -0.12505673062138980556   \n",
       "2017-11-17 14:00:00 -0.12505673062138980556 -0.08609545733916637578   \n",
       "2017-11-17 16:00:00 -0.08609545733916637578 -0.10624784007135092134   \n",
       "2017-11-17 18:00:00 -0.10624784007135092134 -0.11632403143744318719   \n",
       "2017-11-17 20:00:00 -0.11632403143744318719 -0.22581864428231251951   \n",
       "2017-11-17 22:00:00 -0.22581864428231251951 -0.34807643285756539875   \n",
       "2017-11-18 12:00:00 -0.34807643285756539875 -0.68193424012075598561   \n",
       "2017-11-18 14:00:00 -0.68193424012075598561 -0.64633169729389661207   \n",
       "2017-11-18 16:00:00 -0.64633169729389661207 -0.57311137336695938682   \n",
       "2017-11-18 18:00:00 -0.57311137336695938682 -0.52407390871864367732   \n",
       "2017-11-18 20:00:00 -0.52407390871864367732 -0.60871391619381876037   \n",
       "2017-11-18 22:00:00 -0.60871391619381876037 -0.79008536078347957332   \n",
       "2017-11-19 12:00:00 -0.79008536078347957332 -1.10580602358770407889   \n",
       "2017-11-19 14:00:00 -1.10580602358770407889 -1.09304284785732064300   \n",
       "2017-11-19 16:00:00 -1.09304284785732064300 -0.92376283290697036588   \n",
       "2017-11-19 18:00:00 -0.92376283290697036588 -0.80822250524244576564   \n",
       "2017-11-19 20:00:00 -0.80822250524244576564 -0.84785552461574198446   \n",
       "2017-11-19 22:00:00 -0.84785552461574198446 -1.06415776594118938192   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                               I_dot                           \n",
       "time step                               t-5                     t-4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00 -0.21724738455685779770  0.05086713969423365539   \n",
       "2017-11-03 12:00:00  0.05086713969423365539  0.42987720015107228066   \n",
       "2017-11-03 14:00:00  0.42987720015107228066  0.36670885674094411133   \n",
       "2017-11-03 16:00:00  0.36670885674094411133  0.19966368194448591566   \n",
       "2017-11-03 18:00:00  0.19966368194448591566 -0.11196681220664428391   \n",
       "2017-11-03 20:00:00 -0.11196681220664428391  0.47058568812334411469   \n",
       "2017-11-03 22:00:00  0.47058568812334411469  0.73589273044273850743   \n",
       "2017-11-04 12:00:00  0.73589273044273850743  1.16262998369912717855   \n",
       "2017-11-04 14:00:00  1.16262998369912717855  1.12051775475904102919   \n",
       "2017-11-04 16:00:00  1.12051775475904102919  0.95066509803481602514   \n",
       "2017-11-04 18:00:00  0.95066509803481602514  0.53796525442905296366   \n",
       "2017-11-04 20:00:00  0.53796525442905296366  0.91837905584977530538   \n",
       "2017-11-04 22:00:00  0.91837905584977530538  1.14859257405243142536   \n",
       "2017-11-05 12:00:00  1.14859257405243142536  1.59498220081104902235   \n",
       "2017-11-05 14:00:00  1.59498220081104902235  1.60059716467051305067   \n",
       "2017-11-05 16:00:00  1.60059716467051305067  1.44057069470133258804   \n",
       "2017-11-05 18:00:00  1.44057069470133258804  0.94505013417928218633   \n",
       "2017-11-05 20:00:00  0.94505013417928218633  0.98716236311543736903   \n",
       "2017-11-05 22:00:00  0.98716236311543736903  1.16403372466301013866   \n",
       "2017-11-06 12:00:00  1.16403372466301013866  1.58796349598770136780   \n",
       "2017-11-06 14:00:00  1.58796349598770136780  1.60901961045774410941   \n",
       "2017-11-06 16:00:00  1.60901961045774410941  1.47145299592248957055   \n",
       "2017-11-06 18:00:00  1.47145299592248957055  0.93522394742423720082   \n",
       "2017-11-06 20:00:00  0.93522394742423720082  0.65728323642203179134   \n",
       "2017-11-06 22:00:00  0.65728323642203179134  0.78361992324228824103   \n",
       "2017-11-07 12:00:00  0.78361992324228824103  1.15561127887577907991   \n",
       "2017-11-07 14:00:00  1.15561127887577907991  1.11349904993569381872   \n",
       "2017-11-07 16:00:00  1.11349904993569381872  0.99698854987048202148   \n",
       "2017-11-07 18:00:00  0.99698854987048202148  0.47339317005504100155   \n",
       "2017-11-07 20:00:00  0.47339317005504100155  0.03682973004753833934   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00 -0.52466665581633731286 -0.00247501696477986192   \n",
       "2017-11-15 14:00:00 -0.00247501696477986192 -0.18496134236788674965   \n",
       "2017-11-15 16:00:00 -0.18496134236788674965 -0.22426608938020536899   \n",
       "2017-11-15 18:00:00 -0.22426608938020536899 -0.58923874019034905292   \n",
       "2017-11-15 20:00:00 -0.58923874019034905292  0.01156239268191504473   \n",
       "2017-11-15 22:00:00  0.01156239268191504473  0.05086713969423365539   \n",
       "2017-11-16 12:00:00  0.05086713969423365539  0.48321935680615585307   \n",
       "2017-11-16 14:00:00  0.48321935680615585307  0.30775173622639651061   \n",
       "2017-11-16 16:00:00  0.30775173622639651061  0.26985073017796129546   \n",
       "2017-11-16 18:00:00  0.26985073017796129546 -0.02774235432647321684   \n",
       "2017-11-16 20:00:00 -0.02774235432647321684  0.51410165802731322415   \n",
       "2017-11-16 22:00:00  0.51410165802731322415  0.54779144118016787068   \n",
       "2017-11-17 12:00:00  0.54779144118016787068  0.86503689919076121040   \n",
       "2017-11-17 14:00:00  0.86503689919076121040  0.65447575449426476091   \n",
       "2017-11-17 16:00:00  0.65447575449426476091  0.60815230265859876457   \n",
       "2017-11-17 18:00:00  0.60815230265859876457  0.38355374831540633984   \n",
       "2017-11-17 20:00:00  0.38355374831540633984  0.84678826665241602178   \n",
       "2017-11-17 22:00:00  0.84678826665241602178  0.88468927269692076987   \n",
       "2017-11-18 12:00:00  0.88468927269692076987  1.08963545353788426873   \n",
       "2017-11-18 14:00:00  1.08963545353788426873  0.81450222446344522353   \n",
       "2017-11-18 16:00:00  0.81450222446344522353  0.75273762202113092545   \n",
       "2017-11-18 18:00:00  0.75273762202113092545  0.58569244722467206365   \n",
       "2017-11-18 20:00:00  0.58569244722467206365  0.95908754382597771748   \n",
       "2017-11-18 22:00:00  0.95908754382597771748  1.00821847758547988860   \n",
       "2017-11-19 12:00:00  1.00821847758547988860  1.12472897765462209740   \n",
       "2017-11-19 14:00:00  1.12472897765462209740  0.78221618227840450377   \n",
       "2017-11-19 16:00:00  0.78221618227840450377  0.70220294729381438348   \n",
       "2017-11-19 18:00:00  0.70220294729381438348  0.56182885082686295775   \n",
       "2017-11-19 20:00:00  0.56182885082686295775  0.83836582086518485202   \n",
       "2017-11-19 22:00:00  0.83836582086518485202  0.90293790523919681412   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-02 22:00:00  0.42987720015107228066  0.36670885674094411133   \n",
       "2017-11-03 12:00:00  0.36670885674094411133  0.19966368194448591566   \n",
       "2017-11-03 14:00:00  0.19966368194448591566 -0.11196681220664428391   \n",
       "2017-11-03 16:00:00 -0.11196681220664428391  0.47058568812334411469   \n",
       "2017-11-03 18:00:00  0.47058568812334411469  0.73589273044273850743   \n",
       "2017-11-03 20:00:00  0.73589273044273850743  1.16262998369912717855   \n",
       "2017-11-03 22:00:00  1.16262998369912717855  1.12051775475904102919   \n",
       "2017-11-04 12:00:00  1.12051775475904102919  0.95066509803481602514   \n",
       "2017-11-04 14:00:00  0.95066509803481602514  0.53796525442905296366   \n",
       "2017-11-04 16:00:00  0.53796525442905296366  0.91837905584977530538   \n",
       "2017-11-04 18:00:00  0.91837905584977530538  1.14859257405243142536   \n",
       "2017-11-04 20:00:00  1.14859257405243142536  1.59498220081104902235   \n",
       "2017-11-04 22:00:00  1.59498220081104902235  1.60059716467051305067   \n",
       "2017-11-05 12:00:00  1.60059716467051305067  1.44057069470133258804   \n",
       "2017-11-05 14:00:00  1.44057069470133258804  0.94505013417928218633   \n",
       "2017-11-05 16:00:00  0.94505013417928218633  0.98716236311543736903   \n",
       "2017-11-05 18:00:00  0.98716236311543736903  1.16403372466301013866   \n",
       "2017-11-05 20:00:00  1.16403372466301013866  1.58796349598770136780   \n",
       "2017-11-05 22:00:00  1.58796349598770136780  1.60901961045774410941   \n",
       "2017-11-06 12:00:00  1.60901961045774410941  1.47145299592248957055   \n",
       "2017-11-06 14:00:00  1.47145299592248957055  0.93522394742423720082   \n",
       "2017-11-06 16:00:00  0.93522394742423720082  0.65728323642203179134   \n",
       "2017-11-06 18:00:00  0.65728323642203179134  0.78361992324228824103   \n",
       "2017-11-06 20:00:00  0.78361992324228824103  1.15561127887577907991   \n",
       "2017-11-06 22:00:00  1.15561127887577907991  1.11349904993569381872   \n",
       "2017-11-07 12:00:00  1.11349904993569381872  0.99698854987048202148   \n",
       "2017-11-07 14:00:00  0.99698854987048202148  0.47339317005504100155   \n",
       "2017-11-07 16:00:00  0.47339317005504100155  0.03682973004753833934   \n",
       "2017-11-07 18:00:00  0.03682973004753833934  0.13228411564270725620   \n",
       "2017-11-07 20:00:00  0.13228411564270725620  0.45654827847664902762   \n",
       "...                                     ...                     ...   \n",
       "2017-11-15 12:00:00 -0.18496134236788674965 -0.22426608938020536899   \n",
       "2017-11-15 14:00:00 -0.22426608938020536899 -0.58923874019034905292   \n",
       "2017-11-15 16:00:00 -0.58923874019034905292  0.01156239268191504473   \n",
       "2017-11-15 18:00:00  0.01156239268191504473  0.05086713969423365539   \n",
       "2017-11-15 20:00:00  0.05086713969423365539  0.48321935680615585307   \n",
       "2017-11-15 22:00:00  0.48321935680615585307  0.30775173622639651061   \n",
       "2017-11-16 12:00:00  0.30775173622639651061  0.26985073017796129546   \n",
       "2017-11-16 14:00:00  0.26985073017796129546 -0.02774235432647321684   \n",
       "2017-11-16 16:00:00 -0.02774235432647321684  0.51410165802731322415   \n",
       "2017-11-16 18:00:00  0.51410165802731322415  0.54779144118016787068   \n",
       "2017-11-16 20:00:00  0.54779144118016787068  0.86503689919076121040   \n",
       "2017-11-16 22:00:00  0.86503689919076121040  0.65447575449426476091   \n",
       "2017-11-17 12:00:00  0.65447575449426476091  0.60815230265859876457   \n",
       "2017-11-17 14:00:00  0.60815230265859876457  0.38355374831540633984   \n",
       "2017-11-17 16:00:00  0.38355374831540633984  0.84678826665241602178   \n",
       "2017-11-17 18:00:00  0.84678826665241602178  0.88468927269692076987   \n",
       "2017-11-17 20:00:00  0.88468927269692076987  1.08963545353788426873   \n",
       "2017-11-17 22:00:00  1.08963545353788426873  0.81450222446344522353   \n",
       "2017-11-18 12:00:00  0.81450222446344522353  0.75273762202113092545   \n",
       "2017-11-18 14:00:00  0.75273762202113092545  0.58569244722467206365   \n",
       "2017-11-18 16:00:00  0.58569244722467206365  0.95908754382597771748   \n",
       "2017-11-18 18:00:00  0.95908754382597771748  1.00821847758547988860   \n",
       "2017-11-18 20:00:00  1.00821847758547988860  1.12472897765462209740   \n",
       "2017-11-18 22:00:00  1.12472897765462209740  0.78221618227840450377   \n",
       "2017-11-19 12:00:00  0.78221618227840450377  0.70220294729381438348   \n",
       "2017-11-19 14:00:00  0.70220294729381438348  0.56182885082686295775   \n",
       "2017-11-19 16:00:00  0.56182885082686295775  0.83836582086518485202   \n",
       "2017-11-19 18:00:00  0.83836582086518485202  0.90293790523919681412   \n",
       "2017-11-19 20:00:00  0.90293790523919681412  0.97312495346874228197   \n",
       "2017-11-19 22:00:00  0.97312495346874228197  0.58007748336913866893   \n",
       "\n",
       "tensor                                                               \n",
       "feature                                                              \n",
       "time step                               t-1                       t  \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-02 22:00:00  0.19966368194448591566 -0.11196681220664428391  \n",
       "2017-11-03 12:00:00 -0.11196681220664428391  0.47058568812334411469  \n",
       "2017-11-03 14:00:00  0.47058568812334411469  0.73589273044273850743  \n",
       "2017-11-03 16:00:00  0.73589273044273850743  1.16262998369912717855  \n",
       "2017-11-03 18:00:00  1.16262998369912717855  1.12051775475904102919  \n",
       "2017-11-03 20:00:00  1.12051775475904102919  0.95066509803481602514  \n",
       "2017-11-03 22:00:00  0.95066509803481602514  0.53796525442905296366  \n",
       "2017-11-04 12:00:00  0.53796525442905296366  0.91837905584977530538  \n",
       "2017-11-04 14:00:00  0.91837905584977530538  1.14859257405243142536  \n",
       "2017-11-04 16:00:00  1.14859257405243142536  1.59498220081104902235  \n",
       "2017-11-04 18:00:00  1.59498220081104902235  1.60059716467051305067  \n",
       "2017-11-04 20:00:00  1.60059716467051305067  1.44057069470133258804  \n",
       "2017-11-04 22:00:00  1.44057069470133258804  0.94505013417928218633  \n",
       "2017-11-05 12:00:00  0.94505013417928218633  0.98716236311543736903  \n",
       "2017-11-05 14:00:00  0.98716236311543736903  1.16403372466301013866  \n",
       "2017-11-05 16:00:00  1.16403372466301013866  1.58796349598770136780  \n",
       "2017-11-05 18:00:00  1.58796349598770136780  1.60901961045774410941  \n",
       "2017-11-05 20:00:00  1.60901961045774410941  1.47145299592248957055  \n",
       "2017-11-05 22:00:00  1.47145299592248957055  0.93522394742423720082  \n",
       "2017-11-06 12:00:00  0.93522394742423720082  0.65728323642203179134  \n",
       "2017-11-06 14:00:00  0.65728323642203179134  0.78361992324228824103  \n",
       "2017-11-06 16:00:00  0.78361992324228824103  1.15561127887577907991  \n",
       "2017-11-06 18:00:00  1.15561127887577907991  1.11349904993569381872  \n",
       "2017-11-06 20:00:00  1.11349904993569381872  0.99698854987048202148  \n",
       "2017-11-06 22:00:00  0.99698854987048202148  0.47339317005504100155  \n",
       "2017-11-07 12:00:00  0.47339317005504100155  0.03682973004753833934  \n",
       "2017-11-07 14:00:00  0.03682973004753833934  0.13228411564270725620  \n",
       "2017-11-07 16:00:00  0.13228411564270725620  0.45654827847664902762  \n",
       "2017-11-07 18:00:00  0.45654827847664902762  0.27546569403742610094  \n",
       "2017-11-07 20:00:00  0.27546569403742610094  0.17299260361890922422  \n",
       "...                                     ...                     ...  \n",
       "2017-11-15 12:00:00 -0.58923874019034905292  0.01156239268191504473  \n",
       "2017-11-15 14:00:00  0.01156239268191504473  0.05086713969423365539  \n",
       "2017-11-15 16:00:00  0.05086713969423365539  0.48321935680615585307  \n",
       "2017-11-15 18:00:00  0.48321935680615585307  0.30775173622639651061  \n",
       "2017-11-15 20:00:00  0.30775173622639651061  0.26985073017796129546  \n",
       "2017-11-15 22:00:00  0.26985073017796129546 -0.02774235432647321684  \n",
       "2017-11-16 12:00:00 -0.02774235432647321684  0.51410165802731322415  \n",
       "2017-11-16 14:00:00  0.51410165802731322415  0.54779144118016787068  \n",
       "2017-11-16 16:00:00  0.54779144118016787068  0.86503689919076121040  \n",
       "2017-11-16 18:00:00  0.86503689919076121040  0.65447575449426476091  \n",
       "2017-11-16 20:00:00  0.65447575449426476091  0.60815230265859876457  \n",
       "2017-11-16 22:00:00  0.60815230265859876457  0.38355374831540633984  \n",
       "2017-11-17 12:00:00  0.38355374831540633984  0.84678826665241602178  \n",
       "2017-11-17 14:00:00  0.84678826665241602178  0.88468927269692076987  \n",
       "2017-11-17 16:00:00  0.88468927269692076987  1.08963545353788426873  \n",
       "2017-11-17 18:00:00  1.08963545353788426873  0.81450222446344522353  \n",
       "2017-11-17 20:00:00  0.81450222446344522353  0.75273762202113092545  \n",
       "2017-11-17 22:00:00  0.75273762202113092545  0.58569244722467206365  \n",
       "2017-11-18 12:00:00  0.58569244722467206365  0.95908754382597771748  \n",
       "2017-11-18 14:00:00  0.95908754382597771748  1.00821847758547988860  \n",
       "2017-11-18 16:00:00  1.00821847758547988860  1.12472897765462209740  \n",
       "2017-11-18 18:00:00  1.12472897765462209740  0.78221618227840450377  \n",
       "2017-11-18 20:00:00  0.78221618227840450377  0.70220294729381438348  \n",
       "2017-11-18 22:00:00  0.70220294729381438348  0.56182885082686295775  \n",
       "2017-11-19 12:00:00  0.56182885082686295775  0.83836582086518485202  \n",
       "2017-11-19 14:00:00  0.83836582086518485202  0.90293790523919681412  \n",
       "2017-11-19 16:00:00  0.90293790523919681412  0.97312495346874228197  \n",
       "2017-11-19 18:00:00  0.97312495346874228197  0.58007748336913866893  \n",
       "2017-11-19 20:00:00  0.58007748336913866893  0.47900813391057522894  \n",
       "2017-11-19 22:00:00  0.47900813391057522894  0.33442281455197325757  \n",
       "\n",
       "[103 rows x 24 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs['target'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct validation set (keeping T hours from the training set in order to construct initial features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"15\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"4\" halign=\"left\">e</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Crc</th>\n",
       "      <th colspan=\"6\" halign=\"left\">I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>...</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-21 22:00:00</th>\n",
       "      <td>-0.48957313169959953969</td>\n",
       "      <td>-0.37587011356215488389</td>\n",
       "      <td>-0.22707357130797228839</td>\n",
       "      <td>-0.59766118597758000064</td>\n",
       "      <td>-0.71838290893837242201</td>\n",
       "      <td>-0.99913110186834508397</td>\n",
       "      <td>0.25846325095622635359</td>\n",
       "      <td>0.19109481904778460870</td>\n",
       "      <td>0.30945855154248819163</td>\n",
       "      <td>0.44261091903228955147</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.11991269150023331491</td>\n",
       "      <td>-0.91234314935873250807</td>\n",
       "      <td>-0.84113806370501376097</td>\n",
       "      <td>-1.00974633256429102701</td>\n",
       "      <td>0.04384843487088609798</td>\n",
       "      <td>0.14351404336163542386</td>\n",
       "      <td>0.24177591088457145441</td>\n",
       "      <td>-0.16530896886565779602</td>\n",
       "      <td>-0.28603069182251983360</td>\n",
       "      <td>-0.51905169195687317352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 12:00:00</th>\n",
       "      <td>-0.37587011356215488389</td>\n",
       "      <td>-0.22707357130797228839</td>\n",
       "      <td>-0.59766118597758000064</td>\n",
       "      <td>-0.71838290893837242201</td>\n",
       "      <td>-0.99913110186834508397</td>\n",
       "      <td>-0.99913110186834508397</td>\n",
       "      <td>0.19109481904778460870</td>\n",
       "      <td>0.30945855154248819163</td>\n",
       "      <td>0.44261091903228955147</td>\n",
       "      <td>0.37249238248652727368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.91234314935873250807</td>\n",
       "      <td>-0.84113806370501376097</td>\n",
       "      <td>-1.00974633256429102701</td>\n",
       "      <td>-1.06953173466977191630</td>\n",
       "      <td>0.14351404336163542386</td>\n",
       "      <td>0.24177591088457145441</td>\n",
       "      <td>-0.16530896886565779602</td>\n",
       "      <td>-0.28603069182251983360</td>\n",
       "      <td>-0.51905169195687317352</td>\n",
       "      <td>-0.48957313169959953969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 14:00:00</th>\n",
       "      <td>-0.22707357130797228839</td>\n",
       "      <td>-0.59766118597758000064</td>\n",
       "      <td>-0.71838290893837242201</td>\n",
       "      <td>-0.99913110186834508397</td>\n",
       "      <td>-0.99913110186834508397</td>\n",
       "      <td>-0.87419815601590245002</td>\n",
       "      <td>0.30945855154248819163</td>\n",
       "      <td>0.44261091903228955147</td>\n",
       "      <td>0.37249238248652727368</td>\n",
       "      <td>0.41203195295805883358</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.84113806370501376097</td>\n",
       "      <td>-1.00974633256429102701</td>\n",
       "      <td>-1.06953173466977191630</td>\n",
       "      <td>-1.19581999979146158530</td>\n",
       "      <td>0.24177591088457145441</td>\n",
       "      <td>-0.16530896886565779602</td>\n",
       "      <td>-0.28603069182251983360</td>\n",
       "      <td>-0.51905169195687317352</td>\n",
       "      <td>-0.48957313169959953969</td>\n",
       "      <td>-0.37587011356215488389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 16:00:00</th>\n",
       "      <td>-0.59766118597758000064</td>\n",
       "      <td>-0.71838290893837242201</td>\n",
       "      <td>-0.99913110186834508397</td>\n",
       "      <td>-0.99913110186834508397</td>\n",
       "      <td>-0.87419815601590245002</td>\n",
       "      <td>-0.67907816192605385819</td>\n",
       "      <td>0.44261091903228955147</td>\n",
       "      <td>0.37249238248652727368</td>\n",
       "      <td>0.41203195295805883358</td>\n",
       "      <td>0.65739219163440154592</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.00974633256429102701</td>\n",
       "      <td>-1.06953173466977191630</td>\n",
       "      <td>-1.19581999979146158530</td>\n",
       "      <td>-0.94996093045881035977</td>\n",
       "      <td>-0.16530896886565779602</td>\n",
       "      <td>-0.28603069182251983360</td>\n",
       "      <td>-0.51905169195687317352</td>\n",
       "      <td>-0.48957313169959953969</td>\n",
       "      <td>-0.37587011356215488389</td>\n",
       "      <td>-0.22707357130797228839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 18:00:00</th>\n",
       "      <td>-0.71838290893837242201</td>\n",
       "      <td>-0.99913110186834508397</td>\n",
       "      <td>-0.99913110186834508397</td>\n",
       "      <td>-0.87419815601590245002</td>\n",
       "      <td>-0.67907816192605385819</td>\n",
       "      <td>-0.99772736090446167978</td>\n",
       "      <td>0.37249238248652727368</td>\n",
       "      <td>0.41203195295805883358</td>\n",
       "      <td>0.65739219163440154592</td>\n",
       "      <td>0.62400848403586117730</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.06953173466977191630</td>\n",
       "      <td>-1.19581999979146158530</td>\n",
       "      <td>-0.94996093045881035977</td>\n",
       "      <td>-0.73701741958872701854</td>\n",
       "      <td>-0.28603069182251983360</td>\n",
       "      <td>-0.51905169195687317352</td>\n",
       "      <td>-0.48957313169959953969</td>\n",
       "      <td>-0.37587011356215488389</td>\n",
       "      <td>-0.22707357130797228839</td>\n",
       "      <td>-0.59766118597758000064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                               target                          \\\n",
       "feature                                   y                           \n",
       "time step                               t+1                     t+2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -0.48957313169959953969 -0.37587011356215488389   \n",
       "2017-11-22 12:00:00 -0.37587011356215488389 -0.22707357130797228839   \n",
       "2017-11-22 14:00:00 -0.22707357130797228839 -0.59766118597758000064   \n",
       "2017-11-22 16:00:00 -0.59766118597758000064 -0.71838290893837242201   \n",
       "2017-11-22 18:00:00 -0.71838290893837242201 -0.99913110186834508397   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+3                     t+4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -0.22707357130797228839 -0.59766118597758000064   \n",
       "2017-11-22 12:00:00 -0.59766118597758000064 -0.71838290893837242201   \n",
       "2017-11-22 14:00:00 -0.71838290893837242201 -0.99913110186834508397   \n",
       "2017-11-22 16:00:00 -0.99913110186834508397 -0.99913110186834508397   \n",
       "2017-11-22 18:00:00 -0.99913110186834508397 -0.87419815601590245002   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+5                     t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -0.71838290893837242201 -0.99913110186834508397   \n",
       "2017-11-22 12:00:00 -0.99913110186834508397 -0.99913110186834508397   \n",
       "2017-11-22 14:00:00 -0.99913110186834508397 -0.87419815601590245002   \n",
       "2017-11-22 16:00:00 -0.87419815601590245002 -0.67907816192605385819   \n",
       "2017-11-22 18:00:00 -0.67907816192605385819 -0.99772736090446167978   \n",
       "\n",
       "tensor                                   X                         \\\n",
       "feature                                  e                          \n",
       "time step                              t-5                    t-4   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-21 22:00:00 0.25846325095622635359 0.19109481904778460870   \n",
       "2017-11-22 12:00:00 0.19109481904778460870 0.30945855154248819163   \n",
       "2017-11-22 14:00:00 0.30945855154248819163 0.44261091903228955147   \n",
       "2017-11-22 16:00:00 0.44261091903228955147 0.37249238248652727368   \n",
       "2017-11-22 18:00:00 0.37249238248652727368 0.41203195295805883358   \n",
       "\n",
       "tensor                                                             ...  \\\n",
       "feature                                                            ...   \n",
       "time step                              t-3                    t-2  ...   \n",
       "Epoch_Time_of_Clock                                                ...   \n",
       "2017-11-21 22:00:00 0.30945855154248819163 0.44261091903228955147  ...   \n",
       "2017-11-22 12:00:00 0.44261091903228955147 0.37249238248652727368  ...   \n",
       "2017-11-22 14:00:00 0.37249238248652727368 0.41203195295805883358  ...   \n",
       "2017-11-22 16:00:00 0.41203195295805883358 0.65739219163440154592  ...   \n",
       "2017-11-22 18:00:00 0.65739219163440154592 0.62400848403586117730  ...   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                 Crc                           \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -1.11991269150023331491 -0.91234314935873250807   \n",
       "2017-11-22 12:00:00 -0.91234314935873250807 -0.84113806370501376097   \n",
       "2017-11-22 14:00:00 -0.84113806370501376097 -1.00974633256429102701   \n",
       "2017-11-22 16:00:00 -1.00974633256429102701 -1.06953173466977191630   \n",
       "2017-11-22 18:00:00 -1.06953173466977191630 -1.19581999979146158530   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-1                       t   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00 -0.84113806370501376097 -1.00974633256429102701   \n",
       "2017-11-22 12:00:00 -1.00974633256429102701 -1.06953173466977191630   \n",
       "2017-11-22 14:00:00 -1.06953173466977191630 -1.19581999979146158530   \n",
       "2017-11-22 16:00:00 -1.19581999979146158530 -0.94996093045881035977   \n",
       "2017-11-22 18:00:00 -0.94996093045881035977 -0.73701741958872701854   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                               I_dot                           \n",
       "time step                               t-5                     t-4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00  0.04384843487088609798  0.14351404336163542386   \n",
       "2017-11-22 12:00:00  0.14351404336163542386  0.24177591088457145441   \n",
       "2017-11-22 14:00:00  0.24177591088457145441 -0.16530896886565779602   \n",
       "2017-11-22 16:00:00 -0.16530896886565779602 -0.28603069182251983360   \n",
       "2017-11-22 18:00:00 -0.28603069182251983360 -0.51905169195687317352   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-21 22:00:00  0.24177591088457145441 -0.16530896886565779602   \n",
       "2017-11-22 12:00:00 -0.16530896886565779602 -0.28603069182251983360   \n",
       "2017-11-22 14:00:00 -0.28603069182251983360 -0.51905169195687317352   \n",
       "2017-11-22 16:00:00 -0.51905169195687317352 -0.48957313169959953969   \n",
       "2017-11-22 18:00:00 -0.48957313169959953969 -0.37587011356215488389   \n",
       "\n",
       "tensor                                                               \n",
       "feature                                                              \n",
       "time step                               t-1                       t  \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-21 22:00:00 -0.28603069182251983360 -0.51905169195687317352  \n",
       "2017-11-22 12:00:00 -0.51905169195687317352 -0.48957313169959953969  \n",
       "2017-11-22 14:00:00 -0.48957313169959953969 -0.37587011356215488389  \n",
       "2017-11-22 16:00:00 -0.37587011356215488389 -0.22707357130797228839  \n",
       "2017-11-22 18:00:00 -0.22707357130797228839 -0.59766118597758000064  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back_dt = dt.datetime.strptime(valid_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1)\n",
    "valid = df.copy()[(df.index >=look_back_dt) & (df.index < test_start_dt)][['e', 'Crc', 'I_dot']]\n",
    "valid[['e', 'Crc', 'I_dot']] = X_scaler.transform(valid)\n",
    "valid_inputs = TimeSeriesTensor(valid, var_name, HORIZON, tensor_structure,freq = None)\n",
    "valid_inputs.dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a RNN forecasting model with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image('./images/simple_encoder_decoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Flatten\n",
    "from keras.callbacks import EarlyStopping ,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(LATENT_DIM, input_shape=(T,3 ) ,   return_sequences=True))\n",
    "model.add(LSTM(LATENT_DIM ) )\n",
    "model.add(RepeatVector(HORIZON))\n",
    "model.add(LSTM(LATENT_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='RMSprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 6, 64)             17408     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 6, 64)             33024     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 6, 1)              65        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 83,521\n",
      "Trainable params: 83,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val = ModelCheckpoint(str(sat_var) +'_' +  var_name + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103 samples, validate on 13 samples\n",
      "Epoch 1/1000\n",
      "103/103 [==============================] - 3s 24ms/step - loss: 0.9920 - val_loss: 0.8504\n",
      "Epoch 2/1000\n",
      "103/103 [==============================] - 0s 615us/step - loss: 0.6521 - val_loss: 0.4504\n",
      "Epoch 3/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.1876\n",
      "Epoch 4/1000\n",
      "103/103 [==============================] - 0s 644us/step - loss: 0.2952 - val_loss: 0.1089\n",
      "Epoch 5/1000\n",
      "103/103 [==============================] - 0s 873us/step - loss: 0.2345 - val_loss: 0.1190\n",
      "Epoch 6/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2090 - val_loss: 0.1056\n",
      "Epoch 7/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1916 - val_loss: 0.0976\n",
      "Epoch 8/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1749 - val_loss: 0.0802\n",
      "Epoch 9/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1602 - val_loss: 0.0698\n",
      "Epoch 10/1000\n",
      "103/103 [==============================] - 0s 711us/step - loss: 0.1459 - val_loss: 0.0579\n",
      "Epoch 11/1000\n",
      "103/103 [==============================] - 0s 833us/step - loss: 0.1345 - val_loss: 0.0560\n",
      "Epoch 12/1000\n",
      "103/103 [==============================] - 0s 717us/step - loss: 0.1229 - val_loss: 0.0528\n",
      "Epoch 13/1000\n",
      "103/103 [==============================] - 0s 703us/step - loss: 0.1185 - val_loss: 0.0629\n",
      "Epoch 14/1000\n",
      "103/103 [==============================] - 0s 821us/step - loss: 0.1076 - val_loss: 0.0596\n",
      "Epoch 15/1000\n",
      "103/103 [==============================] - 0s 660us/step - loss: 0.1186 - val_loss: 0.0976\n",
      "Epoch 16/1000\n",
      "103/103 [==============================] - 0s 774us/step - loss: 0.0987 - val_loss: 0.0665\n",
      "Epoch 17/1000\n",
      "103/103 [==============================] - 0s 905us/step - loss: 0.1246 - val_loss: 0.1416\n",
      "Epoch 18/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.0605\n",
      "Epoch 19/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1163 - val_loss: 0.1469\n",
      "Epoch 20/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0934 - val_loss: 0.0543\n",
      "Epoch 21/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1081 - val_loss: 0.1368\n",
      "Epoch 22/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0892 - val_loss: 0.0503\n",
      "Epoch 23/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1021 - val_loss: 0.1250\n",
      "Epoch 24/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0848 - val_loss: 0.0478\n",
      "Epoch 25/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0976 - val_loss: 0.1096\n",
      "Epoch 26/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0809 - val_loss: 0.0473\n",
      "Epoch 27/1000\n",
      "103/103 [==============================] - 0s 646us/step - loss: 0.0920 - val_loss: 0.0859\n",
      "Epoch 28/1000\n",
      "103/103 [==============================] - 0s 554us/step - loss: 0.0785 - val_loss: 0.0519\n",
      "Epoch 29/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.0648\n",
      "Epoch 30/1000\n",
      "103/103 [==============================] - 0s 850us/step - loss: 0.0798 - val_loss: 0.0609\n",
      "Epoch 31/1000\n",
      "103/103 [==============================] - 0s 749us/step - loss: 0.0855 - val_loss: 0.1403\n",
      "Epoch 32/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0857 - val_loss: 0.0411\n",
      "Epoch 33/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0991 - val_loss: 0.0974\n",
      "Epoch 34/1000\n",
      "103/103 [==============================] - 0s 460us/step - loss: 0.0770 - val_loss: 0.0375\n",
      "Epoch 35/1000\n",
      "103/103 [==============================] - 0s 395us/step - loss: 0.0939 - val_loss: 0.1143\n",
      "Epoch 36/1000\n",
      "103/103 [==============================] - 0s 568us/step - loss: 0.0743 - val_loss: 0.0372\n",
      "Epoch 37/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0865 - val_loss: 0.1017\n",
      "Epoch 38/1000\n",
      "103/103 [==============================] - 0s 992us/step - loss: 0.0727 - val_loss: 0.0369\n",
      "Epoch 39/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0856 - val_loss: 0.0953\n",
      "Epoch 40/1000\n",
      "103/103 [==============================] - 0s 914us/step - loss: 0.0722 - val_loss: 0.0386\n",
      "Epoch 41/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0811 - val_loss: 0.0899\n",
      "Epoch 42/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0729 - val_loss: 0.0400\n",
      "Epoch 43/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0776 - val_loss: 0.0812\n",
      "Epoch 44/1000\n",
      "103/103 [==============================] - 0s 508us/step - loss: 0.0716 - val_loss: 0.0413\n",
      "Epoch 45/1000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0769 - val_loss: 0.0688\n",
      "Epoch 46/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0700 - val_loss: 0.0409\n",
      "Epoch 47/1000\n",
      "103/103 [==============================] - 0s 445us/step - loss: 0.0794 - val_loss: 0.0830\n",
      "Epoch 48/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0720 - val_loss: 0.0389\n",
      "Epoch 49/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0780 - val_loss: 0.0681\n",
      "Epoch 50/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0687 - val_loss: 0.0349\n",
      "Epoch 51/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0885 - val_loss: 0.0945\n",
      "Epoch 52/1000\n",
      "103/103 [==============================] - 0s 758us/step - loss: 0.0678 - val_loss: 0.0371\n",
      "Epoch 53/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0778 - val_loss: 0.0915\n",
      "Epoch 54/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0687 - val_loss: 0.0333\n",
      "Epoch 55/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0814 - val_loss: 0.1019\n",
      "Epoch 56/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0693 - val_loss: 0.0337\n",
      "Epoch 57/1000\n",
      "103/103 [==============================] - 0s 777us/step - loss: 0.0783 - val_loss: 0.1087\n",
      "Epoch 58/1000\n",
      "103/103 [==============================] - 0s 838us/step - loss: 0.0687 - val_loss: 0.0334\n",
      "Epoch 59/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0768 - val_loss: 0.0984\n",
      "Epoch 60/1000\n",
      "103/103 [==============================] - 0s 630us/step - loss: 0.0678 - val_loss: 0.0330\n",
      "Epoch 61/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0787 - val_loss: 0.1054\n",
      "Epoch 62/1000\n",
      "103/103 [==============================] - 0s 737us/step - loss: 0.0674 - val_loss: 0.0335\n",
      "Epoch 63/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0771 - val_loss: 0.1041\n",
      "Epoch 64/1000\n",
      "103/103 [==============================] - 0s 610us/step - loss: 0.0672 - val_loss: 0.0325\n",
      "Epoch 65/1000\n",
      "103/103 [==============================] - 0s 439us/step - loss: 0.0775 - val_loss: 0.1050\n",
      "Epoch 66/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0669 - val_loss: 0.0326\n",
      "Epoch 67/1000\n",
      "103/103 [==============================] - 0s 488us/step - loss: 0.0769 - val_loss: 0.1074\n",
      "Epoch 68/1000\n",
      "103/103 [==============================] - 0s 483us/step - loss: 0.0666 - val_loss: 0.0325\n",
      "Epoch 69/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0762 - val_loss: 0.1062\n",
      "Epoch 70/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0664 - val_loss: 0.0321\n",
      "Epoch 71/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0764 - val_loss: 0.1093\n",
      "Epoch 72/1000\n",
      "103/103 [==============================] - 0s 555us/step - loss: 0.0661 - val_loss: 0.0320\n",
      "Epoch 73/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0759 - val_loss: 0.1096\n",
      "Epoch 74/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0659 - val_loss: 0.0317\n",
      "Epoch 75/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0757 - val_loss: 0.1106\n",
      "Epoch 76/1000\n",
      "103/103 [==============================] - 0s 700us/step - loss: 0.0657 - val_loss: 0.0316\n",
      "Epoch 77/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0754 - val_loss: 0.1125\n",
      "Epoch 78/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0655 - val_loss: 0.0314\n",
      "Epoch 79/1000\n",
      "103/103 [==============================] - 0s 512us/step - loss: 0.0751 - val_loss: 0.1134\n",
      "Epoch 80/1000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0654 - val_loss: 0.0312\n",
      "Epoch 81/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0749 - val_loss: 0.1152\n",
      "Epoch 82/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0653 - val_loss: 0.0311\n",
      "Epoch 83/1000\n",
      "103/103 [==============================] - 0s 834us/step - loss: 0.0746 - val_loss: 0.1165\n",
      "Epoch 84/1000\n",
      "103/103 [==============================] - 0s 503us/step - loss: 0.0651 - val_loss: 0.0310\n",
      "Epoch 85/1000\n",
      "103/103 [==============================] - 0s 407us/step - loss: 0.0742 - val_loss: 0.1177\n",
      "Epoch 86/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0650 - val_loss: 0.0309\n",
      "Epoch 87/1000\n",
      "103/103 [==============================] - 0s 800us/step - loss: 0.0738 - val_loss: 0.1190\n",
      "Epoch 88/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0649 - val_loss: 0.0309\n",
      "Epoch 89/1000\n",
      "103/103 [==============================] - 0s 412us/step - loss: 0.0734 - val_loss: 0.1198\n",
      "Epoch 90/1000\n",
      "103/103 [==============================] - 0s 445us/step - loss: 0.0648 - val_loss: 0.0308\n",
      "Epoch 91/1000\n",
      "103/103 [==============================] - 0s 412us/step - loss: 0.0730 - val_loss: 0.1204\n",
      "Epoch 92/1000\n",
      "103/103 [==============================] - 0s 530us/step - loss: 0.0647 - val_loss: 0.0308\n",
      "Epoch 93/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0725 - val_loss: 0.1206\n",
      "Epoch 94/1000\n",
      "103/103 [==============================] - 0s 479us/step - loss: 0.0645 - val_loss: 0.0308\n",
      "Epoch 95/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0720 - val_loss: 0.1205\n",
      "Epoch 96/1000\n",
      "103/103 [==============================] - 0s 442us/step - loss: 0.0643 - val_loss: 0.0309\n",
      "Epoch 97/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0715 - val_loss: 0.1204\n",
      "Epoch 98/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0642 - val_loss: 0.0309\n",
      "Epoch 99/1000\n",
      "103/103 [==============================] - 0s 412us/step - loss: 0.0710 - val_loss: 0.1200\n",
      "Epoch 100/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0639 - val_loss: 0.0309\n",
      "Epoch 101/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0705 - val_loss: 0.1196\n",
      "Epoch 102/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0637 - val_loss: 0.0309\n",
      "Epoch 103/1000\n",
      "103/103 [==============================] - 0s 408us/step - loss: 0.0700 - val_loss: 0.1191\n",
      "Epoch 104/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0635 - val_loss: 0.0309\n",
      "Epoch 105/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0695 - val_loss: 0.1187\n",
      "Epoch 106/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0633 - val_loss: 0.0309\n",
      "Epoch 107/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0691 - val_loss: 0.1182\n",
      "Epoch 108/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0631 - val_loss: 0.0309\n",
      "Epoch 109/1000\n",
      "103/103 [==============================] - 0s 491us/step - loss: 0.0686 - val_loss: 0.1177\n",
      "Epoch 110/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0629 - val_loss: 0.0308\n",
      "Epoch 111/1000\n",
      "103/103 [==============================] - 0s 410us/step - loss: 0.0681 - val_loss: 0.1172\n",
      "Epoch 112/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0626 - val_loss: 0.0308\n",
      "Epoch 113/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0676 - val_loss: 0.1167\n",
      "Epoch 114/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0624 - val_loss: 0.0307\n",
      "Epoch 115/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0671 - val_loss: 0.1161\n",
      "Epoch 116/1000\n",
      "103/103 [==============================] - 0s 456us/step - loss: 0.0622 - val_loss: 0.0307\n",
      "Epoch 117/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0666 - val_loss: 0.1155\n",
      "Epoch 118/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0619 - val_loss: 0.0306\n",
      "Epoch 119/1000\n",
      "103/103 [==============================] - 0s 401us/step - loss: 0.0661 - val_loss: 0.1149\n",
      "Epoch 120/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0617 - val_loss: 0.0305\n",
      "Epoch 121/1000\n",
      "103/103 [==============================] - 0s 407us/step - loss: 0.0656 - val_loss: 0.1144\n",
      "Epoch 122/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0614 - val_loss: 0.0305\n",
      "Epoch 123/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0651 - val_loss: 0.1139\n",
      "Epoch 124/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0611 - val_loss: 0.0304\n",
      "Epoch 125/1000\n",
      "103/103 [==============================] - 0s 401us/step - loss: 0.0646 - val_loss: 0.1133\n",
      "Epoch 126/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0609 - val_loss: 0.0304\n",
      "Epoch 127/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0640 - val_loss: 0.1127\n",
      "Epoch 128/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0606 - val_loss: 0.0304\n",
      "Epoch 129/1000\n",
      "103/103 [==============================] - 0s 399us/step - loss: 0.0635 - val_loss: 0.1117\n",
      "Epoch 130/1000\n",
      "103/103 [==============================] - 0s 409us/step - loss: 0.0603 - val_loss: 0.0304\n",
      "Epoch 131/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0629 - val_loss: 0.1110\n",
      "Epoch 132/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0599 - val_loss: 0.0304\n",
      "Epoch 133/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0624 - val_loss: 0.1113\n",
      "Epoch 134/1000\n",
      "103/103 [==============================] - 0s 439us/step - loss: 0.0596 - val_loss: 0.0303\n",
      "Epoch 135/1000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0622 - val_loss: 0.1129\n",
      "Epoch 136/1000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0592 - val_loss: 0.0302\n",
      "Epoch 137/1000\n",
      "103/103 [==============================] - 0s 469us/step - loss: 0.0620 - val_loss: 0.1134\n",
      "Epoch 138/1000\n",
      "103/103 [==============================] - 0s 509us/step - loss: 0.0588 - val_loss: 0.0302\n",
      "Epoch 139/1000\n",
      "103/103 [==============================] - 0s 475us/step - loss: 0.0616 - val_loss: 0.1041\n",
      "Epoch 140/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0585 - val_loss: 0.0339\n",
      "Epoch 141/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0586 - val_loss: 0.0769\n",
      "Epoch 142/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0566 - val_loss: 0.0570\n",
      "Epoch 143/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0523 - val_loss: 0.0417\n",
      "Epoch 144/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0533 - val_loss: 0.0683\n",
      "Epoch 145/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0548 - val_loss: 0.0567\n",
      "Epoch 146/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0603 - val_loss: 0.2538\n",
      "Epoch 147/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0784 - val_loss: 0.0303\n",
      "Epoch 148/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0578 - val_loss: 0.0936\n",
      "Epoch 149/1000\n",
      "103/103 [==============================] - 0s 476us/step - loss: 0.0543 - val_loss: 0.0312\n",
      "Epoch 150/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0597 - val_loss: 0.0696\n",
      "Epoch 151/1000\n",
      "103/103 [==============================] - 0s 479us/step - loss: 0.0523 - val_loss: 0.0372\n",
      "Epoch 152/1000\n",
      "103/103 [==============================] - 0s 479us/step - loss: 0.0557 - val_loss: 0.0993\n",
      "Epoch 153/1000\n",
      "103/103 [==============================] - 0s 508us/step - loss: 0.0577 - val_loss: 0.0383\n",
      "Epoch 154/1000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0528 - val_loss: 0.0784\n",
      "Epoch 155/1000\n",
      "103/103 [==============================] - 0s 460us/step - loss: 0.0557 - val_loss: 0.0516\n",
      "Epoch 156/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 481us/step - loss: 0.0537 - val_loss: 0.0312\n",
      "Epoch 157/1000\n",
      "103/103 [==============================] - 0s 470us/step - loss: 0.0612 - val_loss: 0.1195\n",
      "Epoch 158/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0585 - val_loss: 0.1036\n",
      "Epoch 159/1000\n",
      "103/103 [==============================] - 0s 501us/step - loss: 0.0598 - val_loss: 0.0307\n",
      "Epoch 160/1000\n",
      "103/103 [==============================] - 0s 475us/step - loss: 0.0623 - val_loss: 0.1103\n",
      "Epoch 161/1000\n",
      "103/103 [==============================] - 0s 473us/step - loss: 0.0554 - val_loss: 0.0346\n",
      "Epoch 162/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0546 - val_loss: 0.1149\n",
      "Epoch 163/1000\n",
      "103/103 [==============================] - 0s 467us/step - loss: 0.0544 - val_loss: 0.0325\n",
      "Epoch 164/1000\n",
      "103/103 [==============================] - 0s 440us/step - loss: 0.0555 - val_loss: 0.0898\n",
      "Epoch 165/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0530 - val_loss: 0.0350\n",
      "Epoch 166/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0579 - val_loss: 0.1503\n",
      "Epoch 167/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0586 - val_loss: 0.0288\n",
      "Epoch 168/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0581 - val_loss: 0.1098\n",
      "Epoch 169/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0555 - val_loss: 0.0324\n",
      "Epoch 170/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0584 - val_loss: 0.0732\n",
      "Epoch 171/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0536 - val_loss: 0.0511\n",
      "Epoch 172/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0489 - val_loss: 0.0587\n",
      "Epoch 173/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0490 - val_loss: 0.0666\n",
      "Epoch 174/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0507 - val_loss: 0.0394\n",
      "Epoch 175/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0533 - val_loss: 0.0364\n",
      "Epoch 176/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0594 - val_loss: 0.2804\n",
      "Epoch 177/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0693 - val_loss: 0.0288\n",
      "Epoch 178/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0644 - val_loss: 0.1017\n",
      "Epoch 179/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0542 - val_loss: 0.0350\n",
      "Epoch 180/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0498 - val_loss: 0.0753\n",
      "Epoch 181/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0486 - val_loss: 0.0399\n",
      "Epoch 182/1000\n",
      "103/103 [==============================] - 0s 408us/step - loss: 0.0489 - val_loss: 0.0724\n",
      "Epoch 183/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0497 - val_loss: 0.0781\n",
      "Epoch 184/1000\n",
      "103/103 [==============================] - 0s 408us/step - loss: 0.0520 - val_loss: 0.0396\n",
      "Epoch 185/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0535 - val_loss: 0.0441\n",
      "Epoch 186/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0522 - val_loss: 0.1268\n",
      "Epoch 187/1000\n",
      "103/103 [==============================] - 0s 407us/step - loss: 0.0563 - val_loss: 0.1250\n",
      "Epoch 188/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0664 - val_loss: 0.0357\n",
      "Epoch 189/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0664 - val_loss: 0.1067\n",
      "Epoch 190/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0501 - val_loss: 0.0425\n",
      "Epoch 191/1000\n",
      "103/103 [==============================] - 0s 409us/step - loss: 0.0491 - val_loss: 0.0648\n",
      "Epoch 192/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0480 - val_loss: 0.0405\n",
      "Epoch 193/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0496 - val_loss: 0.0653\n",
      "Epoch 194/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0486 - val_loss: 0.0719\n",
      "Epoch 195/1000\n",
      "103/103 [==============================] - 0s 454us/step - loss: 0.0487 - val_loss: 0.0571\n",
      "Epoch 196/1000\n",
      "103/103 [==============================] - 0s 451us/step - loss: 0.0558 - val_loss: 0.0308\n",
      "Epoch 197/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0566 - val_loss: 0.1222\n",
      "Epoch 198/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0520 - val_loss: 0.0691\n",
      "Epoch 199/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0492 - val_loss: 0.0482\n",
      "Epoch 200/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0496 - val_loss: 0.0539\n",
      "Epoch 201/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0484 - val_loss: 0.0368\n",
      "Epoch 202/1000\n",
      "103/103 [==============================] - 0s 457us/step - loss: 0.0533 - val_loss: 0.1543\n",
      "Epoch 203/1000\n",
      "103/103 [==============================] - 0s 456us/step - loss: 0.0556 - val_loss: 0.0384\n",
      "Epoch 204/1000\n",
      "103/103 [==============================] - 0s 445us/step - loss: 0.0493 - val_loss: 0.0851\n",
      "Epoch 205/1000\n",
      "103/103 [==============================] - 0s 478us/step - loss: 0.0553 - val_loss: 0.0317\n",
      "Epoch 206/1000\n",
      "103/103 [==============================] - 0s 469us/step - loss: 0.0645 - val_loss: 0.1012\n",
      "Epoch 207/1000\n",
      "103/103 [==============================] - 0s 482us/step - loss: 0.0523 - val_loss: 0.0381\n",
      "Epoch 208/1000\n",
      "103/103 [==============================] - 0s 956us/step - loss: 0.0495 - val_loss: 0.0752\n",
      "Epoch 209/1000\n",
      "103/103 [==============================] - 0s 482us/step - loss: 0.0481 - val_loss: 0.0387\n",
      "Epoch 210/1000\n",
      "103/103 [==============================] - 0s 865us/step - loss: 0.0480 - val_loss: 0.0890\n",
      "Epoch 211/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0502 - val_loss: 0.0468\n",
      "Epoch 212/1000\n",
      "103/103 [==============================] - 0s 439us/step - loss: 0.0498 - val_loss: 0.0679\n",
      "Epoch 213/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0547 - val_loss: 0.0342\n",
      "Epoch 214/1000\n",
      "103/103 [==============================] - 0s 445us/step - loss: 0.0636 - val_loss: 0.1428\n",
      "Epoch 215/1000\n",
      "103/103 [==============================] - 0s 443us/step - loss: 0.0499 - val_loss: 0.0398\n",
      "Epoch 216/1000\n",
      "103/103 [==============================] - 0s 500us/step - loss: 0.0492 - val_loss: 0.0719\n",
      "Epoch 217/1000\n",
      "103/103 [==============================] - 0s 473us/step - loss: 0.0478 - val_loss: 0.0335\n",
      "Epoch 218/1000\n",
      "103/103 [==============================] - 0s 472us/step - loss: 0.0542 - val_loss: 0.0956\n",
      "Epoch 219/1000\n",
      "103/103 [==============================] - 0s 481us/step - loss: 0.0518 - val_loss: 0.0350\n",
      "Epoch 220/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0505 - val_loss: 0.0919\n",
      "Epoch 221/1000\n",
      "103/103 [==============================] - 0s 485us/step - loss: 0.0511 - val_loss: 0.0337\n",
      "Epoch 222/1000\n",
      "103/103 [==============================] - 0s 470us/step - loss: 0.0524 - val_loss: 0.0995\n",
      "Epoch 223/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0496 - val_loss: 0.0342\n",
      "Epoch 224/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0545 - val_loss: 0.1154\n",
      "Epoch 225/1000\n",
      "103/103 [==============================] - 0s 480us/step - loss: 0.0508 - val_loss: 0.0300\n",
      "Epoch 226/1000\n",
      "103/103 [==============================] - 0s 476us/step - loss: 0.0557 - val_loss: 0.1096\n",
      "Epoch 227/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0504 - val_loss: 0.0392\n",
      "Epoch 228/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0503 - val_loss: 0.0674\n",
      "Epoch 229/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0473 - val_loss: 0.0520\n",
      "Epoch 230/1000\n",
      "103/103 [==============================] - 0s 494us/step - loss: 0.0441 - val_loss: 0.0752\n",
      "Epoch 231/1000\n",
      "103/103 [==============================] - 0s 439us/step - loss: 0.0496 - val_loss: 0.0397\n",
      "Epoch 232/1000\n",
      "103/103 [==============================] - 0s 463us/step - loss: 0.0580 - val_loss: 0.1090\n",
      "Epoch 233/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0517 - val_loss: 0.0533\n",
      "Epoch 234/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0459 - val_loss: 0.0986\n",
      "Epoch 235/1000\n",
      "103/103 [==============================] - 0s 615us/step - loss: 0.0503 - val_loss: 0.0287\n",
      "Epoch 236/1000\n",
      "103/103 [==============================] - 0s 709us/step - loss: 0.0577 - val_loss: 0.1148\n",
      "Epoch 237/1000\n",
      "103/103 [==============================] - 0s 699us/step - loss: 0.0506 - val_loss: 0.0383\n",
      "Epoch 238/1000\n",
      "103/103 [==============================] - 0s 544us/step - loss: 0.0481 - val_loss: 0.0704\n",
      "Epoch 239/1000\n",
      "103/103 [==============================] - 0s 636us/step - loss: 0.0469 - val_loss: 0.0431\n",
      "Epoch 240/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0440 - val_loss: 0.0630\n",
      "Epoch 241/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0435 - val_loss: 0.0681\n",
      "Epoch 242/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0450 - val_loss: 0.0853\n",
      "Epoch 243/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0624 - val_loss: 0.0343\n",
      "Epoch 244/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0694 - val_loss: 0.1059\n",
      "Epoch 245/1000\n",
      "103/103 [==============================] - 0s 409us/step - loss: 0.0452 - val_loss: 0.0589\n",
      "Epoch 246/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0440 - val_loss: 0.0492\n",
      "Epoch 247/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0430 - val_loss: 0.0581\n",
      "Epoch 248/1000\n",
      "103/103 [==============================] - 0s 463us/step - loss: 0.0427 - val_loss: 0.0806\n",
      "Epoch 249/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0452 - val_loss: 0.0783\n",
      "Epoch 250/1000\n",
      "103/103 [==============================] - 0s 459us/step - loss: 0.0445 - val_loss: 0.0493\n",
      "Epoch 251/1000\n",
      "103/103 [==============================] - 0s 462us/step - loss: 0.0542 - val_loss: 0.0313\n",
      "Epoch 252/1000\n",
      "103/103 [==============================] - 0s 466us/step - loss: 0.0507 - val_loss: 0.0915\n",
      "Epoch 253/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0452 - val_loss: 0.0568\n",
      "Epoch 254/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0414 - val_loss: 0.0423\n",
      "Epoch 255/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0465 - val_loss: 0.0302\n",
      "Epoch 256/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0624 - val_loss: 0.1214\n",
      "Epoch 257/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0469 - val_loss: 0.0535\n",
      "Epoch 258/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0448 - val_loss: 0.0490\n",
      "Epoch 259/1000\n",
      "103/103 [==============================] - 0s 655us/step - loss: 0.0426 - val_loss: 0.0642\n",
      "Epoch 260/1000\n",
      "103/103 [==============================] - 0s 452us/step - loss: 0.0453 - val_loss: 0.0980\n",
      "Epoch 261/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0453 - val_loss: 0.0369\n",
      "Epoch 262/1000\n",
      "103/103 [==============================] - 0s 410us/step - loss: 0.0433 - val_loss: 0.0549\n",
      "Epoch 263/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0471 - val_loss: 0.0324\n",
      "Epoch 264/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0580 - val_loss: 0.1165\n",
      "Epoch 265/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0481 - val_loss: 0.0281\n",
      "Epoch 266/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0521 - val_loss: 0.0940\n",
      "Epoch 267/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0424 - val_loss: 0.0352\n",
      "Epoch 268/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0446 - val_loss: 0.0914\n",
      "Epoch 269/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0422 - val_loss: 0.0385\n",
      "Epoch 270/1000\n",
      "103/103 [==============================] - 0s 508us/step - loss: 0.0461 - val_loss: 0.0710\n",
      "Epoch 271/1000\n",
      "103/103 [==============================] - 0s 450us/step - loss: 0.0445 - val_loss: 0.0620\n",
      "Epoch 272/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0413 - val_loss: 0.0619\n",
      "Epoch 273/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0464 - val_loss: 0.0294\n",
      "Epoch 274/1000\n",
      "103/103 [==============================] - 0s 582us/step - loss: 0.0538 - val_loss: 0.1047\n",
      "Epoch 275/1000\n",
      "103/103 [==============================] - 0s 476us/step - loss: 0.0402 - val_loss: 0.0460\n",
      "Epoch 276/1000\n",
      "103/103 [==============================] - 0s 439us/step - loss: 0.0403 - val_loss: 0.0680\n",
      "Epoch 277/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0400 - val_loss: 0.0311\n",
      "Epoch 278/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0458 - val_loss: 0.1382\n",
      "Epoch 279/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0480 - val_loss: 0.0267\n",
      "Epoch 280/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0437 - val_loss: 0.0941\n",
      "Epoch 281/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0409 - val_loss: 0.0387\n",
      "Epoch 282/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0421 - val_loss: 0.0527\n",
      "Epoch 283/1000\n",
      "103/103 [==============================] - 0s 442us/step - loss: 0.0391 - val_loss: 0.0458\n",
      "Epoch 284/1000\n",
      "103/103 [==============================] - 0s 470us/step - loss: 0.0361 - val_loss: 0.0876\n",
      "Epoch 285/1000\n",
      "103/103 [==============================] - 0s 550us/step - loss: 0.0362 - val_loss: 0.0360\n",
      "Epoch 286/1000\n",
      "103/103 [==============================] - 0s 496us/step - loss: 0.0373 - val_loss: 0.0785\n",
      "Epoch 287/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0464 - val_loss: 0.0313\n",
      "Epoch 288/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0473 - val_loss: 0.1386\n",
      "Epoch 289/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0394 - val_loss: 0.0269\n",
      "Epoch 290/1000\n",
      "103/103 [==============================] - 0s 443us/step - loss: 0.0406 - val_loss: 0.0818\n",
      "Epoch 291/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0353 - val_loss: 0.0481\n",
      "Epoch 292/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0372 - val_loss: 0.0656\n",
      "Epoch 293/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0356 - val_loss: 0.0398\n",
      "Epoch 294/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0368 - val_loss: 0.1271\n",
      "Epoch 295/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0415 - val_loss: 0.0251\n",
      "Epoch 296/1000\n",
      "103/103 [==============================] - 0s 463us/step - loss: 0.0359 - val_loss: 0.0895\n",
      "Epoch 297/1000\n",
      "103/103 [==============================] - 0s 614us/step - loss: 0.0346 - val_loss: 0.0315\n",
      "Epoch 298/1000\n",
      "103/103 [==============================] - 0s 439us/step - loss: 0.0423 - val_loss: 0.0944\n",
      "Epoch 299/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0353 - val_loss: 0.0364\n",
      "Epoch 300/1000\n",
      "103/103 [==============================] - 0s 451us/step - loss: 0.0416 - val_loss: 0.0823\n",
      "Epoch 301/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0348 - val_loss: 0.0466\n",
      "Epoch 302/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0302 - val_loss: 0.0799\n",
      "Epoch 303/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0328 - val_loss: 0.0259\n",
      "Epoch 304/1000\n",
      "103/103 [==============================] - 0s 633us/step - loss: 0.0383 - val_loss: 0.1007\n",
      "Epoch 305/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0307 - val_loss: 0.0278\n",
      "Epoch 306/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0377 - val_loss: 0.1030\n",
      "Epoch 307/1000\n",
      "103/103 [==============================] - 0s 575us/step - loss: 0.0316 - val_loss: 0.0302\n",
      "Epoch 308/1000\n",
      "103/103 [==============================] - 0s 484us/step - loss: 0.0357 - val_loss: 0.0940\n",
      "Epoch 309/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0420 - val_loss: 0.0469\n",
      "Epoch 310/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 424us/step - loss: 0.0310 - val_loss: 0.0687\n",
      "Epoch 311/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0307 - val_loss: 0.0300\n",
      "Epoch 312/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0373 - val_loss: 0.0946\n",
      "Epoch 313/1000\n",
      "103/103 [==============================] - 0s 408us/step - loss: 0.0280 - val_loss: 0.0334\n",
      "Epoch 314/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0306 - val_loss: 0.0712\n",
      "Epoch 315/1000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0298 - val_loss: 0.0376\n",
      "Epoch 316/1000\n",
      "103/103 [==============================] - 0s 477us/step - loss: 0.0331 - val_loss: 0.1145\n",
      "Epoch 317/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0390 - val_loss: 0.0234\n",
      "Epoch 318/1000\n",
      "103/103 [==============================] - 0s 400us/step - loss: 0.0348 - val_loss: 0.0838\n",
      "Epoch 319/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0308 - val_loss: 0.0333\n",
      "Epoch 320/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0417 - val_loss: 0.1049\n",
      "Epoch 321/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0290 - val_loss: 0.0331\n",
      "Epoch 322/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0294 - val_loss: 0.0705\n",
      "Epoch 323/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0253 - val_loss: 0.0549\n",
      "Epoch 324/1000\n",
      "103/103 [==============================] - 0s 464us/step - loss: 0.0260 - val_loss: 0.0543\n",
      "Epoch 325/1000\n",
      "103/103 [==============================] - 0s 443us/step - loss: 0.0269 - val_loss: 0.0525\n",
      "Epoch 326/1000\n",
      "103/103 [==============================] - 0s 556us/step - loss: 0.0301 - val_loss: 0.0752\n",
      "Epoch 327/1000\n",
      "103/103 [==============================] - 0s 469us/step - loss: 0.0357 - val_loss: 0.0257\n",
      "Epoch 328/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0398 - val_loss: 0.1286\n",
      "Epoch 329/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0283 - val_loss: 0.0340\n",
      "Epoch 330/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0298 - val_loss: 0.0702\n",
      "Epoch 331/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0278 - val_loss: 0.0595\n",
      "Epoch 332/1000\n",
      "103/103 [==============================] - 0s 478us/step - loss: 0.0270 - val_loss: 0.0634\n",
      "Epoch 333/1000\n",
      "103/103 [==============================] - 0s 458us/step - loss: 0.0294 - val_loss: 0.0224\n",
      "Epoch 334/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0377 - val_loss: 0.1049\n",
      "Epoch 335/1000\n",
      "103/103 [==============================] - 0s 515us/step - loss: 0.0257 - val_loss: 0.0365\n",
      "Epoch 336/1000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0265 - val_loss: 0.0715\n",
      "Epoch 337/1000\n",
      "103/103 [==============================] - 0s 460us/step - loss: 0.0267 - val_loss: 0.0448\n",
      "Epoch 338/1000\n",
      "103/103 [==============================] - 0s 481us/step - loss: 0.0289 - val_loss: 0.1034\n",
      "Epoch 339/1000\n",
      "103/103 [==============================] - 0s 585us/step - loss: 0.0338 - val_loss: 0.0236\n",
      "Epoch 340/1000\n",
      "103/103 [==============================] - 0s 442us/step - loss: 0.0291 - val_loss: 0.0804\n",
      "Epoch 341/1000\n",
      "103/103 [==============================] - 0s 860us/step - loss: 0.0250 - val_loss: 0.0273\n",
      "Epoch 342/1000\n",
      "103/103 [==============================] - 0s 602us/step - loss: 0.0313 - val_loss: 0.0869\n",
      "Epoch 343/1000\n",
      "103/103 [==============================] - 0s 565us/step - loss: 0.0241 - val_loss: 0.0349\n",
      "Epoch 344/1000\n",
      "103/103 [==============================] - 0s 459us/step - loss: 0.0305 - val_loss: 0.0844\n",
      "Epoch 345/1000\n",
      "103/103 [==============================] - 0s 466us/step - loss: 0.0302 - val_loss: 0.0547\n",
      "Epoch 346/1000\n",
      "103/103 [==============================] - 0s 485us/step - loss: 0.0252 - val_loss: 0.0830\n",
      "Epoch 347/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0285 - val_loss: 0.0271\n",
      "Epoch 348/1000\n",
      "103/103 [==============================] - 0s 572us/step - loss: 0.0269 - val_loss: 0.0904\n",
      "Epoch 349/1000\n",
      "103/103 [==============================] - 0s 476us/step - loss: 0.0277 - val_loss: 0.0311\n",
      "Epoch 350/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0362 - val_loss: 0.0932\n",
      "Epoch 351/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0259 - val_loss: 0.0464\n",
      "Epoch 352/1000\n",
      "103/103 [==============================] - 0s 455us/step - loss: 0.0265 - val_loss: 0.0622\n",
      "Epoch 353/1000\n",
      "103/103 [==============================] - 0s 494us/step - loss: 0.0219 - val_loss: 0.0503\n",
      "Epoch 354/1000\n",
      "103/103 [==============================] - 0s 480us/step - loss: 0.0296 - val_loss: 0.1088\n",
      "Epoch 355/1000\n",
      "103/103 [==============================] - 0s 458us/step - loss: 0.0272 - val_loss: 0.0282\n",
      "Epoch 356/1000\n",
      "103/103 [==============================] - 0s 466us/step - loss: 0.0254 - val_loss: 0.0942\n",
      "Epoch 357/1000\n",
      "103/103 [==============================] - 0s 475us/step - loss: 0.0312 - val_loss: 0.0384\n",
      "Epoch 358/1000\n",
      "103/103 [==============================] - 0s 485us/step - loss: 0.0300 - val_loss: 0.0825\n",
      "Epoch 359/1000\n",
      "103/103 [==============================] - 0s 571us/step - loss: 0.0258 - val_loss: 0.0257\n",
      "Epoch 360/1000\n",
      "103/103 [==============================] - 0s 467us/step - loss: 0.0277 - val_loss: 0.0986\n",
      "Epoch 361/1000\n",
      "103/103 [==============================] - 0s 498us/step - loss: 0.0215 - val_loss: 0.0372\n",
      "Epoch 362/1000\n",
      "103/103 [==============================] - 0s 454us/step - loss: 0.0267 - val_loss: 0.0795\n",
      "Epoch 363/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0254 - val_loss: 0.0230\n",
      "Epoch 364/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0298 - val_loss: 0.1314\n",
      "Epoch 365/1000\n",
      "103/103 [==============================] - 0s 483us/step - loss: 0.0276 - val_loss: 0.0452\n",
      "Epoch 366/1000\n",
      "103/103 [==============================] - 0s 442us/step - loss: 0.0299 - val_loss: 0.0688\n",
      "Epoch 367/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0220 - val_loss: 0.0445\n",
      "Epoch 368/1000\n",
      "103/103 [==============================] - 0s 464us/step - loss: 0.0255 - val_loss: 0.0664\n",
      "Epoch 369/1000\n",
      "103/103 [==============================] - 0s 675us/step - loss: 0.0217 - val_loss: 0.0297\n",
      "Epoch 370/1000\n",
      "103/103 [==============================] - 0s 490us/step - loss: 0.0316 - val_loss: 0.1103\n",
      "Epoch 371/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0256 - val_loss: 0.0231\n",
      "Epoch 372/1000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0282 - val_loss: 0.1049\n",
      "Epoch 373/1000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0248 - val_loss: 0.0315\n",
      "Epoch 374/1000\n",
      "103/103 [==============================] - 0s 468us/step - loss: 0.0233 - val_loss: 0.0763\n",
      "Epoch 375/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0249 - val_loss: 0.0706\n",
      "Epoch 376/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0220 - val_loss: 0.0354\n",
      "Epoch 377/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0279 - val_loss: 0.0806\n",
      "Epoch 378/1000\n",
      "103/103 [==============================] - 0s 473us/step - loss: 0.0213 - val_loss: 0.0629\n",
      "Epoch 379/1000\n",
      "103/103 [==============================] - 0s 470us/step - loss: 0.0252 - val_loss: 0.0752\n",
      "Epoch 380/1000\n",
      "103/103 [==============================] - 0s 516us/step - loss: 0.0284 - val_loss: 0.0187\n",
      "Epoch 381/1000\n",
      "103/103 [==============================] - 0s 406us/step - loss: 0.0329 - val_loss: 0.1261\n",
      "Epoch 382/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0237 - val_loss: 0.0380\n",
      "Epoch 383/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0195 - val_loss: 0.0622\n",
      "Epoch 384/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0225 - val_loss: 0.0409\n",
      "Epoch 385/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0212 - val_loss: 0.0584\n",
      "Epoch 386/1000\n",
      "103/103 [==============================] - 0s 649us/step - loss: 0.0249 - val_loss: 0.1017\n",
      "Epoch 387/1000\n",
      "103/103 [==============================] - 0s 456us/step - loss: 0.0246 - val_loss: 0.0416\n",
      "Epoch 388/1000\n",
      "103/103 [==============================] - 0s 493us/step - loss: 0.0243 - val_loss: 0.0288\n",
      "Epoch 389/1000\n",
      "103/103 [==============================] - 0s 588us/step - loss: 0.0312 - val_loss: 0.1554\n",
      "Epoch 390/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0245 - val_loss: 0.0260\n",
      "Epoch 391/1000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0213 - val_loss: 0.0591\n",
      "Epoch 392/1000\n",
      "103/103 [==============================] - 0s 479us/step - loss: 0.0223 - val_loss: 0.0785\n",
      "Epoch 393/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0216 - val_loss: 0.0542\n",
      "Epoch 394/1000\n",
      "103/103 [==============================] - 0s 539us/step - loss: 0.0241 - val_loss: 0.0241\n",
      "Epoch 395/1000\n",
      "103/103 [==============================] - 0s 499us/step - loss: 0.0288 - val_loss: 0.1215\n",
      "Epoch 396/1000\n",
      "103/103 [==============================] - 0s 472us/step - loss: 0.0208 - val_loss: 0.0321\n",
      "Epoch 397/1000\n",
      "103/103 [==============================] - 0s 579us/step - loss: 0.0207 - val_loss: 0.0703\n",
      "Epoch 398/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0258 - val_loss: 0.0570\n",
      "Epoch 399/1000\n",
      "103/103 [==============================] - 0s 466us/step - loss: 0.0257 - val_loss: 0.0928\n",
      "Epoch 400/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0245 - val_loss: 0.0244\n",
      "Epoch 401/1000\n",
      "103/103 [==============================] - 0s 569us/step - loss: 0.0275 - val_loss: 0.1118\n",
      "Epoch 402/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0209 - val_loss: 0.0318\n",
      "Epoch 403/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0268 - val_loss: 0.0824\n",
      "Epoch 404/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0194 - val_loss: 0.0389\n",
      "Epoch 405/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0214 - val_loss: 0.0937\n",
      "Epoch 406/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0224 - val_loss: 0.0348\n",
      "Epoch 407/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0267 - val_loss: 0.1295\n",
      "Epoch 408/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0288 - val_loss: 0.0194\n",
      "Epoch 409/1000\n",
      "103/103 [==============================] - 0s 534us/step - loss: 0.0228 - val_loss: 0.0972\n",
      "Epoch 410/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0203 - val_loss: 0.0438\n",
      "Epoch 411/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0229 - val_loss: 0.0640\n",
      "Epoch 412/1000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0233 - val_loss: 0.0370\n",
      "Epoch 413/1000\n",
      "103/103 [==============================] - 0s 440us/step - loss: 0.0189 - val_loss: 0.0777\n",
      "Epoch 414/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0203 - val_loss: 0.0814\n",
      "Epoch 415/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0211 - val_loss: 0.0502\n",
      "Epoch 416/1000\n",
      "103/103 [==============================] - 0s 451us/step - loss: 0.0220 - val_loss: 0.0193\n",
      "Epoch 417/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0292 - val_loss: 0.1627\n",
      "Epoch 418/1000\n",
      "103/103 [==============================] - 0s 630us/step - loss: 0.0228 - val_loss: 0.0291\n",
      "Epoch 419/1000\n",
      "103/103 [==============================] - 0s 798us/step - loss: 0.0211 - val_loss: 0.0543\n",
      "Epoch 420/1000\n",
      "103/103 [==============================] - 0s 900us/step - loss: 0.0204 - val_loss: 0.0840\n",
      "Epoch 421/1000\n",
      "103/103 [==============================] - 0s 595us/step - loss: 0.0188 - val_loss: 0.0472\n",
      "Epoch 422/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0219 - val_loss: 0.0311\n",
      "Epoch 423/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0212 - val_loss: 0.1052\n",
      "Epoch 424/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0207 - val_loss: 0.0414\n",
      "Epoch 425/1000\n",
      "103/103 [==============================] - 0s 458us/step - loss: 0.0178 - val_loss: 0.0740\n",
      "Epoch 426/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0287 - val_loss: 0.0405\n",
      "Epoch 427/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0286 - val_loss: 0.1187\n",
      "Epoch 428/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0218 - val_loss: 0.0292\n",
      "Epoch 429/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0239 - val_loss: 0.0945\n",
      "Epoch 430/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0175 - val_loss: 0.0357\n",
      "Epoch 431/1000\n",
      "103/103 [==============================] - 0s 449us/step - loss: 0.0220 - val_loss: 0.0853\n",
      "Epoch 432/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0183 - val_loss: 0.0297\n",
      "Epoch 433/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0211 - val_loss: 0.1104\n",
      "Epoch 434/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0226 - val_loss: 0.0310\n",
      "Epoch 435/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0248 - val_loss: 0.1028\n",
      "Epoch 436/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0237 - val_loss: 0.0340\n",
      "Epoch 437/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0225 - val_loss: 0.0890\n",
      "Epoch 438/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0178 - val_loss: 0.0321\n",
      "Epoch 439/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0209 - val_loss: 0.0931\n",
      "Epoch 440/1000\n",
      "103/103 [==============================] - 0s 412us/step - loss: 0.0159 - val_loss: 0.0372\n",
      "Epoch 441/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0211 - val_loss: 0.0674\n",
      "Epoch 442/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0243 - val_loss: 0.0473\n",
      "Epoch 443/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0250 - val_loss: 0.1313\n",
      "Epoch 444/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0229 - val_loss: 0.0245\n",
      "Epoch 445/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0225 - val_loss: 0.0900\n",
      "Epoch 446/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0162 - val_loss: 0.0419\n",
      "Epoch 447/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0204 - val_loss: 0.0760\n",
      "Epoch 448/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0180 - val_loss: 0.0249\n",
      "Epoch 449/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0228 - val_loss: 0.1167\n",
      "Epoch 450/1000\n",
      "103/103 [==============================] - 0s 452us/step - loss: 0.0200 - val_loss: 0.0440\n",
      "Epoch 451/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0206 - val_loss: 0.0881\n",
      "Epoch 452/1000\n",
      "103/103 [==============================] - 0s 446us/step - loss: 0.0211 - val_loss: 0.0303\n",
      "Epoch 453/1000\n",
      "103/103 [==============================] - 0s 483us/step - loss: 0.0206 - val_loss: 0.0912\n",
      "Epoch 454/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0259\n",
      "Epoch 455/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.1097\n",
      "Epoch 456/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0287\n",
      "Epoch 457/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0246 - val_loss: 0.0882\n",
      "Epoch 458/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0196 - val_loss: 0.0515\n",
      "Epoch 459/1000\n",
      "103/103 [==============================] - 0s 463us/step - loss: 0.0208 - val_loss: 0.1047\n",
      "Epoch 460/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0199 - val_loss: 0.0260\n",
      "Epoch 461/1000\n",
      "103/103 [==============================] - 0s 483us/step - loss: 0.0165 - val_loss: 0.0866\n",
      "Epoch 462/1000\n",
      "103/103 [==============================] - 0s 700us/step - loss: 0.0174 - val_loss: 0.0469\n",
      "Epoch 463/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0687\n",
      "Epoch 464/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0220\n",
      "Epoch 465/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0248 - val_loss: 0.1345\n",
      "Epoch 466/1000\n",
      "103/103 [==============================] - 0s 621us/step - loss: 0.0171 - val_loss: 0.0439\n",
      "Epoch 467/1000\n",
      "103/103 [==============================] - 0s 517us/step - loss: 0.0191 - val_loss: 0.0766\n",
      "Epoch 468/1000\n",
      "103/103 [==============================] - 0s 986us/step - loss: 0.0166 - val_loss: 0.0348\n",
      "Epoch 469/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.1136\n",
      "Epoch 470/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0194\n",
      "Epoch 471/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0958\n",
      "Epoch 472/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0343\n",
      "Epoch 473/1000\n",
      "103/103 [==============================] - 0s 452us/step - loss: 0.0221 - val_loss: 0.0781\n",
      "Epoch 474/1000\n",
      "103/103 [==============================] - 0s 455us/step - loss: 0.0175 - val_loss: 0.0322\n",
      "Epoch 475/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0185 - val_loss: 0.1086\n",
      "Epoch 476/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0184 - val_loss: 0.0476\n",
      "Epoch 477/1000\n",
      "103/103 [==============================] - 0s 454us/step - loss: 0.0179 - val_loss: 0.0832\n",
      "Epoch 478/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0187 - val_loss: 0.0251\n",
      "Epoch 479/1000\n",
      "103/103 [==============================] - 0s 660us/step - loss: 0.0190 - val_loss: 0.1048\n",
      "Epoch 480/1000\n",
      "103/103 [==============================] - 0s 757us/step - loss: 0.0201 - val_loss: 0.0238\n",
      "Epoch 481/1000\n",
      "103/103 [==============================] - 0s 674us/step - loss: 0.0205 - val_loss: 0.1028\n",
      "Epoch 482/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0156 - val_loss: 0.0318\n",
      "Epoch 483/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0191 - val_loss: 0.0787\n",
      "Epoch 484/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0152 - val_loss: 0.0731\n",
      "Epoch 485/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0176 - val_loss: 0.0612\n",
      "Epoch 486/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0162 - val_loss: 0.0406\n",
      "Epoch 487/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0186 - val_loss: 0.1346\n",
      "Epoch 488/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0178 - val_loss: 0.0202\n",
      "Epoch 489/1000\n",
      "103/103 [==============================] - 0s 464us/step - loss: 0.0200 - val_loss: 0.0554\n",
      "Epoch 490/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0169 - val_loss: 0.0782\n",
      "Epoch 491/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0187 - val_loss: 0.0830\n",
      "Epoch 492/1000\n",
      "103/103 [==============================] - 0s 479us/step - loss: 0.0189 - val_loss: 0.0168\n",
      "Epoch 493/1000\n",
      "103/103 [==============================] - 0s 894us/step - loss: 0.0198 - val_loss: 0.1167\n",
      "Epoch 494/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0452\n",
      "Epoch 495/1000\n",
      "103/103 [==============================] - 0s 649us/step - loss: 0.0137 - val_loss: 0.0631\n",
      "Epoch 496/1000\n",
      "103/103 [==============================] - 0s 820us/step - loss: 0.0176 - val_loss: 0.0373\n",
      "Epoch 497/1000\n",
      "103/103 [==============================] - 0s 907us/step - loss: 0.0201 - val_loss: 0.1243\n",
      "Epoch 498/1000\n",
      "103/103 [==============================] - 0s 665us/step - loss: 0.0201 - val_loss: 0.0252\n",
      "Epoch 499/1000\n",
      "103/103 [==============================] - 0s 684us/step - loss: 0.0180 - val_loss: 0.0784\n",
      "Epoch 500/1000\n",
      "103/103 [==============================] - 0s 653us/step - loss: 0.0130 - val_loss: 0.0471\n",
      "Epoch 501/1000\n",
      "103/103 [==============================] - 0s 987us/step - loss: 0.0190 - val_loss: 0.0883\n",
      "Epoch 502/1000\n",
      "103/103 [==============================] - 0s 764us/step - loss: 0.0163 - val_loss: 0.0216\n",
      "Epoch 503/1000\n",
      "103/103 [==============================] - 0s 511us/step - loss: 0.0198 - val_loss: 0.1187\n",
      "Epoch 504/1000\n",
      "103/103 [==============================] - 0s 455us/step - loss: 0.0169 - val_loss: 0.0486\n",
      "Epoch 505/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0167 - val_loss: 0.0643\n",
      "Epoch 506/1000\n",
      "103/103 [==============================] - 0s 410us/step - loss: 0.0134 - val_loss: 0.0433\n",
      "Epoch 507/1000\n",
      "103/103 [==============================] - 0s 403us/step - loss: 0.0152 - val_loss: 0.1062\n",
      "Epoch 508/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0180 - val_loss: 0.0202\n",
      "Epoch 509/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0247 - val_loss: 0.1065\n",
      "Epoch 510/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0133 - val_loss: 0.0408\n",
      "Epoch 511/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0144 - val_loss: 0.0792\n",
      "Epoch 512/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0133 - val_loss: 0.0427\n",
      "Epoch 513/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0165 - val_loss: 0.1032\n",
      "Epoch 514/1000\n",
      "103/103 [==============================] - 0s 412us/step - loss: 0.0233 - val_loss: 0.0194\n",
      "Epoch 515/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0161 - val_loss: 0.0992\n",
      "Epoch 516/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0126 - val_loss: 0.0349\n",
      "Epoch 517/1000\n",
      "103/103 [==============================] - 0s 406us/step - loss: 0.0166 - val_loss: 0.0656\n",
      "Epoch 518/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0129 - val_loss: 0.0511\n",
      "Epoch 519/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0153 - val_loss: 0.1088\n",
      "Epoch 520/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0183 - val_loss: 0.0367\n",
      "Epoch 521/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0172 - val_loss: 0.0867\n",
      "Epoch 522/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0141 - val_loss: 0.0423\n",
      "Epoch 523/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0139 - val_loss: 0.0805\n",
      "Epoch 524/1000\n",
      "103/103 [==============================] - 0s 494us/step - loss: 0.0185 - val_loss: 0.0260\n",
      "Epoch 525/1000\n",
      "103/103 [==============================] - 0s 452us/step - loss: 0.0234 - val_loss: 0.1148\n",
      "Epoch 526/1000\n",
      "103/103 [==============================] - 0s 455us/step - loss: 0.0121 - val_loss: 0.0403\n",
      "Epoch 527/1000\n",
      "103/103 [==============================] - 0s 470us/step - loss: 0.0138 - val_loss: 0.0723\n",
      "Epoch 528/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0109 - val_loss: 0.0534\n",
      "Epoch 529/1000\n",
      "103/103 [==============================] - 0s 743us/step - loss: 0.0143 - val_loss: 0.0919\n",
      "Epoch 530/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0266\n",
      "Epoch 531/1000\n",
      "103/103 [==============================] - 0s 620us/step - loss: 0.0168 - val_loss: 0.1226\n",
      "Epoch 532/1000\n",
      "103/103 [==============================] - 0s 731us/step - loss: 0.0135 - val_loss: 0.0234\n",
      "Epoch 533/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0751\n",
      "Epoch 534/1000\n",
      "103/103 [==============================] - 0s 596us/step - loss: 0.0125 - val_loss: 0.0608\n",
      "Epoch 535/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0147 - val_loss: 0.0830\n",
      "Epoch 536/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0140 - val_loss: 0.0205\n",
      "Epoch 537/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0177 - val_loss: 0.1183\n",
      "Epoch 538/1000\n",
      "103/103 [==============================] - 0s 471us/step - loss: 0.0188 - val_loss: 0.0630\n",
      "Epoch 539/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0147 - val_loss: 0.0503\n",
      "Epoch 540/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0120 - val_loss: 0.0372\n",
      "Epoch 541/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0139 - val_loss: 0.1059\n",
      "Epoch 542/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0126 - val_loss: 0.0276\n",
      "Epoch 543/1000\n",
      "103/103 [==============================] - 0s 412us/step - loss: 0.0201 - val_loss: 0.1027\n",
      "Epoch 544/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0130 - val_loss: 0.0335\n",
      "Epoch 545/1000\n",
      "103/103 [==============================] - 0s 403us/step - loss: 0.0137 - val_loss: 0.0935\n",
      "Epoch 546/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0142 - val_loss: 0.0550\n",
      "Epoch 547/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0144 - val_loss: 0.0509\n",
      "Epoch 548/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0137 - val_loss: 0.0258\n",
      "Epoch 549/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0178 - val_loss: 0.1274\n",
      "Epoch 550/1000\n",
      "103/103 [==============================] - 0s 670us/step - loss: 0.0116 - val_loss: 0.0309\n",
      "Epoch 551/1000\n",
      "103/103 [==============================] - 0s 639us/step - loss: 0.0159 - val_loss: 0.0765\n",
      "Epoch 552/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0105 - val_loss: 0.0448\n",
      "Epoch 553/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0143 - val_loss: 0.1164\n",
      "Epoch 554/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0176 - val_loss: 0.0319\n",
      "Epoch 555/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0169 - val_loss: 0.0711\n",
      "Epoch 556/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0104 - val_loss: 0.0636\n",
      "Epoch 557/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0119 - val_loss: 0.0499\n",
      "Epoch 558/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0139 - val_loss: 0.0318\n",
      "Epoch 559/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0170 - val_loss: 0.1378\n",
      "Epoch 560/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0134 - val_loss: 0.0400\n",
      "Epoch 561/1000\n",
      "103/103 [==============================] - 0s 493us/step - loss: 0.0122 - val_loss: 0.0610\n",
      "Epoch 562/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0140 - val_loss: 0.0643\n",
      "Epoch 563/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0141 - val_loss: 0.1100\n",
      "Epoch 564/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0167 - val_loss: 0.0154\n",
      "Epoch 565/1000\n",
      "103/103 [==============================] - 0s 498us/step - loss: 0.0165 - val_loss: 0.0966\n",
      "Epoch 566/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0090 - val_loss: 0.0651\n",
      "Epoch 567/1000\n",
      "103/103 [==============================] - 0s 409us/step - loss: 0.0120 - val_loss: 0.0494\n",
      "Epoch 568/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0094 - val_loss: 0.0402\n",
      "Epoch 569/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0131 - val_loss: 0.1382\n",
      "Epoch 570/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0153 - val_loss: 0.0414\n",
      "Epoch 571/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0143 - val_loss: 0.0406\n",
      "Epoch 572/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0184 - val_loss: 0.0861\n",
      "Epoch 573/1000\n",
      "103/103 [==============================] - 0s 410us/step - loss: 0.0121 - val_loss: 0.0839\n",
      "Epoch 574/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0156 - val_loss: 0.0191\n",
      "Epoch 575/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0177 - val_loss: 0.1097\n",
      "Epoch 576/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0095 - val_loss: 0.0614\n",
      "Epoch 577/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0085 - val_loss: 0.0420\n",
      "Epoch 578/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0084 - val_loss: 0.0707\n",
      "Epoch 579/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0122 - val_loss: 0.1273\n",
      "Epoch 580/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0183 - val_loss: 0.0170\n",
      "Epoch 581/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0151 - val_loss: 0.0804\n",
      "Epoch 582/1000\n",
      "103/103 [==============================] - 0s 410us/step - loss: 0.0114 - val_loss: 0.0645\n",
      "Epoch 583/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0147 - val_loss: 0.0484\n",
      "Epoch 584/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0125 - val_loss: 0.0430\n",
      "Epoch 585/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0118 - val_loss: 0.1284\n",
      "Epoch 586/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0112 - val_loss: 0.0252\n",
      "Epoch 587/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0110 - val_loss: 0.0787\n",
      "Epoch 588/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0139 - val_loss: 0.0553\n",
      "Epoch 589/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0130 - val_loss: 0.1042\n",
      "Epoch 590/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0146 - val_loss: 0.0164\n",
      "Epoch 591/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0158 - val_loss: 0.0961\n",
      "Epoch 592/1000\n",
      "103/103 [==============================] - 0s 408us/step - loss: 0.0082 - val_loss: 0.0717\n",
      "Epoch 593/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0092 - val_loss: 0.0507\n",
      "Epoch 594/1000\n",
      "103/103 [==============================] - 0s 440us/step - loss: 0.0100 - val_loss: 0.0342\n",
      "Epoch 595/1000\n",
      "103/103 [==============================] - 0s 502us/step - loss: 0.0133 - val_loss: 0.1425\n",
      "Epoch 596/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0180 - val_loss: 0.0448\n",
      "Epoch 597/1000\n",
      "103/103 [==============================] - 0s 479us/step - loss: 0.0160 - val_loss: 0.0463\n",
      "Epoch 598/1000\n",
      "103/103 [==============================] - 0s 464us/step - loss: 0.0110 - val_loss: 0.0908\n",
      "Epoch 599/1000\n",
      "103/103 [==============================] - 0s 540us/step - loss: 0.0088 - val_loss: 0.0576\n",
      "Epoch 600/1000\n",
      "103/103 [==============================] - 0s 513us/step - loss: 0.0108 - val_loss: 0.0254\n",
      "Epoch 601/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0129 - val_loss: 0.1271\n",
      "Epoch 602/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.0601\n",
      "Epoch 603/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.0481\n",
      "Epoch 604/1000\n",
      "103/103 [==============================] - 0s 878us/step - loss: 0.0120 - val_loss: 0.0546\n",
      "Epoch 605/1000\n",
      "103/103 [==============================] - 0s 463us/step - loss: 0.0147 - val_loss: 0.1329\n",
      "Epoch 606/1000\n",
      "103/103 [==============================] - 0s 446us/step - loss: 0.0143 - val_loss: 0.0168\n",
      "Epoch 607/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0126 - val_loss: 0.0758\n",
      "Epoch 608/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0075 - val_loss: 0.0724\n",
      "Epoch 609/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0099 - val_loss: 0.0635\n",
      "Epoch 610/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0118 - val_loss: 0.0328\n",
      "Epoch 611/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0166 - val_loss: 0.1196\n",
      "Epoch 612/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0129 - val_loss: 0.0475\n",
      "Epoch 613/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0094 - val_loss: 0.0538\n",
      "Epoch 614/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0092 - val_loss: 0.0930\n",
      "Epoch 615/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0092 - val_loss: 0.0556\n",
      "Epoch 616/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0123 - val_loss: 0.0242\n",
      "Epoch 617/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0149 - val_loss: 0.1291\n",
      "Epoch 618/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 437us/step - loss: 0.0102 - val_loss: 0.0614\n",
      "Epoch 619/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0128 - val_loss: 0.0454\n",
      "Epoch 620/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0125 - val_loss: 0.0808\n",
      "Epoch 621/1000\n",
      "103/103 [==============================] - 0s 443us/step - loss: 0.0120 - val_loss: 0.1055\n",
      "Epoch 622/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0127 - val_loss: 0.0190\n",
      "Epoch 623/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0107 - val_loss: 0.0937\n",
      "Epoch 624/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0069 - val_loss: 0.0638\n",
      "Epoch 625/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0111 - val_loss: 0.0499\n",
      "Epoch 626/1000\n",
      "103/103 [==============================] - 0s 792us/step - loss: 0.0097 - val_loss: 0.0420\n",
      "Epoch 627/1000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0104 - val_loss: 0.1349\n",
      "Epoch 628/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0099 - val_loss: 0.0372\n",
      "Epoch 629/1000\n",
      "103/103 [==============================] - 0s 443us/step - loss: 0.0143 - val_loss: 0.0514\n",
      "Epoch 630/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0140 - val_loss: 0.0907\n",
      "Epoch 631/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0097 - val_loss: 0.0938\n",
      "Epoch 632/1000\n",
      "103/103 [==============================] - 0s 547us/step - loss: 0.0138 - val_loss: 0.0193\n",
      "Epoch 633/1000\n",
      "103/103 [==============================] - 0s 651us/step - loss: 0.0139 - val_loss: 0.0988\n",
      "Epoch 634/1000\n",
      "103/103 [==============================] - 0s 792us/step - loss: 0.0070 - val_loss: 0.0635\n",
      "Epoch 635/1000\n",
      "103/103 [==============================] - 0s 590us/step - loss: 0.0066 - val_loss: 0.0485\n",
      "Epoch 636/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0064 - val_loss: 0.0453\n",
      "Epoch 637/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0104 - val_loss: 0.1401\n",
      "Epoch 638/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0141 - val_loss: 0.0421\n",
      "Epoch 639/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0196 - val_loss: 0.0351\n",
      "Epoch 640/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0131 - val_loss: 0.1026\n",
      "Epoch 641/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0080 - val_loss: 0.0507\n",
      "Epoch 642/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0091 - val_loss: 0.0368\n",
      "Epoch 643/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0079 - val_loss: 0.1072\n",
      "Epoch 644/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0074 - val_loss: 0.0507\n",
      "Epoch 645/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0075 - val_loss: 0.0502\n",
      "Epoch 646/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0095 - val_loss: 0.0439\n",
      "Epoch 647/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0159 - val_loss: 0.1434\n",
      "Epoch 648/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0147 - val_loss: 0.0205\n",
      "Epoch 649/1000\n",
      "103/103 [==============================] - 0s 646us/step - loss: 0.0113 - val_loss: 0.0794\n",
      "Epoch 650/1000\n",
      "103/103 [==============================] - 0s 649us/step - loss: 0.0080 - val_loss: 0.0659\n",
      "Epoch 651/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0085 - val_loss: 0.0552\n",
      "Epoch 652/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0081 - val_loss: 0.0462\n",
      "Epoch 653/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0093 - val_loss: 0.1246\n",
      "Epoch 654/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0128 - val_loss: 0.0298\n",
      "Epoch 655/1000\n",
      "103/103 [==============================] - 0s 475us/step - loss: 0.0082 - val_loss: 0.0649\n",
      "Epoch 656/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0081 - val_loss: 0.0683\n",
      "Epoch 657/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0114 - val_loss: 0.0809\n",
      "Epoch 658/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0141 - val_loss: 0.0221\n",
      "Epoch 659/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0143 - val_loss: 0.1085\n",
      "Epoch 660/1000\n",
      "103/103 [==============================] - 0s 405us/step - loss: 0.0070 - val_loss: 0.0511\n",
      "Epoch 661/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0060 - val_loss: 0.0483\n",
      "Epoch 662/1000\n",
      "103/103 [==============================] - 0s 440us/step - loss: 0.0067 - val_loss: 0.0868\n",
      "Epoch 663/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0083 - val_loss: 0.0809\n",
      "Epoch 664/1000\n",
      "103/103 [==============================] - 0s 406us/step - loss: 0.0128 - val_loss: 0.0242\n",
      "Epoch 665/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0156 - val_loss: 0.1092\n",
      "Epoch 666/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0082 - val_loss: 0.0526\n",
      "Epoch 667/1000\n",
      "103/103 [==============================] - 0s 409us/step - loss: 0.0103 - val_loss: 0.0424\n",
      "Epoch 668/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0083 - val_loss: 0.0926\n",
      "Epoch 669/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0066 - val_loss: 0.0565\n",
      "Epoch 670/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0073 - val_loss: 0.0497\n",
      "Epoch 671/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0102 - val_loss: 0.1192\n",
      "Epoch 672/1000\n",
      "103/103 [==============================] - 0s 657us/step - loss: 0.0095 - val_loss: 0.0500\n",
      "Epoch 673/1000\n",
      "103/103 [==============================] - 0s 865us/step - loss: 0.0102 - val_loss: 0.0261\n",
      "Epoch 674/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0140 - val_loss: 0.1178\n",
      "Epoch 675/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0078 - val_loss: 0.0486\n",
      "Epoch 676/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0100 - val_loss: 0.0534\n",
      "Epoch 677/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0102 - val_loss: 0.0858\n",
      "Epoch 678/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0087 - val_loss: 0.0851\n",
      "Epoch 679/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0100 - val_loss: 0.0241\n",
      "Epoch 680/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0099 - val_loss: 0.1191\n",
      "Epoch 681/1000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0070 - val_loss: 0.0386\n",
      "Epoch 682/1000\n",
      "103/103 [==============================] - 0s 510us/step - loss: 0.0099 - val_loss: 0.0589\n",
      "Epoch 683/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0072 - val_loss: 0.0600\n",
      "Epoch 684/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0092 - val_loss: 0.1086\n",
      "Epoch 685/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0109 - val_loss: 0.0243\n",
      "Epoch 686/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0110 - val_loss: 0.1096\n",
      "Epoch 687/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0098 - val_loss: 0.0536\n",
      "Epoch 688/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0096 - val_loss: 0.0530\n",
      "Epoch 689/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0099 - val_loss: 0.0748\n",
      "Epoch 690/1000\n",
      "103/103 [==============================] - 0s 589us/step - loss: 0.0094 - val_loss: 0.0857\n",
      "Epoch 691/1000\n",
      "103/103 [==============================] - 0s 674us/step - loss: 0.0108 - val_loss: 0.0257\n",
      "Epoch 692/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0109 - val_loss: 0.1098\n",
      "Epoch 693/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0069 - val_loss: 0.0542\n",
      "Epoch 694/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0054 - val_loss: 0.0446\n",
      "Epoch 695/1000\n",
      "103/103 [==============================] - 0s 518us/step - loss: 0.0062 - val_loss: 0.0818\n",
      "Epoch 696/1000\n",
      "103/103 [==============================] - 0s 410us/step - loss: 0.0068 - val_loss: 0.0956\n",
      "Epoch 697/1000\n",
      "103/103 [==============================] - 0s 613us/step - loss: 0.0140 - val_loss: 0.0225\n",
      "Epoch 698/1000\n",
      "103/103 [==============================] - 0s 768us/step - loss: 0.0159 - val_loss: 0.1027\n",
      "Epoch 699/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0068 - val_loss: 0.0595\n",
      "Epoch 700/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0076 - val_loss: 0.0489\n",
      "Epoch 701/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0072 - val_loss: 0.0864\n",
      "Epoch 702/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0075 - val_loss: 0.0774\n",
      "Epoch 703/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0092 - val_loss: 0.0275\n",
      "Epoch 704/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0087 - val_loss: 0.1213\n",
      "Epoch 705/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0068 - val_loss: 0.0484\n",
      "Epoch 706/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0133 - val_loss: 0.0534\n",
      "Epoch 707/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0117 - val_loss: 0.0688\n",
      "Epoch 708/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0072 - val_loss: 0.1003\n",
      "Epoch 709/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0082 - val_loss: 0.0247\n",
      "Epoch 710/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0087 - val_loss: 0.1096\n",
      "Epoch 711/1000\n",
      "103/103 [==============================] - 0s 408us/step - loss: 0.0082 - val_loss: 0.0510\n",
      "Epoch 712/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0067 - val_loss: 0.0445\n",
      "Epoch 713/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0068 - val_loss: 0.0710\n",
      "Epoch 714/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0086 - val_loss: 0.1025\n",
      "Epoch 715/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0115 - val_loss: 0.0214\n",
      "Epoch 716/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0113 - val_loss: 0.1070\n",
      "Epoch 717/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0082 - val_loss: 0.0734\n",
      "Epoch 718/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0098 - val_loss: 0.0486\n",
      "Epoch 719/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0091 - val_loss: 0.0773\n",
      "Epoch 720/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0075 - val_loss: 0.0982\n",
      "Epoch 721/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0112 - val_loss: 0.0228\n",
      "Epoch 722/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0112 - val_loss: 0.0993\n",
      "Epoch 723/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0054 - val_loss: 0.0675\n",
      "Epoch 724/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0057 - val_loss: 0.0393\n",
      "Epoch 725/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0059 - val_loss: 0.0838\n",
      "Epoch 726/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0095 - val_loss: 0.1024\n",
      "Epoch 727/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0132 - val_loss: 0.0238\n",
      "Epoch 728/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0098 - val_loss: 0.1023\n",
      "Epoch 729/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0065 - val_loss: 0.0519\n",
      "Epoch 730/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0095 - val_loss: 0.0474\n",
      "Epoch 731/1000\n",
      "103/103 [==============================] - 0s 432us/step - loss: 0.0065 - val_loss: 0.0840\n",
      "Epoch 732/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0052 - val_loss: 0.0669\n",
      "Epoch 733/1000\n",
      "103/103 [==============================] - 0s 409us/step - loss: 0.0069 - val_loss: 0.0449\n",
      "Epoch 734/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0077 - val_loss: 0.1205\n",
      "Epoch 735/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0080 - val_loss: 0.0457\n",
      "Epoch 736/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0092 - val_loss: 0.0392\n",
      "Epoch 737/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0118 - val_loss: 0.1045\n",
      "Epoch 738/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0075 - val_loss: 0.0515\n",
      "Epoch 739/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0079 - val_loss: 0.0452\n",
      "Epoch 740/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0097 - val_loss: 0.1173\n",
      "Epoch 741/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0080 - val_loss: 0.0401\n",
      "Epoch 742/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0057 - val_loss: 0.0572\n",
      "Epoch 743/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0062 - val_loss: 0.1134\n",
      "Epoch 744/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0081 - val_loss: 0.0244\n",
      "Epoch 745/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0111 - val_loss: 0.0820\n",
      "Epoch 746/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0081 - val_loss: 0.0772\n",
      "Epoch 747/1000\n",
      "103/103 [==============================] - 0s 455us/step - loss: 0.0074 - val_loss: 0.0615\n",
      "Epoch 748/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0067 - val_loss: 0.0326\n",
      "Epoch 749/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0120 - val_loss: 0.1383\n",
      "Epoch 750/1000\n",
      "103/103 [==============================] - 0s 409us/step - loss: 0.0087 - val_loss: 0.0317\n",
      "Epoch 751/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0057 - val_loss: 0.0656\n",
      "Epoch 752/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0044 - val_loss: 0.0753\n",
      "Epoch 753/1000\n",
      "103/103 [==============================] - 0s 430us/step - loss: 0.0087 - val_loss: 0.0494\n",
      "Epoch 754/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0090 - val_loss: 0.0375\n",
      "Epoch 755/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0120 - val_loss: 0.1206\n",
      "Epoch 756/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0078 - val_loss: 0.0350\n",
      "Epoch 757/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0064 - val_loss: 0.0788\n",
      "Epoch 758/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0077 - val_loss: 0.0806\n",
      "Epoch 759/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0090 - val_loss: 0.0580\n",
      "Epoch 760/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0086 - val_loss: 0.0378\n",
      "Epoch 761/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0082 - val_loss: 0.1223\n",
      "Epoch 762/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0056 - val_loss: 0.0322\n",
      "Epoch 763/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0077 - val_loss: 0.0596\n",
      "Epoch 764/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0070 - val_loss: 0.0735\n",
      "Epoch 765/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0066 - val_loss: 0.0953\n",
      "Epoch 766/1000\n",
      "103/103 [==============================] - 0s 456us/step - loss: 0.0121 - val_loss: 0.0238\n",
      "Epoch 767/1000\n",
      "103/103 [==============================] - 0s 580us/step - loss: 0.0108 - val_loss: 0.1058\n",
      "Epoch 768/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0526\n",
      "Epoch 769/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.0522\n",
      "Epoch 770/1000\n",
      "103/103 [==============================] - 0s 734us/step - loss: 0.0056 - val_loss: 0.0797\n",
      "Epoch 771/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0757\n",
      "Epoch 772/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0077 - val_loss: 0.0332\n",
      "Epoch 773/1000\n",
      "103/103 [==============================] - 0s 468us/step - loss: 0.0073 - val_loss: 0.1236\n",
      "Epoch 774/1000\n",
      "103/103 [==============================] - 0s 454us/step - loss: 0.0072 - val_loss: 0.0412\n",
      "Epoch 775/1000\n",
      "103/103 [==============================] - 0s 478us/step - loss: 0.0110 - val_loss: 0.0640\n",
      "Epoch 776/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0121 - val_loss: 0.0590\n",
      "Epoch 777/1000\n",
      "103/103 [==============================] - 0s 723us/step - loss: 0.0071 - val_loss: 0.0979\n",
      "Epoch 778/1000\n",
      "103/103 [==============================] - 0s 697us/step - loss: 0.0063 - val_loss: 0.0285\n",
      "Epoch 779/1000\n",
      "103/103 [==============================] - 0s 803us/step - loss: 0.0073 - val_loss: 0.1072\n",
      "Epoch 780/1000\n",
      "103/103 [==============================] - 0s 644us/step - loss: 0.0076 - val_loss: 0.0487\n",
      "Epoch 781/1000\n",
      "103/103 [==============================] - 0s 588us/step - loss: 0.0049 - val_loss: 0.0427\n",
      "Epoch 782/1000\n",
      "103/103 [==============================] - 0s 476us/step - loss: 0.0046 - val_loss: 0.0744\n",
      "Epoch 783/1000\n",
      "103/103 [==============================] - 0s 464us/step - loss: 0.0081 - val_loss: 0.0877\n",
      "Epoch 784/1000\n",
      "103/103 [==============================] - 0s 456us/step - loss: 0.0097 - val_loss: 0.0251\n",
      "Epoch 785/1000\n",
      "103/103 [==============================] - 0s 446us/step - loss: 0.0110 - val_loss: 0.1129\n",
      "Epoch 786/1000\n",
      "103/103 [==============================] - 0s 498us/step - loss: 0.0069 - val_loss: 0.0607\n",
      "Epoch 787/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0080 - val_loss: 0.0545\n",
      "Epoch 788/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0085 - val_loss: 0.0875\n",
      "Epoch 789/1000\n",
      "103/103 [==============================] - 0s 597us/step - loss: 0.0059 - val_loss: 0.0733\n",
      "Epoch 790/1000\n",
      "103/103 [==============================] - 0s 559us/step - loss: 0.0089 - val_loss: 0.0263\n",
      "Epoch 791/1000\n",
      "103/103 [==============================] - 0s 588us/step - loss: 0.0106 - val_loss: 0.1088\n",
      "Epoch 792/1000\n",
      "103/103 [==============================] - 0s 533us/step - loss: 0.0044 - val_loss: 0.0537\n",
      "Epoch 793/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0049 - val_loss: 0.0443\n",
      "Epoch 794/1000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0072 - val_loss: 0.1041\n",
      "Epoch 795/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0072 - val_loss: 0.0641\n",
      "Epoch 796/1000\n",
      "103/103 [==============================] - 0s 462us/step - loss: 0.0094 - val_loss: 0.0308\n",
      "Epoch 797/1000\n",
      "103/103 [==============================] - 0s 469us/step - loss: 0.0111 - val_loss: 0.1120\n",
      "Epoch 798/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0059 - val_loss: 0.0430\n",
      "Epoch 799/1000\n",
      "103/103 [==============================] - 0s 506us/step - loss: 0.0067 - val_loss: 0.0599\n",
      "Epoch 800/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0832\n",
      "Epoch 801/1000\n",
      "103/103 [==============================] - 0s 680us/step - loss: 0.0047 - val_loss: 0.0604\n",
      "Epoch 802/1000\n",
      "103/103 [==============================] - 0s 426us/step - loss: 0.0069 - val_loss: 0.0351\n",
      "Epoch 803/1000\n",
      "103/103 [==============================] - 0s 459us/step - loss: 0.0101 - val_loss: 0.1365\n",
      "Epoch 804/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0083 - val_loss: 0.0323\n",
      "Epoch 805/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0078 - val_loss: 0.0719\n",
      "Epoch 806/1000\n",
      "103/103 [==============================] - 0s 727us/step - loss: 0.0063 - val_loss: 0.0662\n",
      "Epoch 807/1000\n",
      "103/103 [==============================] - 0s 697us/step - loss: 0.0059 - val_loss: 0.0609\n",
      "Epoch 808/1000\n",
      "103/103 [==============================] - 0s 768us/step - loss: 0.0047 - val_loss: 0.0375\n",
      "Epoch 809/1000\n",
      "103/103 [==============================] - 0s 455us/step - loss: 0.0091 - val_loss: 0.1385\n",
      "Epoch 810/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0108 - val_loss: 0.0319\n",
      "Epoch 811/1000\n",
      "103/103 [==============================] - 0s 535us/step - loss: 0.0060 - val_loss: 0.0711\n",
      "Epoch 812/1000\n",
      "103/103 [==============================] - 0s 800us/step - loss: 0.0046 - val_loss: 0.0739\n",
      "Epoch 813/1000\n",
      "103/103 [==============================] - 0s 476us/step - loss: 0.0073 - val_loss: 0.0411\n",
      "Epoch 814/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0062 - val_loss: 0.0538\n",
      "Epoch 815/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0075 - val_loss: 0.1240\n",
      "Epoch 816/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0092 - val_loss: 0.0246\n",
      "Epoch 817/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0063 - val_loss: 0.0946\n",
      "Epoch 818/1000\n",
      "103/103 [==============================] - 0s 440us/step - loss: 0.0056 - val_loss: 0.0632\n",
      "Epoch 819/1000\n",
      "103/103 [==============================] - 0s 586us/step - loss: 0.0073 - val_loss: 0.0448\n",
      "Epoch 820/1000\n",
      "103/103 [==============================] - 0s 452us/step - loss: 0.0071 - val_loss: 0.0628\n",
      "Epoch 821/1000\n",
      "103/103 [==============================] - 0s 610us/step - loss: 0.0058 - val_loss: 0.1102\n",
      "Epoch 822/1000\n",
      "103/103 [==============================] - 0s 496us/step - loss: 0.0081 - val_loss: 0.0234\n",
      "Epoch 823/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0076 - val_loss: 0.1072\n",
      "Epoch 824/1000\n",
      "103/103 [==============================] - 0s 462us/step - loss: 0.0077 - val_loss: 0.0642\n",
      "Epoch 825/1000\n",
      "103/103 [==============================] - 0s 469us/step - loss: 0.0083 - val_loss: 0.0544\n",
      "Epoch 826/1000\n",
      "103/103 [==============================] - 0s 470us/step - loss: 0.0078 - val_loss: 0.0565\n",
      "Epoch 827/1000\n",
      "103/103 [==============================] - 0s 491us/step - loss: 0.0064 - val_loss: 0.1080\n",
      "Epoch 828/1000\n",
      "103/103 [==============================] - 0s 560us/step - loss: 0.0064 - val_loss: 0.0277\n",
      "Epoch 829/1000\n",
      "103/103 [==============================] - 0s 450us/step - loss: 0.0069 - val_loss: 0.0925\n",
      "Epoch 830/1000\n",
      "103/103 [==============================] - 0s 487us/step - loss: 0.0089 - val_loss: 0.0699\n",
      "Epoch 831/1000\n",
      "103/103 [==============================] - 0s 492us/step - loss: 0.0049 - val_loss: 0.0414\n",
      "Epoch 832/1000\n",
      "103/103 [==============================] - 0s 468us/step - loss: 0.0041 - val_loss: 0.0627\n",
      "Epoch 833/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0045 - val_loss: 0.1005\n",
      "Epoch 834/1000\n",
      "103/103 [==============================] - 0s 475us/step - loss: 0.0064 - val_loss: 0.0252\n",
      "Epoch 835/1000\n",
      "103/103 [==============================] - 0s 504us/step - loss: 0.0073 - val_loss: 0.0832\n",
      "Epoch 836/1000\n",
      "103/103 [==============================] - 0s 552us/step - loss: 0.0122 - val_loss: 0.1027\n",
      "Epoch 837/1000\n",
      "103/103 [==============================] - 0s 670us/step - loss: 0.0100 - val_loss: 0.0459\n",
      "Epoch 838/1000\n",
      "103/103 [==============================] - 0s 518us/step - loss: 0.0050 - val_loss: 0.0603\n",
      "Epoch 839/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0039 - val_loss: 0.0949\n",
      "Epoch 840/1000\n",
      "103/103 [==============================] - 0s 667us/step - loss: 0.0060 - val_loss: 0.0252\n",
      "Epoch 841/1000\n",
      "103/103 [==============================] - 0s 690us/step - loss: 0.0082 - val_loss: 0.0983\n",
      "Epoch 842/1000\n",
      "103/103 [==============================] - 0s 511us/step - loss: 0.0072 - val_loss: 0.0688\n",
      "Epoch 843/1000\n",
      "103/103 [==============================] - 0s 558us/step - loss: 0.0051 - val_loss: 0.0426\n",
      "Epoch 844/1000\n",
      "103/103 [==============================] - 0s 454us/step - loss: 0.0040 - val_loss: 0.0635\n",
      "Epoch 845/1000\n",
      "103/103 [==============================] - 0s 458us/step - loss: 0.0056 - val_loss: 0.1212\n",
      "Epoch 846/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0106 - val_loss: 0.0211\n",
      "Epoch 847/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0110 - val_loss: 0.0961\n",
      "Epoch 848/1000\n",
      "103/103 [==============================] - 0s 463us/step - loss: 0.0045 - val_loss: 0.0654\n",
      "Epoch 849/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0046 - val_loss: 0.0501\n",
      "Epoch 850/1000\n",
      "103/103 [==============================] - 0s 561us/step - loss: 0.0050 - val_loss: 0.0802\n",
      "Epoch 851/1000\n",
      "103/103 [==============================] - 0s 486us/step - loss: 0.0055 - val_loss: 0.0988\n",
      "Epoch 852/1000\n",
      "103/103 [==============================] - 0s 607us/step - loss: 0.0106 - val_loss: 0.0221\n",
      "Epoch 853/1000\n",
      "103/103 [==============================] - 0s 482us/step - loss: 0.0095 - val_loss: 0.1070\n",
      "Epoch 854/1000\n",
      "103/103 [==============================] - 0s 790us/step - loss: 0.0054 - val_loss: 0.0517\n",
      "Epoch 855/1000\n",
      "103/103 [==============================] - 0s 690us/step - loss: 0.0080 - val_loss: 0.0494\n",
      "Epoch 856/1000\n",
      "103/103 [==============================] - 0s 754us/step - loss: 0.0054 - val_loss: 0.0847\n",
      "Epoch 857/1000\n",
      "103/103 [==============================] - 0s 997us/step - loss: 0.0037 - val_loss: 0.0628\n",
      "Epoch 858/1000\n",
      "103/103 [==============================] - 0s 525us/step - loss: 0.0041 - val_loss: 0.0418\n",
      "Epoch 859/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0047 - val_loss: 0.1218\n",
      "Epoch 860/1000\n",
      "103/103 [==============================] - 0s 443us/step - loss: 0.0063 - val_loss: 0.0439\n",
      "Epoch 861/1000\n",
      "103/103 [==============================] - 0s 537us/step - loss: 0.0106 - val_loss: 0.0498\n",
      "Epoch 862/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0107 - val_loss: 0.0702\n",
      "Epoch 863/1000\n",
      "103/103 [==============================] - 0s 643us/step - loss: 0.0058 - val_loss: 0.0903\n",
      "Epoch 864/1000\n",
      "103/103 [==============================] - 0s 773us/step - loss: 0.0063 - val_loss: 0.0261\n",
      "Epoch 865/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0081 - val_loss: 0.1155\n",
      "Epoch 866/1000\n",
      "103/103 [==============================] - 0s 467us/step - loss: 0.0062 - val_loss: 0.0449\n",
      "Epoch 867/1000\n",
      "103/103 [==============================] - 0s 464us/step - loss: 0.0039 - val_loss: 0.0511\n",
      "Epoch 868/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0034 - val_loss: 0.0867\n",
      "Epoch 869/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0066 - val_loss: 0.0477\n",
      "Epoch 870/1000\n",
      "103/103 [==============================] - 0s 417us/step - loss: 0.0081 - val_loss: 0.0372\n",
      "Epoch 871/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0111 - val_loss: 0.1159\n",
      "Epoch 872/1000\n",
      "103/103 [==============================] - 0s 472us/step - loss: 0.0061 - val_loss: 0.0371\n",
      "Epoch 873/1000\n",
      "103/103 [==============================] - 0s 599us/step - loss: 0.0043 - val_loss: 0.0781\n",
      "Epoch 874/1000\n",
      "103/103 [==============================] - 0s 435us/step - loss: 0.0039 - val_loss: 0.0757\n",
      "Epoch 875/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0056 - val_loss: 0.0345\n",
      "Epoch 876/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0065 - val_loss: 0.0763\n",
      "Epoch 877/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0059 - val_loss: 0.1004\n",
      "Epoch 878/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0105 - val_loss: 0.0251\n",
      "Epoch 879/1000\n",
      "103/103 [==============================] - 0s 529us/step - loss: 0.0087 - val_loss: 0.0994\n",
      "Epoch 880/1000\n",
      "103/103 [==============================] - 0s 752us/step - loss: 0.0048 - val_loss: 0.0620\n",
      "Epoch 881/1000\n",
      "103/103 [==============================] - 0s 486us/step - loss: 0.0042 - val_loss: 0.0529\n",
      "Epoch 882/1000\n",
      "103/103 [==============================] - 0s 574us/step - loss: 0.0036 - val_loss: 0.0608\n",
      "Epoch 883/1000\n",
      "103/103 [==============================] - 0s 526us/step - loss: 0.0050 - val_loss: 0.1228\n",
      "Epoch 884/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0094 - val_loss: 0.0207\n",
      "Epoch 885/1000\n",
      "103/103 [==============================] - 0s 455us/step - loss: 0.0078 - val_loss: 0.1132\n",
      "Epoch 886/1000\n",
      "103/103 [==============================] - 0s 461us/step - loss: 0.0055 - val_loss: 0.0586\n",
      "Epoch 887/1000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0102 - val_loss: 0.0498\n",
      "Epoch 888/1000\n",
      "103/103 [==============================] - 0s 454us/step - loss: 0.0080 - val_loss: 0.0667\n",
      "Epoch 889/1000\n",
      "103/103 [==============================] - 0s 450us/step - loss: 0.0040 - val_loss: 0.0944\n",
      "Epoch 890/1000\n",
      "103/103 [==============================] - 0s 640us/step - loss: 0.0048 - val_loss: 0.0272\n",
      "Epoch 891/1000\n",
      "103/103 [==============================] - 0s 438us/step - loss: 0.0051 - val_loss: 0.1056\n",
      "Epoch 892/1000\n",
      "103/103 [==============================] - 0s 685us/step - loss: 0.0067 - val_loss: 0.0556\n",
      "Epoch 893/1000\n",
      "103/103 [==============================] - 0s 897us/step - loss: 0.0058 - val_loss: 0.0337\n",
      "Epoch 894/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0872\n",
      "Epoch 895/1000\n",
      "103/103 [==============================] - 0s 567us/step - loss: 0.0048 - val_loss: 0.0643\n",
      "Epoch 896/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0067 - val_loss: 0.0296\n",
      "Epoch 897/1000\n",
      "103/103 [==============================] - 0s 811us/step - loss: 0.0085 - val_loss: 0.1196\n",
      "Epoch 898/1000\n",
      "103/103 [==============================] - 0s 611us/step - loss: 0.0075 - val_loss: 0.0504\n",
      "Epoch 899/1000\n",
      "103/103 [==============================] - 0s 702us/step - loss: 0.0053 - val_loss: 0.0560\n",
      "Epoch 900/1000\n",
      "103/103 [==============================] - 0s 543us/step - loss: 0.0042 - val_loss: 0.0926\n",
      "Epoch 901/1000\n",
      "103/103 [==============================] - 0s 538us/step - loss: 0.0050 - val_loss: 0.0348\n",
      "Epoch 902/1000\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0629\n",
      "Epoch 903/1000\n",
      "103/103 [==============================] - 0s 623us/step - loss: 0.0054 - val_loss: 0.0996\n",
      "Epoch 904/1000\n",
      "103/103 [==============================] - 0s 551us/step - loss: 0.0062 - val_loss: 0.0302\n",
      "Epoch 905/1000\n",
      "103/103 [==============================] - 0s 447us/step - loss: 0.0049 - val_loss: 0.0780\n",
      "Epoch 906/1000\n",
      "103/103 [==============================] - 0s 566us/step - loss: 0.0074 - val_loss: 0.1022\n",
      "Epoch 907/1000\n",
      "103/103 [==============================] - 0s 458us/step - loss: 0.0081 - val_loss: 0.0266\n",
      "Epoch 908/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0073 - val_loss: 0.0954\n",
      "Epoch 909/1000\n",
      "103/103 [==============================] - 0s 440us/step - loss: 0.0032 - val_loss: 0.0524\n",
      "Epoch 910/1000\n",
      "103/103 [==============================] - 0s 436us/step - loss: 0.0065 - val_loss: 0.0611\n",
      "Epoch 911/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0041 - val_loss: 0.0480\n",
      "Epoch 912/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0074 - val_loss: 0.1394\n",
      "Epoch 913/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0111 - val_loss: 0.0232\n",
      "Epoch 914/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0054 - val_loss: 0.0918\n",
      "Epoch 915/1000\n",
      "103/103 [==============================] - 0s 414us/step - loss: 0.0039 - val_loss: 0.0482\n",
      "Epoch 916/1000\n",
      "103/103 [==============================] - 0s 660us/step - loss: 0.0065 - val_loss: 0.0549\n",
      "Epoch 917/1000\n",
      "103/103 [==============================] - 0s 527us/step - loss: 0.0031 - val_loss: 0.0679\n",
      "Epoch 918/1000\n",
      "103/103 [==============================] - 0s 498us/step - loss: 0.0040 - val_loss: 0.0921\n",
      "Epoch 919/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0082 - val_loss: 0.0280\n",
      "Epoch 920/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0055 - val_loss: 0.1129\n",
      "Epoch 921/1000\n",
      "103/103 [==============================] - 0s 453us/step - loss: 0.0051 - val_loss: 0.0371\n",
      "Epoch 922/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0098 - val_loss: 0.0756\n",
      "Epoch 923/1000\n",
      "103/103 [==============================] - 0s 440us/step - loss: 0.0042 - val_loss: 0.0457\n",
      "Epoch 924/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0049 - val_loss: 0.1055\n",
      "Epoch 925/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0064 - val_loss: 0.0316\n",
      "Epoch 926/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 431us/step - loss: 0.0036 - val_loss: 0.0910\n",
      "Epoch 927/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0039 - val_loss: 0.0432\n",
      "Epoch 928/1000\n",
      "103/103 [==============================] - 0s 409us/step - loss: 0.0097 - val_loss: 0.0852\n",
      "Epoch 929/1000\n",
      "103/103 [==============================] - 0s 412us/step - loss: 0.0053 - val_loss: 0.0352\n",
      "Epoch 930/1000\n",
      "103/103 [==============================] - 0s 425us/step - loss: 0.0086 - val_loss: 0.1212\n",
      "Epoch 931/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0066 - val_loss: 0.0330\n",
      "Epoch 932/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0038 - val_loss: 0.0777\n",
      "Epoch 933/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0028 - val_loss: 0.0654\n",
      "Epoch 934/1000\n",
      "103/103 [==============================] - 0s 416us/step - loss: 0.0058 - val_loss: 0.0387\n",
      "Epoch 935/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0052 - val_loss: 0.0747\n",
      "Epoch 936/1000\n",
      "103/103 [==============================] - 0s 625us/step - loss: 0.0070 - val_loss: 0.1131\n",
      "Epoch 937/1000\n",
      "103/103 [==============================] - 0s 500us/step - loss: 0.0096 - val_loss: 0.0254\n",
      "Epoch 938/1000\n",
      "103/103 [==============================] - 0s 424us/step - loss: 0.0062 - val_loss: 0.1056\n",
      "Epoch 939/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0045 - val_loss: 0.0499\n",
      "Epoch 940/1000\n",
      "103/103 [==============================] - 0s 408us/step - loss: 0.0056 - val_loss: 0.0469\n",
      "Epoch 941/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0036 - val_loss: 0.0762\n",
      "Epoch 942/1000\n",
      "103/103 [==============================] - 0s 415us/step - loss: 0.0033 - val_loss: 0.0785\n",
      "Epoch 943/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0060 - val_loss: 0.0271\n",
      "Epoch 944/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0081 - val_loss: 0.1375\n",
      "Epoch 945/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0079 - val_loss: 0.0435\n",
      "Epoch 946/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0065 - val_loss: 0.0635\n",
      "Epoch 947/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0063 - val_loss: 0.0642\n",
      "Epoch 948/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0045 - val_loss: 0.0646\n",
      "Epoch 949/1000\n",
      "103/103 [==============================] - 0s 444us/step - loss: 0.0028 - val_loss: 0.0408\n",
      "Epoch 950/1000\n",
      "103/103 [==============================] - 0s 587us/step - loss: 0.0051 - val_loss: 0.1287\n",
      "Epoch 951/1000\n",
      "103/103 [==============================] - 0s 477us/step - loss: 0.0085 - val_loss: 0.0302\n",
      "Epoch 952/1000\n",
      "103/103 [==============================] - 0s 410us/step - loss: 0.0037 - val_loss: 0.0726\n",
      "Epoch 953/1000\n",
      "103/103 [==============================] - 0s 459us/step - loss: 0.0031 - val_loss: 0.0533\n",
      "Epoch 954/1000\n",
      "103/103 [==============================] - 0s 474us/step - loss: 0.0080 - val_loss: 0.0590\n",
      "Epoch 955/1000\n",
      "103/103 [==============================] - 0s 477us/step - loss: 0.0036 - val_loss: 0.0392\n",
      "Epoch 956/1000\n",
      "103/103 [==============================] - 0s 483us/step - loss: 0.0077 - val_loss: 0.1338\n",
      "Epoch 957/1000\n",
      "103/103 [==============================] - 0s 503us/step - loss: 0.0086 - val_loss: 0.0266\n",
      "Epoch 958/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0044 - val_loss: 0.0891\n",
      "Epoch 959/1000\n",
      "103/103 [==============================] - 0s 474us/step - loss: 0.0032 - val_loss: 0.0527\n",
      "Epoch 960/1000\n",
      "103/103 [==============================] - 0s 497us/step - loss: 0.0057 - val_loss: 0.0491\n",
      "Epoch 961/1000\n",
      "103/103 [==============================] - 0s 482us/step - loss: 0.0038 - val_loss: 0.0738\n",
      "Epoch 962/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0050 - val_loss: 0.1058\n",
      "Epoch 963/1000\n",
      "103/103 [==============================] - 0s 467us/step - loss: 0.0095 - val_loss: 0.0238\n",
      "Epoch 964/1000\n",
      "103/103 [==============================] - 0s 487us/step - loss: 0.0063 - val_loss: 0.1133\n",
      "Epoch 965/1000\n",
      "103/103 [==============================] - 0s 460us/step - loss: 0.0049 - val_loss: 0.0415\n",
      "Epoch 966/1000\n",
      "103/103 [==============================] - 0s 679us/step - loss: 0.0067 - val_loss: 0.0560\n",
      "Epoch 967/1000\n",
      "103/103 [==============================] - 0s 708us/step - loss: 0.0031 - val_loss: 0.0754\n",
      "Epoch 968/1000\n",
      "103/103 [==============================] - 0s 465us/step - loss: 0.0025 - val_loss: 0.0665\n",
      "Epoch 969/1000\n",
      "103/103 [==============================] - 0s 448us/step - loss: 0.0046 - val_loss: 0.0385\n",
      "Epoch 970/1000\n",
      "103/103 [==============================] - 0s 578us/step - loss: 0.0062 - val_loss: 0.1375\n",
      "Epoch 971/1000\n",
      "103/103 [==============================] - 0s 458us/step - loss: 0.0083 - val_loss: 0.0288\n",
      "Epoch 972/1000\n",
      "103/103 [==============================] - 0s 576us/step - loss: 0.0062 - val_loss: 0.0812\n",
      "Epoch 973/1000\n",
      "103/103 [==============================] - 0s 457us/step - loss: 0.0053 - val_loss: 0.0485\n",
      "Epoch 974/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0049 - val_loss: 0.0688\n",
      "Epoch 975/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0027 - val_loss: 0.0536\n",
      "Epoch 976/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0034 - val_loss: 0.1117\n",
      "Epoch 977/1000\n",
      "103/103 [==============================] - 0s 422us/step - loss: 0.0087 - val_loss: 0.0237\n",
      "Epoch 978/1000\n",
      "103/103 [==============================] - 0s 423us/step - loss: 0.0058 - val_loss: 0.1041\n",
      "Epoch 979/1000\n",
      "103/103 [==============================] - 0s 433us/step - loss: 0.0045 - val_loss: 0.0368\n",
      "Epoch 980/1000\n",
      "103/103 [==============================] - 0s 431us/step - loss: 0.0083 - val_loss: 0.0747\n",
      "Epoch 981/1000\n",
      "103/103 [==============================] - 0s 420us/step - loss: 0.0026 - val_loss: 0.0546\n",
      "Epoch 982/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0036 - val_loss: 0.1019\n",
      "Epoch 983/1000\n",
      "103/103 [==============================] - 0s 577us/step - loss: 0.0070 - val_loss: 0.0298\n",
      "Epoch 984/1000\n",
      "103/103 [==============================] - 0s 457us/step - loss: 0.0050 - val_loss: 0.0985\n",
      "Epoch 985/1000\n",
      "103/103 [==============================] - 0s 442us/step - loss: 0.0039 - val_loss: 0.0401\n",
      "Epoch 986/1000\n",
      "103/103 [==============================] - 0s 437us/step - loss: 0.0096 - val_loss: 0.0885\n",
      "Epoch 987/1000\n",
      "103/103 [==============================] - 0s 460us/step - loss: 0.0038 - val_loss: 0.0382\n",
      "Epoch 988/1000\n",
      "103/103 [==============================] - 0s 428us/step - loss: 0.0052 - val_loss: 0.1241\n",
      "Epoch 989/1000\n",
      "103/103 [==============================] - 0s 419us/step - loss: 0.0059 - val_loss: 0.0308\n",
      "Epoch 990/1000\n",
      "103/103 [==============================] - 0s 411us/step - loss: 0.0040 - val_loss: 0.0744\n",
      "Epoch 991/1000\n",
      "103/103 [==============================] - 0s 434us/step - loss: 0.0032 - val_loss: 0.0764\n",
      "Epoch 992/1000\n",
      "103/103 [==============================] - 0s 418us/step - loss: 0.0057 - val_loss: 0.0426\n",
      "Epoch 993/1000\n",
      "103/103 [==============================] - 0s 413us/step - loss: 0.0047 - val_loss: 0.0561\n",
      "Epoch 994/1000\n",
      "103/103 [==============================] - 0s 427us/step - loss: 0.0086 - val_loss: 0.1358\n",
      "Epoch 995/1000\n",
      "103/103 [==============================] - 0s 531us/step - loss: 0.0095 - val_loss: 0.0258\n",
      "Epoch 996/1000\n",
      "103/103 [==============================] - 0s 441us/step - loss: 0.0045 - val_loss: 0.0894\n",
      "Epoch 997/1000\n",
      "103/103 [==============================] - 0s 429us/step - loss: 0.0033 - val_loss: 0.0521\n",
      "Epoch 998/1000\n",
      "103/103 [==============================] - 0s 460us/step - loss: 0.0046 - val_loss: 0.0472\n",
      "Epoch 999/1000\n",
      "103/103 [==============================] - 0s 446us/step - loss: 0.0032 - val_loss: 0.0881\n",
      "Epoch 1000/1000\n",
      "103/103 [==============================] - 0s 421us/step - loss: 0.0035 - val_loss: 0.0798\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_inputs['X'],\n",
    "          train_inputs['target'],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(valid_inputs['X'], valid_inputs['target']),\n",
    "          callbacks=[earlystop ,best_val],\n",
    "          verbose=1 , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(np.array(history.history['val_loss']))+1\n",
    "model.load_weights(str(sat_var) +'_' +  var_name + '_{:02d}.h5'.format(best_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"6\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"6\" halign=\"left\">I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-25 22:00:00</th>\n",
       "      <td>-1.62941079499827590205</td>\n",
       "      <td>-1.45675065634589073760</td>\n",
       "      <td>-1.29110922251646020875</td>\n",
       "      <td>-1.48201799370954900859</td>\n",
       "      <td>-1.58027986123531460905</td>\n",
       "      <td>-1.67994546972559266607</td>\n",
       "      <td>-1.63081453596294556618</td>\n",
       "      <td>-1.47921051178020990235</td>\n",
       "      <td>-1.26864936708214082195</td>\n",
       "      <td>-1.48763295756822699900</td>\n",
       "      <td>-1.58589482509391399567</td>\n",
       "      <td>-1.80066719268599184645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 12:00:00</th>\n",
       "      <td>-1.45675065634589073760</td>\n",
       "      <td>-1.29110922251646020875</td>\n",
       "      <td>-1.48201799370954900859</td>\n",
       "      <td>-1.58027986123531460905</td>\n",
       "      <td>-1.67994546972559266607</td>\n",
       "      <td>-1.37392993943117547317</td>\n",
       "      <td>-1.47921051178020990235</td>\n",
       "      <td>-1.26864936708214082195</td>\n",
       "      <td>-1.48763295756822699900</td>\n",
       "      <td>-1.58589482509391399567</td>\n",
       "      <td>-1.80066719268599184645</td>\n",
       "      <td>-1.62941079499827590205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 14:00:00</th>\n",
       "      <td>-1.29110922251646020875</td>\n",
       "      <td>-1.48201799370954900859</td>\n",
       "      <td>-1.58027986123531460905</td>\n",
       "      <td>-1.67994546972559266607</td>\n",
       "      <td>-1.37392993943117547317</td>\n",
       "      <td>-1.17179124052033722947</td>\n",
       "      <td>-1.26864936708214082195</td>\n",
       "      <td>-1.48763295756822699900</td>\n",
       "      <td>-1.58589482509391399567</td>\n",
       "      <td>-1.80066719268599184645</td>\n",
       "      <td>-1.62941079499827590205</td>\n",
       "      <td>-1.45675065634589073760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 16:00:00</th>\n",
       "      <td>-1.48201799370954900859</td>\n",
       "      <td>-1.58027986123531460905</td>\n",
       "      <td>-1.67994546972559266607</td>\n",
       "      <td>-1.37392993943117547317</td>\n",
       "      <td>-1.17179124052033722947</td>\n",
       "      <td>-1.07493311396128499169</td>\n",
       "      <td>-1.48763295756822699900</td>\n",
       "      <td>-1.58589482509391399567</td>\n",
       "      <td>-1.80066719268599184645</td>\n",
       "      <td>-1.62941079499827590205</td>\n",
       "      <td>-1.45675065634589073760</td>\n",
       "      <td>-1.29110922251646020875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 18:00:00</th>\n",
       "      <td>-1.58027986123531460905</td>\n",
       "      <td>-1.67994546972559266607</td>\n",
       "      <td>-1.37392993943117547317</td>\n",
       "      <td>-1.17179124052033722947</td>\n",
       "      <td>-1.07493311396128499169</td>\n",
       "      <td>-1.25741943936478461907</td>\n",
       "      <td>-1.58589482509391399567</td>\n",
       "      <td>-1.80066719268599184645</td>\n",
       "      <td>-1.62941079499827590205</td>\n",
       "      <td>-1.45675065634589073760</td>\n",
       "      <td>-1.29110922251646020875</td>\n",
       "      <td>-1.48201799370954900859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                               target                          \\\n",
       "feature                                   y                           \n",
       "time step                               t+1                     t+2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -1.62941079499827590205 -1.45675065634589073760   \n",
       "2017-11-26 12:00:00 -1.45675065634589073760 -1.29110922251646020875   \n",
       "2017-11-26 14:00:00 -1.29110922251646020875 -1.48201799370954900859   \n",
       "2017-11-26 16:00:00 -1.48201799370954900859 -1.58027986123531460905   \n",
       "2017-11-26 18:00:00 -1.58027986123531460905 -1.67994546972559266607   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+3                     t+4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -1.29110922251646020875 -1.48201799370954900859   \n",
       "2017-11-26 12:00:00 -1.48201799370954900859 -1.58027986123531460905   \n",
       "2017-11-26 14:00:00 -1.58027986123531460905 -1.67994546972559266607   \n",
       "2017-11-26 16:00:00 -1.67994546972559266607 -1.37392993943117547317   \n",
       "2017-11-26 18:00:00 -1.37392993943117547317 -1.17179124052033722947   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+5                     t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -1.58027986123531460905 -1.67994546972559266607   \n",
       "2017-11-26 12:00:00 -1.67994546972559266607 -1.37392993943117547317   \n",
       "2017-11-26 14:00:00 -1.37392993943117547317 -1.17179124052033722947   \n",
       "2017-11-26 16:00:00 -1.17179124052033722947 -1.07493311396128499169   \n",
       "2017-11-26 18:00:00 -1.07493311396128499169 -1.25741943936478461907   \n",
       "\n",
       "tensor                                    X                          \\\n",
       "feature                               I_dot                           \n",
       "time step                               t-5                     t-4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -1.63081453596294556618 -1.47921051178020990235   \n",
       "2017-11-26 12:00:00 -1.47921051178020990235 -1.26864936708214082195   \n",
       "2017-11-26 14:00:00 -1.26864936708214082195 -1.48763295756822699900   \n",
       "2017-11-26 16:00:00 -1.48763295756822699900 -1.58589482509391399567   \n",
       "2017-11-26 18:00:00 -1.58589482509391399567 -1.80066719268599184645   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 22:00:00 -1.26864936708214082195 -1.48763295756822699900   \n",
       "2017-11-26 12:00:00 -1.48763295756822699900 -1.58589482509391399567   \n",
       "2017-11-26 14:00:00 -1.58589482509391399567 -1.80066719268599184645   \n",
       "2017-11-26 16:00:00 -1.80066719268599184645 -1.62941079499827590205   \n",
       "2017-11-26 18:00:00 -1.62941079499827590205 -1.45675065634589073760   \n",
       "\n",
       "tensor                                                               \n",
       "feature                                                              \n",
       "time step                               t-1                       t  \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-25 22:00:00 -1.58589482509391399567 -1.80066719268599184645  \n",
       "2017-11-26 12:00:00 -1.80066719268599184645 -1.62941079499827590205  \n",
       "2017-11-26 14:00:00 -1.62941079499827590205 -1.45675065634589073760  \n",
       "2017-11-26 16:00:00 -1.45675065634589073760 -1.29110922251646020875  \n",
       "2017-11-26 18:00:00 -1.29110922251646020875 -1.48201799370954900859  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back_dt = dt.datetime.strptime(test_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1)\n",
    "test = df.copy()[test_start_dt:][['e', 'Crc', 'I_dot']]\n",
    "test[['e', 'Crc', 'I_dot']] = X_scaler.transform(test)\n",
    "test_inputs = TimeSeriesTensor(test, var_name, HORIZON, tensor_structure,freq =None)\n",
    "test_inputs.dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "      <th colspan=\"6\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">y</th>\n",
       "      <th colspan=\"6\" halign=\"left\">I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-25 20:00:00</th>\n",
       "      <td>-1.74880250757891464453</td>\n",
       "      <td>-1.58180810367226354352</td>\n",
       "      <td>-1.41344489317619848023</td>\n",
       "      <td>-1.25192571562682242181</td>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.65161723973163598878</td>\n",
       "      <td>-1.58317691026167817192</td>\n",
       "      <td>-1.43534579860644684324</td>\n",
       "      <td>-1.23002481019657361472</td>\n",
       "      <td>-1.44355863814293372549</td>\n",
       "      <td>-1.53937509940079797488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25 22:00:00</th>\n",
       "      <td>-1.58180810367226354352</td>\n",
       "      <td>-1.41344489317619848023</td>\n",
       "      <td>-1.25192571562682242181</td>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.58317691026167817192</td>\n",
       "      <td>-1.43534579860644684324</td>\n",
       "      <td>-1.23002481019657361472</td>\n",
       "      <td>-1.44355863814293372549</td>\n",
       "      <td>-1.53937509940079797488</td>\n",
       "      <td>-1.74880250757891464453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 12:00:00</th>\n",
       "      <td>-1.41344489317619848023</td>\n",
       "      <td>-1.25192571562682242181</td>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.43534579860644684324</td>\n",
       "      <td>-1.23002481019657361472</td>\n",
       "      <td>-1.44355863814293372549</td>\n",
       "      <td>-1.53937509940079797488</td>\n",
       "      <td>-1.74880250757891464453</td>\n",
       "      <td>-1.58180810367226354352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 14:00:00</th>\n",
       "      <td>-1.25192571562682242181</td>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.23002481019657361472</td>\n",
       "      <td>-1.44355863814293372549</td>\n",
       "      <td>-1.53937509940079797488</td>\n",
       "      <td>-1.74880250757891464453</td>\n",
       "      <td>-1.58180810367226354352</td>\n",
       "      <td>-1.41344489317619848023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 16:00:00</th>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.44355863814293372549</td>\n",
       "      <td>-1.53937509940079797488</td>\n",
       "      <td>-1.74880250757891464453</td>\n",
       "      <td>-1.58180810367226354352</td>\n",
       "      <td>-1.41344489317619848023</td>\n",
       "      <td>-1.25192571562682242181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 18:00:00</th>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.53937509940079797488</td>\n",
       "      <td>-1.74880250757891464453</td>\n",
       "      <td>-1.58180810367226354352</td>\n",
       "      <td>-1.41344489317619848023</td>\n",
       "      <td>-1.25192571562682242181</td>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 20:00:00</th>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.74880250757891464453</td>\n",
       "      <td>-1.58180810367226354352</td>\n",
       "      <td>-1.41344489317619848023</td>\n",
       "      <td>-1.25192571562682242181</td>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26 22:00:00</th>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-1.58180810367226354352</td>\n",
       "      <td>-1.41344489317619848023</td>\n",
       "      <td>-1.25192571562682242181</td>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 12:00:00</th>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-1.41344489317619848023</td>\n",
       "      <td>-1.25192571562682242181</td>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 14:00:00</th>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "      <td>-1.25192571562682242181</td>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 16:00:00</th>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "      <td>-0.58668571318058104769</td>\n",
       "      <td>-1.43808341178527587800</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 18:00:00</th>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "      <td>-0.58668571318058104769</td>\n",
       "      <td>-0.77284340933635176096</td>\n",
       "      <td>-1.53389987304321673278</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 20:00:00</th>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "      <td>-0.58668571318058104769</td>\n",
       "      <td>-0.77284340933635176096</td>\n",
       "      <td>-0.91930571440216857226</td>\n",
       "      <td>-1.63108514089041878314</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27 22:00:00</th>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "      <td>-0.58668571318058104769</td>\n",
       "      <td>-0.77284340933635176096</td>\n",
       "      <td>-0.91930571440216857226</td>\n",
       "      <td>-0.75094250390648653593</td>\n",
       "      <td>-1.33268530440151034000</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 12:00:00</th>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "      <td>-0.58668571318058104769</td>\n",
       "      <td>-0.77284340933635176096</td>\n",
       "      <td>-0.91930571440216857226</td>\n",
       "      <td>-0.75094250390648653593</td>\n",
       "      <td>-0.20752628791430172139</td>\n",
       "      <td>-1.13557715552735749576</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 14:00:00</th>\n",
       "      <td>-0.58668571318058104769</td>\n",
       "      <td>-0.77284340933635176096</td>\n",
       "      <td>-0.91930571440216857226</td>\n",
       "      <td>-0.75094250390648653593</td>\n",
       "      <td>-0.20752628791430172139</td>\n",
       "      <td>0.08266070903620308108</td>\n",
       "      <td>-1.04112950086082411971</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 16:00:00</th>\n",
       "      <td>-0.77284340933635176096</td>\n",
       "      <td>-0.91930571440216857226</td>\n",
       "      <td>-0.75094250390648653593</td>\n",
       "      <td>-0.20752628791430172139</td>\n",
       "      <td>0.08266070903620308108</td>\n",
       "      <td>0.06623502996476229865</td>\n",
       "      <td>-1.21907435748125791974</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "      <td>-0.58668571318058104769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 18:00:00</th>\n",
       "      <td>-0.91930571440216857226</td>\n",
       "      <td>-0.75094250390648653593</td>\n",
       "      <td>-0.20752628791430172139</td>\n",
       "      <td>0.08266070903620308108</td>\n",
       "      <td>0.06623502996476229865</td>\n",
       "      <td>-0.13497953867763365365</td>\n",
       "      <td>-1.32857888463326689887</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "      <td>-0.58668571318058104769</td>\n",
       "      <td>-0.77284340933635176096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 20:00:00</th>\n",
       "      <td>-0.75094250390648653593</td>\n",
       "      <td>-0.20752628791430172139</td>\n",
       "      <td>0.08266070903620308108</td>\n",
       "      <td>0.06623502996476229865</td>\n",
       "      <td>-0.13497953867763365365</td>\n",
       "      <td>-0.34303814026710222063</td>\n",
       "      <td>-1.28751468695121573660</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "      <td>-0.58668571318058104769</td>\n",
       "      <td>-0.77284340933635176096</td>\n",
       "      <td>-0.91930571440216857226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:00:00</th>\n",
       "      <td>-0.20752628791430172139</td>\n",
       "      <td>0.08266070903620308108</td>\n",
       "      <td>0.06623502996476229865</td>\n",
       "      <td>-0.13497953867763365365</td>\n",
       "      <td>-0.34303814026710222063</td>\n",
       "      <td>-0.10349698711956784036</td>\n",
       "      <td>-0.85086538493144447948</td>\n",
       "      <td>-0.61132423178774231154</td>\n",
       "      <td>-0.58668571318058104769</td>\n",
       "      <td>-0.77284340933635176096</td>\n",
       "      <td>-0.91930571440216857226</td>\n",
       "      <td>-0.75094250390648653593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                               target                          \\\n",
       "feature                                   y                           \n",
       "time step                               t+1                     t+2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 20:00:00 -1.74880250757891464453 -1.58180810367226354352   \n",
       "2017-11-25 22:00:00 -1.58180810367226354352 -1.41344489317619848023   \n",
       "2017-11-26 12:00:00 -1.41344489317619848023 -1.25192571562682242181   \n",
       "2017-11-26 14:00:00 -1.25192571562682242181 -1.43808341178527587800   \n",
       "2017-11-26 16:00:00 -1.43808341178527587800 -1.53389987304321673278   \n",
       "2017-11-26 18:00:00 -1.53389987304321673278 -1.63108514089041878314   \n",
       "2017-11-26 20:00:00 -1.63108514089041878314 -1.33268530440151034000   \n",
       "2017-11-26 22:00:00 -1.33268530440151034000 -1.13557715552735749576   \n",
       "2017-11-27 12:00:00 -1.13557715552735749576 -1.04112950086082411971   \n",
       "2017-11-27 14:00:00 -1.04112950086082411971 -1.21907435748125791974   \n",
       "2017-11-27 16:00:00 -1.21907435748125791974 -1.32857888463326689887   \n",
       "2017-11-27 18:00:00 -1.32857888463326689887 -1.28751468695121573660   \n",
       "2017-11-27 20:00:00 -1.28751468695121573660 -0.85086538493144447948   \n",
       "2017-11-27 22:00:00 -0.85086538493144447948 -0.61132423178774231154   \n",
       "2017-11-28 12:00:00 -0.61132423178774231154 -0.58668571318058104769   \n",
       "2017-11-28 14:00:00 -0.58668571318058104769 -0.77284340933635176096   \n",
       "2017-11-28 16:00:00 -0.77284340933635176096 -0.91930571440216857226   \n",
       "2017-11-28 18:00:00 -0.91930571440216857226 -0.75094250390648653593   \n",
       "2017-11-28 20:00:00 -0.75094250390648653593 -0.20752628791430172139   \n",
       "2017-11-28 22:00:00 -0.20752628791430172139  0.08266070903620308108   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+3                     t+4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 20:00:00 -1.41344489317619848023 -1.25192571562682242181   \n",
       "2017-11-25 22:00:00 -1.25192571562682242181 -1.43808341178527587800   \n",
       "2017-11-26 12:00:00 -1.43808341178527587800 -1.53389987304321673278   \n",
       "2017-11-26 14:00:00 -1.53389987304321673278 -1.63108514089041878314   \n",
       "2017-11-26 16:00:00 -1.63108514089041878314 -1.33268530440151034000   \n",
       "2017-11-26 18:00:00 -1.33268530440151034000 -1.13557715552735749576   \n",
       "2017-11-26 20:00:00 -1.13557715552735749576 -1.04112950086082411971   \n",
       "2017-11-26 22:00:00 -1.04112950086082411971 -1.21907435748125791974   \n",
       "2017-11-27 12:00:00 -1.21907435748125791974 -1.32857888463326689887   \n",
       "2017-11-27 14:00:00 -1.32857888463326689887 -1.28751468695121573660   \n",
       "2017-11-27 16:00:00 -1.28751468695121573660 -0.85086538493144447948   \n",
       "2017-11-27 18:00:00 -0.85086538493144447948 -0.61132423178774231154   \n",
       "2017-11-27 20:00:00 -0.61132423178774231154 -0.58668571318058104769   \n",
       "2017-11-27 22:00:00 -0.58668571318058104769 -0.77284340933635176096   \n",
       "2017-11-28 12:00:00 -0.77284340933635176096 -0.91930571440216857226   \n",
       "2017-11-28 14:00:00 -0.91930571440216857226 -0.75094250390648653593   \n",
       "2017-11-28 16:00:00 -0.75094250390648653593 -0.20752628791430172139   \n",
       "2017-11-28 18:00:00 -0.20752628791430172139  0.08266070903620308108   \n",
       "2017-11-28 20:00:00  0.08266070903620308108  0.06623502996476229865   \n",
       "2017-11-28 22:00:00  0.06623502996476229865 -0.13497953867763365365   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+5                     t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 20:00:00 -1.43808341178527587800 -1.53389987304321673278   \n",
       "2017-11-25 22:00:00 -1.53389987304321673278 -1.63108514089041878314   \n",
       "2017-11-26 12:00:00 -1.63108514089041878314 -1.33268530440151034000   \n",
       "2017-11-26 14:00:00 -1.33268530440151034000 -1.13557715552735749576   \n",
       "2017-11-26 16:00:00 -1.13557715552735749576 -1.04112950086082411971   \n",
       "2017-11-26 18:00:00 -1.04112950086082411971 -1.21907435748125791974   \n",
       "2017-11-26 20:00:00 -1.21907435748125791974 -1.32857888463326689887   \n",
       "2017-11-26 22:00:00 -1.32857888463326689887 -1.28751468695121573660   \n",
       "2017-11-27 12:00:00 -1.28751468695121573660 -0.85086538493144447948   \n",
       "2017-11-27 14:00:00 -0.85086538493144447948 -0.61132423178774231154   \n",
       "2017-11-27 16:00:00 -0.61132423178774231154 -0.58668571318058104769   \n",
       "2017-11-27 18:00:00 -0.58668571318058104769 -0.77284340933635176096   \n",
       "2017-11-27 20:00:00 -0.77284340933635176096 -0.91930571440216857226   \n",
       "2017-11-27 22:00:00 -0.91930571440216857226 -0.75094250390648653593   \n",
       "2017-11-28 12:00:00 -0.75094250390648653593 -0.20752628791430172139   \n",
       "2017-11-28 14:00:00 -0.20752628791430172139  0.08266070903620308108   \n",
       "2017-11-28 16:00:00  0.08266070903620308108  0.06623502996476229865   \n",
       "2017-11-28 18:00:00  0.06623502996476229865 -0.13497953867763365365   \n",
       "2017-11-28 20:00:00 -0.13497953867763365365 -0.34303814026710222063   \n",
       "2017-11-28 22:00:00 -0.34303814026710222063 -0.10349698711956784036   \n",
       "\n",
       "tensor                                    X                          \\\n",
       "feature                               I_dot                           \n",
       "time step                               t-5                     t-4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 20:00:00 -1.65161723973163598878 -1.58317691026167817192   \n",
       "2017-11-25 22:00:00 -1.58317691026167817192 -1.43534579860644684324   \n",
       "2017-11-26 12:00:00 -1.43534579860644684324 -1.23002481019657361472   \n",
       "2017-11-26 14:00:00 -1.23002481019657361472 -1.44355863814293372549   \n",
       "2017-11-26 16:00:00 -1.44355863814293372549 -1.53937509940079797488   \n",
       "2017-11-26 18:00:00 -1.53937509940079797488 -1.74880250757891464453   \n",
       "2017-11-26 20:00:00 -1.74880250757891464453 -1.58180810367226354352   \n",
       "2017-11-26 22:00:00 -1.58180810367226354352 -1.41344489317619848023   \n",
       "2017-11-27 12:00:00 -1.41344489317619848023 -1.25192571562682242181   \n",
       "2017-11-27 14:00:00 -1.25192571562682242181 -1.43808341178527587800   \n",
       "2017-11-27 16:00:00 -1.43808341178527587800 -1.53389987304321673278   \n",
       "2017-11-27 18:00:00 -1.53389987304321673278 -1.63108514089041878314   \n",
       "2017-11-27 20:00:00 -1.63108514089041878314 -1.33268530440151034000   \n",
       "2017-11-27 22:00:00 -1.33268530440151034000 -1.13557715552735749576   \n",
       "2017-11-28 12:00:00 -1.13557715552735749576 -1.04112950086082411971   \n",
       "2017-11-28 14:00:00 -1.04112950086082411971 -1.21907435748125791974   \n",
       "2017-11-28 16:00:00 -1.21907435748125791974 -1.32857888463326689887   \n",
       "2017-11-28 18:00:00 -1.32857888463326689887 -1.28751468695121573660   \n",
       "2017-11-28 20:00:00 -1.28751468695121573660 -0.85086538493144447948   \n",
       "2017-11-28 22:00:00 -0.85086538493144447948 -0.61132423178774231154   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-3                     t-2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-25 20:00:00 -1.43534579860644684324 -1.23002481019657361472   \n",
       "2017-11-25 22:00:00 -1.23002481019657361472 -1.44355863814293372549   \n",
       "2017-11-26 12:00:00 -1.44355863814293372549 -1.53937509940079797488   \n",
       "2017-11-26 14:00:00 -1.53937509940079797488 -1.74880250757891464453   \n",
       "2017-11-26 16:00:00 -1.74880250757891464453 -1.58180810367226354352   \n",
       "2017-11-26 18:00:00 -1.58180810367226354352 -1.41344489317619848023   \n",
       "2017-11-26 20:00:00 -1.41344489317619848023 -1.25192571562682242181   \n",
       "2017-11-26 22:00:00 -1.25192571562682242181 -1.43808341178527587800   \n",
       "2017-11-27 12:00:00 -1.43808341178527587800 -1.53389987304321673278   \n",
       "2017-11-27 14:00:00 -1.53389987304321673278 -1.63108514089041878314   \n",
       "2017-11-27 16:00:00 -1.63108514089041878314 -1.33268530440151034000   \n",
       "2017-11-27 18:00:00 -1.33268530440151034000 -1.13557715552735749576   \n",
       "2017-11-27 20:00:00 -1.13557715552735749576 -1.04112950086082411971   \n",
       "2017-11-27 22:00:00 -1.04112950086082411971 -1.21907435748125791974   \n",
       "2017-11-28 12:00:00 -1.21907435748125791974 -1.32857888463326689887   \n",
       "2017-11-28 14:00:00 -1.32857888463326689887 -1.28751468695121573660   \n",
       "2017-11-28 16:00:00 -1.28751468695121573660 -0.85086538493144447948   \n",
       "2017-11-28 18:00:00 -0.85086538493144447948 -0.61132423178774231154   \n",
       "2017-11-28 20:00:00 -0.61132423178774231154 -0.58668571318058104769   \n",
       "2017-11-28 22:00:00 -0.58668571318058104769 -0.77284340933635176096   \n",
       "\n",
       "tensor                                                               \n",
       "feature                                                              \n",
       "time step                               t-1                       t  \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-25 20:00:00 -1.44355863814293372549 -1.53937509940079797488  \n",
       "2017-11-25 22:00:00 -1.53937509940079797488 -1.74880250757891464453  \n",
       "2017-11-26 12:00:00 -1.74880250757891464453 -1.58180810367226354352  \n",
       "2017-11-26 14:00:00 -1.58180810367226354352 -1.41344489317619848023  \n",
       "2017-11-26 16:00:00 -1.41344489317619848023 -1.25192571562682242181  \n",
       "2017-11-26 18:00:00 -1.25192571562682242181 -1.43808341178527587800  \n",
       "2017-11-26 20:00:00 -1.43808341178527587800 -1.53389987304321673278  \n",
       "2017-11-26 22:00:00 -1.53389987304321673278 -1.63108514089041878314  \n",
       "2017-11-27 12:00:00 -1.63108514089041878314 -1.33268530440151034000  \n",
       "2017-11-27 14:00:00 -1.33268530440151034000 -1.13557715552735749576  \n",
       "2017-11-27 16:00:00 -1.13557715552735749576 -1.04112950086082411971  \n",
       "2017-11-27 18:00:00 -1.04112950086082411971 -1.21907435748125791974  \n",
       "2017-11-27 20:00:00 -1.21907435748125791974 -1.32857888463326689887  \n",
       "2017-11-27 22:00:00 -1.32857888463326689887 -1.28751468695121573660  \n",
       "2017-11-28 12:00:00 -1.28751468695121573660 -0.85086538493144447948  \n",
       "2017-11-28 14:00:00 -0.85086538493144447948 -0.61132423178774231154  \n",
       "2017-11-28 16:00:00 -0.61132423178774231154 -0.58668571318058104769  \n",
       "2017-11-28 18:00:00 -0.58668571318058104769 -0.77284340933635176096  \n",
       "2017-11-28 20:00:00 -0.77284340933635176096 -0.91930571440216857226  \n",
       "2017-11-28 22:00:00 -0.91930571440216857226 -0.75094250390648653593  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs.dataframe.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 12)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs.dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7569746 , -1.0821881 , -1.1455489 , -1.0868398 , -0.9877486 ,\n",
       "        -0.8878964 ],\n",
       "       [-0.78947556, -1.1310291 , -1.2008945 , -1.1442451 , -1.0454767 ,\n",
       "        -0.9453281 ],\n",
       "       [-0.79745567, -1.1420689 , -1.2124467 , -1.1554432 , -1.0561633 ,\n",
       "        -0.9555638 ],\n",
       "       [-0.7850248 , -1.1222794 , -1.1888913 , -1.1300706 , -1.029953  ,\n",
       "        -0.9290093 ],\n",
       "       [-0.7570258 , -1.0791954 , -1.1390588 , -1.0775734 , -0.9765948 ,\n",
       "        -0.8755621 ],\n",
       "       [-0.75204813, -1.0727937 , -1.132901  , -1.0720955 , -0.97175765,\n",
       "        -0.871214  ],\n",
       "       [-0.7591648 , -1.0846236 , -1.1474504 , -1.0881176 , -0.98853946,\n",
       "        -0.888358  ],\n",
       "       [-0.7739749 , -1.108603  , -1.1763425 , -1.1194634 , -1.021033  ,\n",
       "        -0.92131704],\n",
       "       [-0.7528394 , -1.0779111 , -1.142637  , -1.0853668 , -0.9873471 ,\n",
       "        -0.88818693],\n",
       "       [-0.71252626, -1.0187578 , -1.0771397 , -1.0187876 , -0.9214401 ,\n",
       "        -0.82335836],\n",
       "       [-0.6694829 , -0.95574784, -1.0076566 , -0.9485532 , -0.8523303 ,\n",
       "        -0.75574815],\n",
       "       [-0.65395373, -0.93592334, -0.98893666, -0.9323906 , -0.83855486,\n",
       "        -0.74378234],\n",
       "       [-0.6560685 , -0.94252944, -0.99998236, -0.946815  , -0.85522646,\n",
       "        -0.7618259 ],\n",
       "       [-0.65235656, -0.9404124 , -1.0012264 , -0.9511904 , -0.8619051 ,\n",
       "        -0.7700143 ],\n",
       "       [-0.5975194 , -0.86003935, -0.9128324 , -0.8624131 , -0.7752296 ,\n",
       "        -0.68583536],\n",
       "       [-0.52168405, -0.74856097, -0.79020214, -0.7397061 , -0.65618306,\n",
       "        -0.57104456],\n",
       "       [-0.45751536, -0.65552366, -0.68961525, -0.6411439 , -0.56265795,\n",
       "        -0.48271316],\n",
       "       [-0.4292514 , -0.6187606 , -0.65492094, -0.6122323 , -0.5397456 ,\n",
       "        -0.4647426 ],\n",
       "       [-0.4287851 , -0.62402266, -0.66764   , -0.63161117, -0.5641421 ,\n",
       "        -0.4925551 ],\n",
       "       [-0.40847582, -0.5976496 , -0.64295685, -0.61144465, -0.54869735,\n",
       "        -0.48102513]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp    h             prediction                  actual\n",
      "0 2017-11-25 20:00:00  t+1 0.00000000020414892773 -0.00000000005464513333\n",
      "1 2017-11-25 22:00:00  t+1 0.00000000019566856441 -0.00000000001107188976\n",
      "2 2017-11-26 12:00:00  t+1 0.00000000019358634373  0.00000000003285851155\n",
      "3 2017-11-26 14:00:00  t+1 0.00000000019682987943  0.00000000007500312418\n",
      "4 2017-11-26 16:00:00  t+1 0.00000000020413556820  0.00000000002642967233\n",
      "              timestamp    h             prediction                 actual\n",
      "115 2017-11-28 14:00:00  t+6 0.00000000025266297121 0.00000000042323191502\n",
      "116 2017-11-28 16:00:00  t+6 0.00000000027571096309 0.00000000041894602221\n",
      "117 2017-11-28 18:00:00  t+6 0.00000000028039995659 0.00000000036644383529\n",
      "118 2017-11-28 20:00:00  t+6 0.00000000027314293914 0.00000000031215585969\n",
      "119 2017-11-28 22:00:00  t+6 0.00000000027615141551 0.00000000037465846317\n",
      "(120, 4)\n"
     ]
    }
   ],
   "source": [
    "eval_df = create_evaluation_df(predictions, test_inputs, HORIZON, y_scalar)\n",
    "print(eval_df.head())\n",
    "print(eval_df.tail())\n",
    "print(eval_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h\n",
       "t+1   6.84941162099377010009\n",
       "t+2   3.90953440098610593978\n",
       "t+3   3.43766707274219118773\n",
       "t+4   3.69418916099372207640\n",
       "t+5   4.75991103159519468591\n",
       "t+6   6.01544808301865963074\n",
       "Name: APE, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual']\n",
    "eval_df.groupby('h')['APE'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.502439619201988e-11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(eval_df['prediction'], eval_df['actual'])\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "a = mean_absolute_error(eval_df['prediction'], eval_df['actual'])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot actuals vs predictions at each horizon for first week of the test period. As is to be expected, predictions for one step ahead (*t+1*) are more accurate than those for 2 or 3 steps ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df = eval_df[(eval_df.h=='t+1')][['timestamp', 'actual']]\n",
    "for t in range(1, HORIZON+1):\n",
    "    plot_df['t+'+str(t)] = eval_df[ (eval_df.h=='t+'+str(t))]['prediction'].values\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.plot(plot_df['timestamp'], plot_df['actual'], color='red', linewidth=4.0)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(plot_df['timestamp'], plot_df['t+1'], color='blue', linewidth=4.0, alpha=0.75)\n",
    "ax.plot(plot_df['timestamp'], plot_df['t+2'], color='blue', linewidth=3.0, alpha=0.5)\n",
    "ax.plot(plot_df['timestamp'], plot_df['t+3'], color='blue', linewidth=2.0, alpha=0.25)\n",
    "plt.xlabel('timestamp', fontsize=12)\n",
    "plt.ylabel('load', fontsize=12)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take input here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:00:00</th>\n",
       "      <td>0.00000000020572285490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 12:00:00</th>\n",
       "      <td>0.00000000034751447537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 14:00:00</th>\n",
       "      <td>0.00000000042323191502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 16:00:00</th>\n",
       "      <td>0.00000000041894602221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 18:00:00</th>\n",
       "      <td>0.00000000036644383529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 20:00:00</th>\n",
       "      <td>0.00000000031215585969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     I_dot\n",
       "Epoch_Time_of_Clock                       \n",
       "2017-11-28 22:00:00 0.00000000020572285490\n",
       "2017-11-29 12:00:00 0.00000000034751447537\n",
       "2017-11-29 14:00:00 0.00000000042323191502\n",
       "2017-11-29 16:00:00 0.00000000041894602221\n",
       "2017-11-29 18:00:00 0.00000000036644383529\n",
       "2017-11-29 20:00:00 0.00000000031215585969"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df.iloc[156:162  , :]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['e', 'Crc', 'I_dot'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key , value in enumerate(columns):\n",
    "    new_df[value] = a[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.dropna( how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-28 22:00:00</th>\n",
       "      <td>0.00000000020572285490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 12:00:00</th>\n",
       "      <td>0.00000000034751447537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 14:00:00</th>\n",
       "      <td>0.00000000042323191502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 16:00:00</th>\n",
       "      <td>0.00000000041894602221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 18:00:00</th>\n",
       "      <td>0.00000000036644383529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-29 20:00:00</th>\n",
       "      <td>0.00000000031215585969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     I_dot\n",
       "Epoch_Time_of_Clock                       \n",
       "2017-11-28 22:00:00 0.00000000020572285490\n",
       "2017-11-29 12:00:00 0.00000000034751447537\n",
       "2017-11-29 14:00:00 0.00000000042323191502\n",
       "2017-11-29 16:00:00 0.00000000041894602221\n",
       "2017-11-29 18:00:00 0.00000000036644383529\n",
       "2017-11-29 20:00:00 0.00000000031215585969"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 11, 29)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating index for output\n",
    "import datetime\n",
    "date = new_df.index.date[0]\n",
    "date + datetime.timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = new_df.index + datetime.timedelta(days =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2017-11-29 22:00:00', '2017-11-30 12:00:00',\n",
       "               '2017-11-30 14:00:00', '2017-11-30 16:00:00',\n",
       "               '2017-11-30 18:00:00', '2017-11-30 20:00:00'],\n",
       "              dtype='datetime64[ns]', name='Epoch_Time_of_Clock', freq=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.index= date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['I_dot'], dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     I_dot\n",
      "Epoch_Time_of_Clock                       \n",
      "2017-11-29 22:00:00 0.00000000020572285490\n",
      "2017-11-30 12:00:00 0.00000000034751447537\n",
      "2017-11-30 14:00:00 0.00000000042323191502\n",
      "2017-11-30 16:00:00 0.00000000041894602221\n",
      "2017-11-30 18:00:00 0.00000000036644383529\n",
      "2017-11-30 20:00:00 0.00000000031215585969\n",
      "Index(['I_dot'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(new_df)\n",
    "print(new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      I_dot\n",
      "Epoch_Time_of_Clock                        \n",
      "2017-11-29 22:00:00 -0.75094250390648653593\n",
      "2017-11-30 12:00:00 -0.20752628791430172139\n",
      "2017-11-30 14:00:00  0.08266070903620308108\n",
      "2017-11-30 16:00:00  0.06623502996476229865\n",
      "2017-11-30 18:00:00 -0.13497953867763365365\n"
     ]
    }
   ],
   "source": [
    "freq = None\n",
    "idx_tuples = []\n",
    "drop_incomplete  = True\n",
    "new_df[['e', 'Crc', 'I_dot']] = X_scaler.transform(new_df)\n",
    "new_new_df = new_df.copy()\n",
    "tensor_structure={'X':(range(-T+1, 1), ['e', 'Crc', 'I_dot'])}\n",
    "for name, structure in tensor_structure.items():\n",
    "        rng = structure[0]\n",
    "        dataset_cols = structure[1]\n",
    "        for col in dataset_cols:\n",
    "        # do not shift non-sequential 'static' features\n",
    "            if rng is None:\n",
    "                new_df['context_'+col] = new_df[col]\n",
    "                idx_tuples.append((name, col, 'static'))\n",
    "            else:\n",
    "                for t in rng:\n",
    "                    sign = '+' if t > 0 else ''\n",
    "                    shift = str(t) if t != 0 else ''\n",
    "                    period = 't'+sign+shift\n",
    "                    shifted_col = name+'_'+col+'_'+ period\n",
    "                    new_new_df[shifted_col] = new_new_df[col].shift(t*-1, freq=freq)\n",
    "                    idx_tuples.append((name, col, period))\n",
    "        new_new_df = new_new_df.drop(new_df.columns, axis=1)\n",
    "        idx = pd.MultiIndex.from_tuples(idx_tuples, names=['tensor', 'feature', 'time step'])\n",
    "        print(new_df.head())\n",
    "        new_new_df.columns = idx\n",
    "        if drop_incomplete:\n",
    "            new_new_df = new_new_df.dropna(how='any')\n",
    "            \n",
    "inputs = {}           \n",
    "for name, structure in tensor_structure.items():\n",
    "    rng = structure[0]\n",
    "    cols = structure[1]\n",
    "    tensor = new_new_df[name][cols].as_matrix()\n",
    "    if rng is None:\n",
    "        tensor = tensor.reshape(tensor.shape[0], len(cols))\n",
    "    else:\n",
    "        tensor = tensor.reshape(tensor.shape[0], len(cols), len(rng))\n",
    "        tensor = np.transpose(tensor, axes=[0, 2, 1])\n",
    "    inputs[name] = tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor                                    X                          \\\n",
      "feature                               I_dot                           \n",
      "time step                               t-5                     t-4   \n",
      "Epoch_Time_of_Clock                                                   \n",
      "2017-11-30 20:00:00 -0.75094250390648653593 -0.20752628791430172139   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                              t-3                    t-2   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-30 20:00:00 0.08266070903620308108 0.06623502996476229865   \n",
      "\n",
      "tensor                                                               \n",
      "feature                                                              \n",
      "time step                               t-1                       t  \n",
      "Epoch_Time_of_Clock                                                  \n",
      "2017-11-30 20:00:00 -0.13497953867763365365 -0.34303814026710222063  \n",
      "[[[-0.7509425039064865 ]\n",
      "  [-0.20752628791430172]\n",
      "  [ 0.08266070903620308]\n",
      "  [ 0.0662350299647623 ]\n",
      "  [-0.13497953867763365]\n",
      "  [-0.3430381402671022 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(new_new_df)\n",
    "print(inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08822396 , -0.13130815 , -0.14239305 , -0.1350485  ,\n",
       "        -0.119207084, -0.1013129  ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictions[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08822396 , -0.13130815 , -0.14239305 , -0.1350485  ,\n",
       "       -0.119207084, -0.1013129  ], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_dot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.08822395652532577515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.13130815327167510986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.14239305257797241211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.13504849374294281006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.11920708417892456055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.10131289809942245483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    I_dot\n",
       "0 -0.08822395652532577515\n",
       "1 -0.13130815327167510986\n",
       "2 -0.14239305257797241211\n",
       "3 -0.13504849374294281006\n",
       "4 -0.11920708417892456055\n",
       "5 -0.10131289809942245483"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(results , columns = [var_name])\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-29 22:00:00</th>\n",
       "      <td>-0.08822395652532577515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30 12:00:00</th>\n",
       "      <td>-0.13130815327167510986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30 14:00:00</th>\n",
       "      <td>-0.14239305257797241211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30 16:00:00</th>\n",
       "      <td>-0.13504849374294281006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30 18:00:00</th>\n",
       "      <td>-0.11920708417892456055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30 20:00:00</th>\n",
       "      <td>-0.10131289809942245483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      I_dot\n",
       "Epoch_Time_of_Clock                        \n",
       "2017-11-29 22:00:00 -0.08822395652532577515\n",
       "2017-11-30 12:00:00 -0.13130815327167510986\n",
       "2017-11-30 14:00:00 -0.14239305257797241211\n",
       "2017-11-30 16:00:00 -0.13504849374294281006\n",
       "2017-11-30 18:00:00 -0.11920708417892456055\n",
       "2017-11-30 20:00:00 -0.10131289809942245483"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.index = date\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[var_name] = y_scalar.inverse_transform(res_df[[var_name]])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final generated output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-29 22:00:00</th>\n",
       "      <td>0.00000000037864358871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30 12:00:00</th>\n",
       "      <td>0.00000000036740180342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30 14:00:00</th>\n",
       "      <td>0.00000000036450945040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30 16:00:00</th>\n",
       "      <td>0.00000000036642583412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30 18:00:00</th>\n",
       "      <td>0.00000000037055927771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30 20:00:00</th>\n",
       "      <td>0.00000000037522834839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     I_dot\n",
       "Epoch_Time_of_Clock                       \n",
       "2017-11-29 22:00:00 0.00000000037864358871\n",
       "2017-11-30 12:00:00 0.00000000036740180342\n",
       "2017-11-30 14:00:00 0.00000000036450945040\n",
       "2017-11-30 16:00:00 0.00000000036642583412\n",
       "2017-11-30 18:00:00 0.00000000037055927771\n",
       "2017-11-30 20:00:00 0.00000000037522834839"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final generated ouput\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('SA1_I_dot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
