{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi step model (encoder-decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from collections import UserDict\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "from common.utils import load_data, mape, TimeSeriesTensor, create_evaluation_df\n",
    "\n",
    "pd.options.display.float_format = '{:,.20f}'.format\n",
    "np.set_printoptions(precision=20)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paras = {\n",
    "    'M0':['M0', 'e','Del_n' , 'sqrt_A'],\n",
    "    'e':['OMEGA','i0','omega','I_dot','Cus','Crs','e' , 'M0'],\n",
    "    'sqrt_A':['Cuc','Crc','Del_n','Crs','sqrt_A','OMEGA_dot','Cus'],\n",
    "    'OMEGA':['OMEGA','e','i0','omega'],\n",
    "    'i0':['e','i0','omega','OMEGA' ,'I_dot'],\n",
    "    'omega':['omega','e','OMEGA','i0'],\n",
    "    'I_dot':['I_dot','e','Crs','Cuc'],\n",
    "    'Cic':['M0','Cic'],\n",
    "    'Cis':['Cis'],\n",
    "    'OMEGA_dot':['OMEGA_dot'],\n",
    "    'Cuc':['Cuc','e','sqrt_A','I_dot','Crs'],\n",
    "    'Cus':['Cus','sqrt_A','OMEGA_dot','Crc','Del_n','Cus'],\n",
    "    'Crc':['Crc','sqrt_A','OMEGA_dot','Cus','Del_n'],\n",
    "    'Crs':['Crs','e','sqrt_A','I_dot','Cuc'],\n",
    "    'Del_n':['Crc','sqrt_A','OMEGA_dot','Cus','Del_n'],\n",
    "    'Codes' : ['Codes']    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = 'OMEGA_dot'\n",
    "sat_var = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas._libs.tslib.Timestamp'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OMEGA_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 00:00:00</th>\n",
       "      <td>-0.00000000977520683609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 01:00:00</th>\n",
       "      <td>-0.00000000957752760144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 02:00:00</th>\n",
       "      <td>-0.00000000934083602756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 03:00:00</th>\n",
       "      <td>-0.00000000906539628995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 04:00:00</th>\n",
       "      <td>-0.00000000875378145918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  OMEGA_dot\n",
       "Epoch_Time_of_Clock                        \n",
       "2017-11-02 00:00:00 -0.00000000977520683609\n",
       "2017-11-02 01:00:00 -0.00000000957752760144\n",
       "2017-11-02 02:00:00 -0.00000000934083602756\n",
       "2017-11-02 03:00:00 -0.00000000906539628995\n",
       "2017-11-02 04:00:00 -0.00000000875378145918"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hourinterpol_21.csv\" , parse_dates = True)\n",
    "a = pd.to_datetime(df['Epoch_Time_of_Clock'])\n",
    "print(type(a[0]))\n",
    "#df = df.drop(['Unnamed: 0', 'Unnamed: 0.1' ,'sqrt_A'  ,'PRN','SV_Clock_Bias', 'SV_Clock_Drift', 'SV_Clock_Drift_Rate', 'IODE', 'Crs',\n",
    "#       'Del_n', 'Cuc','Cus','Toe', 'Cic', \n",
    "#       'Cis', 'Crc', 'M0', 'OMEGA_dot', 'I_dot', 'Codes', 'GPS_week',\n",
    "#       'L2_P_Data_flag', 'SV_accuracy', 'SV_health', 'Tgd', 'IODC', 'T_Tx',\n",
    "#       'Fit_Interval' ,'Epoch_Time_of_Clock' ],axis =1 )\n",
    "df = df.loc[:,Paras[var_name]]\n",
    "#df.head()\n",
    "#df = df.set_index(['Epoch_Time_of_Clock'])\n",
    "df = df.set_index(a)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "test = df.iloc[265:408,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter number of entries per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''entry = 6\n",
    "print(df.shape[0])\n",
    "no_of_entries = df.shape[0]//entry\n",
    "valid = (no_of_entries * 70)//100\n",
    "test = (no_of_entries * 85)//100\n",
    "indexes = df.index\n",
    "#print(valid , test , indexes)\n",
    "valid_start_dt = indexes[int(valid)*int(entry)] \n",
    "test_start_dt = indexes [int(test)*int(entry)] \n",
    "test_start_dt = str(test_start_dt)\n",
    "valid_start_dt = str(valid_start_dt)\n",
    "print(test_start_dt,valid_start_dt)\n",
    "print(type(test_start_dt))'''\n",
    "valid_start_dt = '2017-11-07 00:00:00'\n",
    "test_start_dt = '2017-11-11 00:00:00'\n",
    "#end_dt = '2017-11-15 00:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load data into Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter lag and no. of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"total = len(df)\n",
    "t = total*70/100\n",
    "t = round(t)\n",
    "indexes = df.index\n",
    "valid_start_dt = str(indexes[t])\n",
    "t = total*85/100\n",
    "t = round(t)\n",
    "test_start_dt = str(indexes[t])\n",
    "print(valid_start_dt , test_start_dt)\n",
    "\"\"\"\n",
    "T = 24\n",
    "HORIZON = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training set containing only the model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OMEGA_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 00:00:00</th>\n",
       "      <td>-0.00000000977520683609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 01:00:00</th>\n",
       "      <td>-0.00000000957752760144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 02:00:00</th>\n",
       "      <td>-0.00000000934083602756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 03:00:00</th>\n",
       "      <td>-0.00000000906539628995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 04:00:00</th>\n",
       "      <td>-0.00000000875378145918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  OMEGA_dot\n",
       "Epoch_Time_of_Clock                        \n",
       "2017-11-02 00:00:00 -0.00000000977520683609\n",
       "2017-11-02 01:00:00 -0.00000000957752760144\n",
       "2017-11-02 02:00:00 -0.00000000934083602756\n",
       "2017-11-02 03:00:00 -0.00000000906539628995\n",
       "2017-11-02 04:00:00 -0.00000000875378145918"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.copy()[df.index < valid_start_dt][Paras[var_name]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OMEGA_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-06 18:00:00</th>\n",
       "      <td>-0.00000000949201484960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 19:00:00</th>\n",
       "      <td>-0.00000000952032852389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 20:00:00</th>\n",
       "      <td>-0.00000000950916032499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 21:00:00</th>\n",
       "      <td>-0.00000000945877773141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 22:00:00</th>\n",
       "      <td>-0.00000000936944822168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 23:00:00</th>\n",
       "      <td>-0.00000000924143927430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  OMEGA_dot\n",
       "Epoch_Time_of_Clock                        \n",
       "2017-11-06 18:00:00 -0.00000000949201484960\n",
       "2017-11-06 19:00:00 -0.00000000952032852389\n",
       "2017-11-06 20:00:00 -0.00000000950916032499\n",
       "2017-11-06 21:00:00 -0.00000000945877773141\n",
       "2017-11-06 22:00:00 -0.00000000936944822168\n",
       "2017-11-06 23:00:00 -0.00000000924143927430"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data to be in range (0, 1). This transformation should be calibrated on the training set only. This is to prevent information from the validation or test sets leaking into the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter variable to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y_scalar = StandardScaler()\n",
    "y_scalar.fit(train[[var_name]])\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "train[Paras[var_name]] = X_scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_x = open(var_name+' X_scaler2_5D_OME_dot.pkl','wb')\n",
    "pickle.dump(X_scaler, file_x)\n",
    "            \n",
    "file_y = open(var_name+' y_scalar2_5D_OME_dot.pkl','wb')\n",
    "pickle.dump(y_scalar, file_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the TimeSeriesTensor convenience class to:\n",
    "1. Shift the values of the time series to create a Pandas dataframe containing all the data for a single training example\n",
    "2. Discard any samples with missing values\n",
    "3. Transform this Pandas dataframe into a numpy array of shape (samples, time steps, features) for input into Keras\n",
    "\n",
    "The class takes the following parameters:\n",
    "\n",
    "- **dataset**: original time series\n",
    "- **H**: the forecast horizon\n",
    "- **tensor_structure**: a dictionary discribing the tensor structure in the form { 'tensor_name' : (range(max_backward_shift, max_forward_shift), [feature, feature, ...] ) }\n",
    "- **freq**: time series frequency\n",
    "- **drop_incomplete**: (Boolean) whether to drop incomplete samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_structure = {'X':(range(-T+1, 1), Paras[var_name])}\n",
    "train_inputs = TimeSeriesTensor(train, var_name, HORIZON, {'X':(range(-T+1, 1), Paras[var_name])} ,freq = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_inputs.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 24)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs['target'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct validation set (keeping T hours from the training set in order to construct initial features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"10\" halign=\"left\">target</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"10\" halign=\"left\">y</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">OMEGA_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t+7</th>\n",
       "      <th>t+8</th>\n",
       "      <th>t+9</th>\n",
       "      <th>t+10</th>\n",
       "      <th>...</th>\n",
       "      <th>t-9</th>\n",
       "      <th>t-8</th>\n",
       "      <th>t-7</th>\n",
       "      <th>t-6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-07 00:00:00</th>\n",
       "      <td>0.14071108129171694401</td>\n",
       "      <td>0.49293198617096850578</td>\n",
       "      <td>0.89979210028077249994</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.78114475329429322592</td>\n",
       "      <td>2.01247259003716960635</td>\n",
       "      <td>2.04384634676761844929</td>\n",
       "      <td>2.01598731575529210858</td>\n",
       "      <td>1.93154447146695740223</td>\n",
       "      <td>1.65501596662646299762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.29085090117642536933</td>\n",
       "      <td>-0.50639472830363441158</td>\n",
       "      <td>-0.66341341169468759364</td>\n",
       "      <td>-0.76229554480740879807</td>\n",
       "      <td>-0.80342972109961929839</td>\n",
       "      <td>-0.78720453401461665255</td>\n",
       "      <td>-0.71400857699569553194</td>\n",
       "      <td>-0.58423044351521380424</td>\n",
       "      <td>-0.39825872700193237774</td>\n",
       "      <td>-0.15648202091368246247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 01:00:00</th>\n",
       "      <td>0.49293198617096850578</td>\n",
       "      <td>0.89979210028077249994</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.78114475329429322592</td>\n",
       "      <td>2.01247259003716960635</td>\n",
       "      <td>2.04384634676761844929</td>\n",
       "      <td>2.01598731575529210858</td>\n",
       "      <td>1.93154447146695740223</td>\n",
       "      <td>1.65501596662646299762</td>\n",
       "      <td>1.19734709801070104973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.50639472830363441158</td>\n",
       "      <td>-0.66341341169468759364</td>\n",
       "      <td>-0.76229554480740879807</td>\n",
       "      <td>-0.80342972109961929839</td>\n",
       "      <td>-0.78720453401461665255</td>\n",
       "      <td>-0.71400857699569553194</td>\n",
       "      <td>-0.58423044351521380424</td>\n",
       "      <td>-0.39825872700193237774</td>\n",
       "      <td>-0.15648202091368246247</td>\n",
       "      <td>0.14071108129171694401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 02:00:00</th>\n",
       "      <td>0.89979210028077249994</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.78114475329429322592</td>\n",
       "      <td>2.01247259003716960635</td>\n",
       "      <td>2.04384634676761844929</td>\n",
       "      <td>2.01598731575529210858</td>\n",
       "      <td>1.93154447146695740223</td>\n",
       "      <td>1.65501596662646299762</td>\n",
       "      <td>1.19734709801070104973</td>\n",
       "      <td>0.71865324034493660577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.66341341169468759364</td>\n",
       "      <td>-0.76229554480740879807</td>\n",
       "      <td>-0.80342972109961929839</td>\n",
       "      <td>-0.78720453401461665255</td>\n",
       "      <td>-0.71400857699569553194</td>\n",
       "      <td>-0.58423044351521380424</td>\n",
       "      <td>-0.39825872700193237774</td>\n",
       "      <td>-0.15648202091368246247</td>\n",
       "      <td>0.14071108129171694401</td>\n",
       "      <td>0.49293198617096850578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 03:00:00</th>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.78114475329429322592</td>\n",
       "      <td>2.01247259003716960635</td>\n",
       "      <td>2.04384634676761844929</td>\n",
       "      <td>2.01598731575529210858</td>\n",
       "      <td>1.93154447146695740223</td>\n",
       "      <td>1.65501596662646299762</td>\n",
       "      <td>1.19734709801070104973</td>\n",
       "      <td>0.71865324034493660577</td>\n",
       "      <td>0.30352642614630975793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.76229554480740879807</td>\n",
       "      <td>-0.80342972109961929839</td>\n",
       "      <td>-0.78720453401461665255</td>\n",
       "      <td>-0.71400857699569553194</td>\n",
       "      <td>-0.58423044351521380424</td>\n",
       "      <td>-0.39825872700193237774</td>\n",
       "      <td>-0.15648202091368246247</td>\n",
       "      <td>0.14071108129171694401</td>\n",
       "      <td>0.49293198617096850578</td>\n",
       "      <td>0.89979210028077249994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 04:00:00</th>\n",
       "      <td>1.78114475329429322592</td>\n",
       "      <td>2.01247259003716960635</td>\n",
       "      <td>2.04384634676761844929</td>\n",
       "      <td>2.01598731575529210858</td>\n",
       "      <td>1.93154447146695740223</td>\n",
       "      <td>1.65501596662646299762</td>\n",
       "      <td>1.19734709801070104973</td>\n",
       "      <td>0.71865324034493660577</td>\n",
       "      <td>0.30352642614630975793</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.80342972109961929839</td>\n",
       "      <td>-0.78720453401461665255</td>\n",
       "      <td>-0.71400857699569553194</td>\n",
       "      <td>-0.58423044351521380424</td>\n",
       "      <td>-0.39825872700193237774</td>\n",
       "      <td>-0.15648202091368246247</td>\n",
       "      <td>0.14071108129171694401</td>\n",
       "      <td>0.49293198617096850578</td>\n",
       "      <td>0.89979210028077249994</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                              target                         \\\n",
       "feature                                  y                          \n",
       "time step                              t+1                    t+2   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-07 00:00:00 0.14071108129171694401 0.49293198617096850578   \n",
       "2017-11-07 01:00:00 0.49293198617096850578 0.89979210028077249994   \n",
       "2017-11-07 02:00:00 0.89979210028077249994 1.40530733235496274602   \n",
       "2017-11-07 03:00:00 1.40530733235496274602 1.78114475329429322592   \n",
       "2017-11-07 04:00:00 1.78114475329429322592 2.01247259003716960635   \n",
       "\n",
       "tensor                                                             \\\n",
       "feature                                                             \n",
       "time step                              t+3                    t+4   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-07 00:00:00 0.89979210028077249994 1.40530733235496274602   \n",
       "2017-11-07 01:00:00 1.40530733235496274602 1.78114475329429322592   \n",
       "2017-11-07 02:00:00 1.78114475329429322592 2.01247259003716960635   \n",
       "2017-11-07 03:00:00 2.01247259003716960635 2.04384634676761844929   \n",
       "2017-11-07 04:00:00 2.04384634676761844929 2.01598731575529210858   \n",
       "\n",
       "tensor                                                             \\\n",
       "feature                                                             \n",
       "time step                              t+5                    t+6   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-07 00:00:00 1.78114475329429322592 2.01247259003716960635   \n",
       "2017-11-07 01:00:00 2.01247259003716960635 2.04384634676761844929   \n",
       "2017-11-07 02:00:00 2.04384634676761844929 2.01598731575529210858   \n",
       "2017-11-07 03:00:00 2.01598731575529210858 1.93154447146695740223   \n",
       "2017-11-07 04:00:00 1.93154447146695740223 1.65501596662646299762   \n",
       "\n",
       "tensor                                                             \\\n",
       "feature                                                             \n",
       "time step                              t+7                    t+8   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-07 00:00:00 2.04384634676761844929 2.01598731575529210858   \n",
       "2017-11-07 01:00:00 2.01598731575529210858 1.93154447146695740223   \n",
       "2017-11-07 02:00:00 1.93154447146695740223 1.65501596662646299762   \n",
       "2017-11-07 03:00:00 1.65501596662646299762 1.19734709801070104973   \n",
       "2017-11-07 04:00:00 1.19734709801070104973 0.71865324034493660577   \n",
       "\n",
       "tensor                                                              \\\n",
       "feature                                                              \n",
       "time step                              t+9                    t+10   \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-07 00:00:00 1.93154447146695740223  1.65501596662646299762   \n",
       "2017-11-07 01:00:00 1.65501596662646299762  1.19734709801070104973   \n",
       "2017-11-07 02:00:00 1.19734709801070104973  0.71865324034493660577   \n",
       "2017-11-07 03:00:00 0.71865324034493660577  0.30352642614630975793   \n",
       "2017-11-07 04:00:00 0.30352642614630975793 -0.04749514996851413040   \n",
       "\n",
       "tensor                        ...                                 X  \\\n",
       "feature                       ...                         OMEGA_dot   \n",
       "time step                     ...                               t-9   \n",
       "Epoch_Time_of_Clock           ...                                     \n",
       "2017-11-07 00:00:00           ...           -0.29085090117642536933   \n",
       "2017-11-07 01:00:00           ...           -0.50639472830363441158   \n",
       "2017-11-07 02:00:00           ...           -0.66341341169468759364   \n",
       "2017-11-07 03:00:00           ...           -0.76229554480740879807   \n",
       "2017-11-07 04:00:00           ...           -0.80342972109961929839   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-8                     t-7   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-07 00:00:00 -0.50639472830363441158 -0.66341341169468759364   \n",
       "2017-11-07 01:00:00 -0.66341341169468759364 -0.76229554480740879807   \n",
       "2017-11-07 02:00:00 -0.76229554480740879807 -0.80342972109961929839   \n",
       "2017-11-07 03:00:00 -0.80342972109961929839 -0.78720453401461665255   \n",
       "2017-11-07 04:00:00 -0.78720453401461665255 -0.71400857699569553194   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-6                     t-5   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-07 00:00:00 -0.76229554480740879807 -0.80342972109961929839   \n",
       "2017-11-07 01:00:00 -0.80342972109961929839 -0.78720453401461665255   \n",
       "2017-11-07 02:00:00 -0.78720453401461665255 -0.71400857699569553194   \n",
       "2017-11-07 03:00:00 -0.71400857699569553194 -0.58423044351521380424   \n",
       "2017-11-07 04:00:00 -0.58423044351521380424 -0.39825872700193237774   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-4                     t-3   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-07 00:00:00 -0.78720453401461665255 -0.71400857699569553194   \n",
       "2017-11-07 01:00:00 -0.71400857699569553194 -0.58423044351521380424   \n",
       "2017-11-07 02:00:00 -0.58423044351521380424 -0.39825872700193237774   \n",
       "2017-11-07 03:00:00 -0.39825872700193237774 -0.15648202091368246247   \n",
       "2017-11-07 04:00:00 -0.15648202091368246247  0.14071108129171694401   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-2                     t-1   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-07 00:00:00 -0.58423044351521380424 -0.39825872700193237774   \n",
       "2017-11-07 01:00:00 -0.39825872700193237774 -0.15648202091368246247   \n",
       "2017-11-07 02:00:00 -0.15648202091368246247  0.14071108129171694401   \n",
       "2017-11-07 03:00:00  0.14071108129171694401  0.49293198617096850578   \n",
       "2017-11-07 04:00:00  0.49293198617096850578  0.89979210028077249994   \n",
       "\n",
       "tensor                                       \n",
       "feature                                      \n",
       "time step                                 t  \n",
       "Epoch_Time_of_Clock                          \n",
       "2017-11-07 00:00:00 -0.15648202091368246247  \n",
       "2017-11-07 01:00:00  0.14071108129171694401  \n",
       "2017-11-07 02:00:00  0.49293198617096850578  \n",
       "2017-11-07 03:00:00  0.89979210028077249994  \n",
       "2017-11-07 04:00:00  1.40530733235496274602  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back_dt = dt.datetime.strptime(valid_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1)\n",
    "valid = df.copy()[(df.index >=look_back_dt) & (df.index < test_start_dt)][Paras[var_name]]\n",
    "valid[Paras[var_name]] = X_scaler.transform(valid)\n",
    "valid_inputs = TimeSeriesTensor(valid, var_name, HORIZON, tensor_structure,freq = None)\n",
    "valid_inputs.dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a RNN forecasting model with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image('./images/simple_encoder_decoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Flatten\n",
    "from keras.callbacks import EarlyStopping ,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(LATENT_DIM, input_shape=(T,1) ,return_sequences=True))\n",
    "model.add(LSTM(LATENT_DIM))\n",
    "model.add(RepeatVector(HORIZON))\n",
    "model.add(LSTM(LATENT_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='RMSprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_19 (LSTM)               (None, 24, 64)            16896     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "repeat_vector_7 (RepeatVecto (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 24, 64)            33024     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 24, 1)             65        \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 83,009\n",
      "Trainable params: 83,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val = ModelCheckpoint(str(sat_var) +'_' +  var_name + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73 samples, validate on 72 samples\n",
      "Epoch 1/1000\n",
      "73/73 [==============================] - 3s 36ms/step - loss: 1.0041 - val_loss: 1.0493\n",
      "Epoch 2/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.9182 - val_loss: 1.0353\n",
      "Epoch 3/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.8977 - val_loss: 0.9830\n",
      "Epoch 4/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.8224 - val_loss: 0.8988\n",
      "Epoch 5/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.7724 - val_loss: 0.8036\n",
      "Epoch 6/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6673 - val_loss: 0.7295\n",
      "Epoch 7/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6523 - val_loss: 0.7045\n",
      "Epoch 8/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5425 - val_loss: 0.6483\n",
      "Epoch 9/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4728 - val_loss: 0.9438\n",
      "Epoch 10/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4812 - val_loss: 0.9564\n",
      "Epoch 11/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4662 - val_loss: 0.8292\n",
      "Epoch 12/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3642 - val_loss: 0.6592\n",
      "Epoch 13/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3483 - val_loss: 0.9058\n",
      "Epoch 14/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3300 - val_loss: 0.6282\n",
      "Epoch 15/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3363 - val_loss: 0.7857\n",
      "Epoch 16/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2907 - val_loss: 0.5681\n",
      "Epoch 17/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2854 - val_loss: 0.6672\n",
      "Epoch 18/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2537 - val_loss: 0.5736\n",
      "Epoch 19/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2405 - val_loss: 0.6168\n",
      "Epoch 20/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2275 - val_loss: 0.5587\n",
      "Epoch 21/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2140 - val_loss: 0.5628\n",
      "Epoch 22/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1998 - val_loss: 0.4985\n",
      "Epoch 23/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1941 - val_loss: 0.5368\n",
      "Epoch 24/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1649 - val_loss: 0.4433\n",
      "Epoch 25/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2468 - val_loss: 0.5825\n",
      "Epoch 26/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2086 - val_loss: 0.6108\n",
      "Epoch 27/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2150 - val_loss: 0.4075\n",
      "Epoch 28/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1632 - val_loss: 0.4615\n",
      "Epoch 29/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1480 - val_loss: 0.4640\n",
      "Epoch 30/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1841 - val_loss: 0.5487\n",
      "Epoch 31/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1638 - val_loss: 0.5118\n",
      "Epoch 32/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1798 - val_loss: 0.4463\n",
      "Epoch 33/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1520 - val_loss: 0.4617\n",
      "Epoch 34/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1456 - val_loss: 0.4033\n",
      "Epoch 35/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1482 - val_loss: 0.4698\n",
      "Epoch 36/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1511 - val_loss: 0.4221\n",
      "Epoch 37/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1531 - val_loss: 0.4440\n",
      "Epoch 38/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1399 - val_loss: 0.3861\n",
      "Epoch 39/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1430 - val_loss: 0.4279\n",
      "Epoch 40/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1312 - val_loss: 0.3786\n",
      "Epoch 41/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 0.4208\n",
      "Epoch 42/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.3667\n",
      "Epoch 43/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1379 - val_loss: 0.4008\n",
      "Epoch 44/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1189 - val_loss: 0.3474\n",
      "Epoch 45/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1321 - val_loss: 0.3905\n",
      "Epoch 46/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1139 - val_loss: 0.3400\n",
      "Epoch 47/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1294 - val_loss: 0.3776\n",
      "Epoch 48/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1078 - val_loss: 0.3248\n",
      "Epoch 49/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1240 - val_loss: 0.3646\n",
      "Epoch 50/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1017 - val_loss: 0.3132\n",
      "Epoch 51/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1197 - val_loss: 0.3528\n",
      "Epoch 52/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0968 - val_loss: 0.3039\n",
      "Epoch 53/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1151 - val_loss: 0.3408\n",
      "Epoch 54/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0921 - val_loss: 0.2942\n",
      "Epoch 55/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1097 - val_loss: 0.3295\n",
      "Epoch 56/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0884 - val_loss: 0.2874\n",
      "Epoch 57/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1046 - val_loss: 0.3189\n",
      "Epoch 58/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0856 - val_loss: 0.2810\n",
      "Epoch 59/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0990 - val_loss: 0.3085\n",
      "Epoch 60/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0835 - val_loss: 0.2750\n",
      "Epoch 61/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0940 - val_loss: 0.3021\n",
      "Epoch 62/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0817 - val_loss: 0.2666\n",
      "Epoch 63/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 0.2958\n",
      "Epoch 64/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0791 - val_loss: 0.2568\n",
      "Epoch 65/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0867 - val_loss: 0.2923\n",
      "Epoch 66/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0757 - val_loss: 0.2453\n",
      "Epoch 67/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0856 - val_loss: 0.2898\n",
      "Epoch 68/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0714 - val_loss: 0.2287\n",
      "Epoch 69/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.2862\n",
      "Epoch 70/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.2069\n",
      "Epoch 71/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0901 - val_loss: 0.2871\n",
      "Epoch 72/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0642 - val_loss: 0.1943\n",
      "Epoch 73/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.2789\n",
      "Epoch 74/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0597 - val_loss: 0.2123\n",
      "Epoch 75/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0747 - val_loss: 0.2632\n",
      "Epoch 76/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0817 - val_loss: 0.3271\n",
      "Epoch 77/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0809 - val_loss: 0.2630\n",
      "Epoch 78/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0888 - val_loss: 0.2685\n",
      "Epoch 79/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.2468\n",
      "Epoch 80/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0688 - val_loss: 0.2170\n",
      "Epoch 81/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0674 - val_loss: 0.2997\n",
      "Epoch 82/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0709 - val_loss: 0.1975\n",
      "Epoch 83/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1001 - val_loss: 0.2874\n",
      "Epoch 84/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0604 - val_loss: 0.1663\n",
      "Epoch 85/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0758 - val_loss: 0.2548\n",
      "Epoch 86/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.1885\n",
      "Epoch 87/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0660 - val_loss: 0.2571\n",
      "Epoch 88/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0595 - val_loss: 0.2792\n",
      "Epoch 89/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0693 - val_loss: 0.2604\n",
      "Epoch 90/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0949 - val_loss: 0.2835\n",
      "Epoch 91/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0539 - val_loss: 0.2354\n",
      "Epoch 92/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.1940\n",
      "Epoch 93/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0567 - val_loss: 0.2560\n",
      "Epoch 94/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0617 - val_loss: 0.1848\n",
      "Epoch 95/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0831 - val_loss: 0.2810\n",
      "Epoch 96/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0565 - val_loss: 0.1787\n",
      "Epoch 97/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0746 - val_loss: 0.2499\n",
      "Epoch 98/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0459 - val_loss: 0.1702\n",
      "Epoch 99/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0588 - val_loss: 0.2451\n",
      "Epoch 100/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.2249\n",
      "Epoch 101/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0644 - val_loss: 0.2423\n",
      "Epoch 102/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0943 - val_loss: 0.2840\n",
      "Epoch 103/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0556 - val_loss: 0.2248\n",
      "Epoch 104/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0698 - val_loss: 0.1926\n",
      "Epoch 105/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0468 - val_loss: 0.2185\n",
      "Epoch 106/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0536 - val_loss: 0.1771\n",
      "Epoch 107/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0673 - val_loss: 0.2794\n",
      "Epoch 108/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.1933\n",
      "Epoch 109/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0742 - val_loss: 0.2395\n",
      "Epoch 110/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0425 - val_loss: 0.1521\n",
      "Epoch 111/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0630 - val_loss: 0.2499\n",
      "Epoch 112/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0519 - val_loss: 0.1954\n",
      "Epoch 113/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0626 - val_loss: 0.2183\n",
      "Epoch 114/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0658 - val_loss: 0.2450\n",
      "Epoch 115/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0498 - val_loss: 0.2049\n",
      "Epoch 116/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0717 - val_loss: 0.2259\n",
      "Epoch 117/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0382 - val_loss: 0.2063\n",
      "Epoch 118/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0525 - val_loss: 0.1510\n",
      "Epoch 119/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0627 - val_loss: 0.2834\n",
      "Epoch 120/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0719 - val_loss: 0.1858\n",
      "Epoch 121/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0802 - val_loss: 0.2181\n",
      "Epoch 122/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0351 - val_loss: 0.1588\n",
      "Epoch 123/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0514 - val_loss: 0.2333\n",
      "Epoch 124/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0433 - val_loss: 0.1806\n",
      "Epoch 125/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0559 - val_loss: 0.2137\n",
      "Epoch 126/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0573 - val_loss: 0.2313\n",
      "Epoch 127/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0529 - val_loss: 0.2169\n",
      "Epoch 128/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0699 - val_loss: 0.2357\n",
      "Epoch 129/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.2040\n",
      "Epoch 130/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0601 - val_loss: 0.1733\n",
      "Epoch 131/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0506 - val_loss: 0.2550\n",
      "Epoch 132/1000\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0632 - val_loss: 0.1703\n",
      "Epoch 133/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0762 - val_loss: 0.2335\n",
      "Epoch 134/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.1558\n",
      "Epoch 135/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.2315\n",
      "Epoch 136/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.1665\n",
      "Epoch 137/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0491 - val_loss: 0.2033\n",
      "Epoch 138/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.2017\n",
      "Epoch 139/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.2077\n",
      "Epoch 140/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0856 - val_loss: 0.2398\n",
      "Epoch 141/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0439 - val_loss: 0.2020\n",
      "Epoch 142/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0471 - val_loss: 0.1589\n",
      "Epoch 143/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.2477\n",
      "Epoch 144/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0635 - val_loss: 0.1681\n",
      "Epoch 145/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0740 - val_loss: 0.2189\n",
      "Epoch 146/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.1625\n",
      "Epoch 147/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0502 - val_loss: 0.2128\n",
      "Epoch 148/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1587\n",
      "Epoch 149/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.2139\n",
      "Epoch 150/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0464 - val_loss: 0.1821\n",
      "Epoch 151/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.1905\n",
      "Epoch 152/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0714 - val_loss: 0.2321\n",
      "Epoch 153/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0483 - val_loss: 0.2163\n",
      "Epoch 154/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.1944\n",
      "Epoch 155/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.2022\n",
      "Epoch 156/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0466 - val_loss: 0.1508\n",
      "Epoch 157/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.2568\n",
      "Epoch 158/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0566 - val_loss: 0.1619\n",
      "Epoch 159/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0785 - val_loss: 0.2074\n",
      "Epoch 160/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.1492\n",
      "Epoch 161/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.1932\n",
      "Epoch 162/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 0.1806\n",
      "Epoch 163/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0439 - val_loss: 0.1861\n",
      "Epoch 164/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0772 - val_loss: 0.2285\n",
      "Epoch 165/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 0.2064\n",
      "Epoch 166/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.1555\n",
      "Epoch 167/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.2427\n",
      "Epoch 168/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0608 - val_loss: 0.1611\n",
      "Epoch 169/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0780 - val_loss: 0.2032\n",
      "Epoch 170/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.1493\n",
      "Epoch 171/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.1939\n",
      "Epoch 172/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 0.1596\n",
      "Epoch 173/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.1946\n",
      "Epoch 174/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0460 - val_loss: 0.1915\n",
      "Epoch 175/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0404 - val_loss: 0.1897\n",
      "Epoch 176/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0574 - val_loss: 0.2164\n",
      "Epoch 177/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.2070\n",
      "Epoch 178/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.1886\n",
      "Epoch 179/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.2274\n",
      "Epoch 180/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0536 - val_loss: 0.1372\n",
      "Epoch 181/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0612 - val_loss: 0.2409\n",
      "Epoch 182/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0653 - val_loss: 0.1496\n",
      "Epoch 183/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0537 - val_loss: 0.1796\n",
      "Epoch 184/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.1488\n",
      "Epoch 185/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.1814\n",
      "Epoch 186/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0530 - val_loss: 0.1991\n",
      "Epoch 187/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.1925\n",
      "Epoch 188/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.1813\n",
      "Epoch 189/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.2375\n",
      "Epoch 190/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0493 - val_loss: 0.1397\n",
      "Epoch 191/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0574 - val_loss: 0.2020\n",
      "Epoch 192/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.1521\n",
      "Epoch 193/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0639 - val_loss: 0.1760\n",
      "Epoch 194/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.1445\n",
      "Epoch 195/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.1812\n",
      "Epoch 196/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0503 - val_loss: 0.1905\n",
      "Epoch 197/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.1854\n",
      "Epoch 198/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0559 - val_loss: 0.2160\n",
      "Epoch 199/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.2221\n",
      "Epoch 200/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0465 - val_loss: 0.1735\n",
      "Epoch 201/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.2281\n",
      "Epoch 202/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0494 - val_loss: 0.1451\n",
      "Epoch 203/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0550 - val_loss: 0.2029\n",
      "Epoch 204/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1473\n",
      "Epoch 205/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.1970\n",
      "Epoch 206/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.1553\n",
      "Epoch 207/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1833\n",
      "Epoch 208/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.1865\n",
      "Epoch 209/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.1775\n",
      "Epoch 210/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1624\n",
      "Epoch 211/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.2336\n",
      "Epoch 212/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.1540\n",
      "Epoch 213/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.1835\n",
      "Epoch 214/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0350 - val_loss: 0.1451\n",
      "Epoch 215/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.1789\n",
      "Epoch 216/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.1452\n",
      "Epoch 217/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1847\n",
      "Epoch 218/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0476 - val_loss: 0.1710\n",
      "Epoch 219/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.1683\n",
      "Epoch 220/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.2153\n",
      "Epoch 221/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2244\n",
      "Epoch 222/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0515 - val_loss: 0.1908\n",
      "Epoch 223/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2185\n",
      "Epoch 224/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.1382\n",
      "Epoch 225/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.2083\n",
      "Epoch 226/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.1409\n",
      "Epoch 227/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0497 - val_loss: 0.1918\n",
      "Epoch 228/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.1532\n",
      "Epoch 229/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.1648\n",
      "Epoch 230/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0404 - val_loss: 0.1778\n",
      "Epoch 231/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0455 - val_loss: 0.1742\n",
      "Epoch 232/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.1716\n",
      "Epoch 233/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.2310\n",
      "Epoch 234/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.1447\n",
      "Epoch 235/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0640 - val_loss: 0.2160\n",
      "Epoch 236/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.1422\n",
      "Epoch 237/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.1502\n",
      "Epoch 238/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.1417\n",
      "Epoch 239/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.1543\n",
      "Epoch 240/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0533 - val_loss: 0.2011\n",
      "Epoch 241/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0393 - val_loss: 0.1974\n",
      "Epoch 242/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0423 - val_loss: 0.1947\n",
      "Epoch 243/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.2280\n",
      "Epoch 244/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.1658\n",
      "Epoch 245/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.2168\n",
      "Epoch 246/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0411 - val_loss: 0.1272\n",
      "Epoch 247/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.1893\n",
      "Epoch 248/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.1428\n",
      "Epoch 249/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.1668\n",
      "Epoch 250/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.1492\n",
      "Epoch 251/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.1714\n",
      "Epoch 252/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0652 - val_loss: 0.1997\n",
      "Epoch 253/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 0.2009\n",
      "Epoch 254/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0340 - val_loss: 0.1657\n",
      "Epoch 255/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.2066\n",
      "Epoch 256/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.1184\n",
      "Epoch 257/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0590 - val_loss: 0.2147\n",
      "Epoch 258/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0546 - val_loss: 0.1470\n",
      "Epoch 259/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0393 - val_loss: 0.1553\n",
      "Epoch 260/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.1453\n",
      "Epoch 261/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.1571\n",
      "Epoch 262/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.1988\n",
      "Epoch 263/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.2107\n",
      "Epoch 264/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0415 - val_loss: 0.1745\n",
      "Epoch 265/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.2091\n",
      "Epoch 266/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.1178\n",
      "Epoch 267/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0639 - val_loss: 0.2142\n",
      "Epoch 268/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0541 - val_loss: 0.1447\n",
      "Epoch 269/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1529\n",
      "Epoch 270/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.1402\n",
      "Epoch 271/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0257 - val_loss: 0.1590\n",
      "Epoch 272/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0527 - val_loss: 0.1933\n",
      "Epoch 273/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0302 - val_loss: 0.2076\n",
      "Epoch 274/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0412 - val_loss: 0.1733\n",
      "Epoch 275/1000\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0313 - val_loss: 0.2068\n",
      "Epoch 276/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0330 - val_loss: 0.1222\n",
      "Epoch 277/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.2092\n",
      "Epoch 278/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.1453\n",
      "Epoch 279/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.1574\n",
      "Epoch 280/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.1355\n",
      "Epoch 281/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.1566\n",
      "Epoch 282/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0516 - val_loss: 0.1884\n",
      "Epoch 283/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0318 - val_loss: 0.2037\n",
      "Epoch 284/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.1769\n",
      "Epoch 285/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0290 - val_loss: 0.2043\n",
      "Epoch 286/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.1201\n",
      "Epoch 287/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0599 - val_loss: 0.2067\n",
      "Epoch 288/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0480 - val_loss: 0.1403\n",
      "Epoch 289/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.1573\n",
      "Epoch 290/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.1342\n",
      "Epoch 291/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.1567\n",
      "Epoch 292/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0512 - val_loss: 0.1841\n",
      "Epoch 293/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.1935\n",
      "Epoch 294/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.1608\n",
      "Epoch 295/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.2034\n",
      "Epoch 296/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.1168\n",
      "Epoch 297/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0564 - val_loss: 0.2008\n",
      "Epoch 298/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0511 - val_loss: 0.1418\n",
      "Epoch 299/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.1496\n",
      "Epoch 300/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.1414\n",
      "Epoch 301/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.1534\n",
      "Epoch 302/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.1872\n",
      "Epoch 303/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.2058\n",
      "Epoch 304/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0384 - val_loss: 0.1648\n",
      "Epoch 305/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.2013\n",
      "Epoch 306/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.1156\n",
      "Epoch 307/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0594 - val_loss: 0.2014\n",
      "Epoch 308/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0475 - val_loss: 0.1408\n",
      "Epoch 309/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.1499\n",
      "Epoch 310/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.1364\n",
      "Epoch 311/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.1544\n",
      "Epoch 312/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.1824\n",
      "Epoch 313/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.2018\n",
      "Epoch 314/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1621\n",
      "Epoch 315/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0300 - val_loss: 0.1986\n",
      "Epoch 316/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0306 - val_loss: 0.1152\n",
      "Epoch 317/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.1978\n",
      "Epoch 318/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.1396\n",
      "Epoch 319/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.1500\n",
      "Epoch 320/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0185 - val_loss: 0.1360\n",
      "Epoch 321/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.1535\n",
      "Epoch 322/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.1803\n",
      "Epoch 323/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.1991\n",
      "Epoch 324/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 0.1588\n",
      "Epoch 325/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.1966\n",
      "Epoch 326/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.1139\n",
      "Epoch 327/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0559 - val_loss: 0.1958\n",
      "Epoch 328/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0454 - val_loss: 0.1384\n",
      "Epoch 329/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.1488\n",
      "Epoch 330/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.1366\n",
      "Epoch 331/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.1526\n",
      "Epoch 332/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.1779\n",
      "Epoch 333/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.1976\n",
      "Epoch 334/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.1535\n",
      "Epoch 335/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0299 - val_loss: 0.1942\n",
      "Epoch 336/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.1136\n",
      "Epoch 337/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0536 - val_loss: 0.1927\n",
      "Epoch 338/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.1383\n",
      "Epoch 339/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.1476\n",
      "Epoch 340/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.1380\n",
      "Epoch 341/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0247 - val_loss: 0.1524\n",
      "Epoch 342/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0459 - val_loss: 0.1755\n",
      "Epoch 343/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.1973\n",
      "Epoch 344/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.1525\n",
      "Epoch 345/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.1925\n",
      "Epoch 346/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.1124\n",
      "Epoch 347/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0518 - val_loss: 0.1904\n",
      "Epoch 348/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.1369\n",
      "Epoch 349/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.1479\n",
      "Epoch 350/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.1369\n",
      "Epoch 351/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.1509\n",
      "Epoch 352/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0455 - val_loss: 0.1738\n",
      "Epoch 353/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.1951\n",
      "Epoch 354/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.1484\n",
      "Epoch 355/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.1901\n",
      "Epoch 356/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.1122\n",
      "Epoch 357/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.1880\n",
      "Epoch 358/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.1368\n",
      "Epoch 359/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.1474\n",
      "Epoch 360/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.1375\n",
      "Epoch 361/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.1503\n",
      "Epoch 362/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 0.1710\n",
      "Epoch 363/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.1936\n",
      "Epoch 364/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.1465\n",
      "Epoch 365/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.1878\n",
      "Epoch 366/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.1113\n",
      "Epoch 367/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.1856\n",
      "Epoch 368/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0400 - val_loss: 0.1360\n",
      "Epoch 369/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.1476\n",
      "Epoch 370/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.1368\n",
      "Epoch 371/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.1486\n",
      "Epoch 372/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.1694\n",
      "Epoch 373/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.1921\n",
      "Epoch 374/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.1443\n",
      "Epoch 375/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.1860\n",
      "Epoch 376/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.1108\n",
      "Epoch 377/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0459 - val_loss: 0.1841\n",
      "Epoch 378/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.1355\n",
      "Epoch 379/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.1471\n",
      "Epoch 380/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.1366\n",
      "Epoch 381/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.1474\n",
      "Epoch 382/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.1670\n",
      "Epoch 383/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.1901\n",
      "Epoch 384/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.1411\n",
      "Epoch 385/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.1842\n",
      "Epoch 386/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.1105\n",
      "Epoch 387/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0441 - val_loss: 0.1817\n",
      "Epoch 388/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.1350\n",
      "Epoch 389/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.1468\n",
      "Epoch 390/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.1366\n",
      "Epoch 391/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.1470\n",
      "Epoch 392/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0404 - val_loss: 0.1643\n",
      "Epoch 393/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.1880\n",
      "Epoch 394/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.1389\n",
      "Epoch 395/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.1822\n",
      "Epoch 396/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.1101\n",
      "Epoch 397/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.1801\n",
      "Epoch 398/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.1346\n",
      "Epoch 399/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.1463\n",
      "Epoch 400/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.1363\n",
      "Epoch 401/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.1460\n",
      "Epoch 402/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.1617\n",
      "Epoch 403/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.1857\n",
      "Epoch 404/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.1370\n",
      "Epoch 405/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.1805\n",
      "Epoch 406/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.1096\n",
      "Epoch 407/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0410 - val_loss: 0.1781\n",
      "Epoch 408/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0347 - val_loss: 0.1340\n",
      "Epoch 409/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.1469\n",
      "Epoch 410/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.1352\n",
      "Epoch 411/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.1437\n",
      "Epoch 412/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.1605\n",
      "Epoch 413/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.1839\n",
      "Epoch 414/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.1357\n",
      "Epoch 415/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.1789\n",
      "Epoch 416/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.1093\n",
      "Epoch 417/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.1770\n",
      "Epoch 418/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.1334\n",
      "Epoch 419/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.1464\n",
      "Epoch 420/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.1339\n",
      "Epoch 421/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.1419\n",
      "Epoch 422/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.1589\n",
      "Epoch 423/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.1828\n",
      "Epoch 424/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.1332\n",
      "Epoch 425/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.1771\n",
      "Epoch 426/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.1096\n",
      "Epoch 427/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1756\n",
      "Epoch 428/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0321 - val_loss: 0.1331\n",
      "Epoch 429/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.1459\n",
      "Epoch 430/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.1337\n",
      "Epoch 431/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.1417\n",
      "Epoch 432/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.1557\n",
      "Epoch 433/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.1802\n",
      "Epoch 434/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0263 - val_loss: 0.1306\n",
      "Epoch 435/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0323 - val_loss: 0.1754\n",
      "Epoch 436/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.1097\n",
      "Epoch 437/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1739\n",
      "Epoch 438/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.1327\n",
      "Epoch 439/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.1461\n",
      "Epoch 440/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.1335\n",
      "Epoch 441/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.1410\n",
      "Epoch 442/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.1532\n",
      "Epoch 443/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.1775\n",
      "Epoch 444/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.1292\n",
      "Epoch 445/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.1742\n",
      "Epoch 446/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.1094\n",
      "Epoch 447/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.1728\n",
      "Epoch 448/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.1320\n",
      "Epoch 449/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.1462\n",
      "Epoch 450/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.1329\n",
      "Epoch 451/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.1394\n",
      "Epoch 452/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 0.1515\n",
      "Epoch 453/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.1763\n",
      "Epoch 454/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.1277\n",
      "Epoch 455/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.1732\n",
      "Epoch 456/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.1097\n",
      "Epoch 457/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0335 - val_loss: 0.1715\n",
      "Epoch 458/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.1315\n",
      "Epoch 459/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.1460\n",
      "Epoch 460/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.1324\n",
      "Epoch 461/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.1384\n",
      "Epoch 462/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.1496\n",
      "Epoch 463/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.1749\n",
      "Epoch 464/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.1261\n",
      "Epoch 465/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 0.1723\n",
      "Epoch 466/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.1098\n",
      "Epoch 467/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.1697\n",
      "Epoch 468/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.1307\n",
      "Epoch 469/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.1465\n",
      "Epoch 470/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.1319\n",
      "Epoch 471/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.1376\n",
      "Epoch 472/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.1480\n",
      "Epoch 473/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.1729\n",
      "Epoch 474/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.1254\n",
      "Epoch 475/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.1715\n",
      "Epoch 476/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.1096\n",
      "Epoch 477/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.1686\n",
      "Epoch 478/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.1298\n",
      "Epoch 479/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.1467\n",
      "Epoch 480/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.1311\n",
      "Epoch 481/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.1360\n",
      "Epoch 482/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.1468\n",
      "Epoch 483/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.1724\n",
      "Epoch 484/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.1250\n",
      "Epoch 485/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 0.1709\n",
      "Epoch 486/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.1094\n",
      "Epoch 487/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.1684\n",
      "Epoch 488/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.1291\n",
      "Epoch 489/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.1461\n",
      "Epoch 490/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.1305\n",
      "Epoch 491/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.1351\n",
      "Epoch 492/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.1451\n",
      "Epoch 493/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.1720\n",
      "Epoch 494/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.1237\n",
      "Epoch 495/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.1704\n",
      "Epoch 496/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.1096\n",
      "Epoch 497/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.1664\n",
      "Epoch 498/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.1280\n",
      "Epoch 499/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.1463\n",
      "Epoch 500/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.1309\n",
      "Epoch 501/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.1354\n",
      "Epoch 502/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.1421\n",
      "Epoch 503/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.1706\n",
      "Epoch 504/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.1220\n",
      "Epoch 505/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.1698\n",
      "Epoch 506/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.1099\n",
      "Epoch 507/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.1643\n",
      "Epoch 508/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.1266\n",
      "Epoch 509/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.1462\n",
      "Epoch 510/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.1325\n",
      "Epoch 511/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.1374\n",
      "Epoch 512/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.1364\n",
      "Epoch 513/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.1675\n",
      "Epoch 514/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.1196\n",
      "Epoch 515/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.1699\n",
      "Epoch 516/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.1116\n",
      "Epoch 517/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.1600\n",
      "Epoch 518/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.1239\n",
      "Epoch 519/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.1439\n",
      "Epoch 520/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.1393\n",
      "Epoch 521/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.1437\n",
      "Epoch 522/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.1175\n",
      "Epoch 523/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.1652\n",
      "Epoch 524/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.1166\n",
      "Epoch 525/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0381 - val_loss: 0.1734\n",
      "Epoch 526/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.1235\n",
      "Epoch 527/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.1306\n",
      "Epoch 528/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.1324\n",
      "Epoch 529/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.1375\n",
      "Epoch 530/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.1320\n",
      "Epoch 531/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.1698\n",
      "Epoch 532/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.1065\n",
      "Epoch 533/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.1689\n",
      "Epoch 534/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.1263\n",
      "Epoch 535/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.1414\n",
      "Epoch 536/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.1243\n",
      "Epoch 537/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.1346\n",
      "Epoch 538/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.1428\n",
      "Epoch 539/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.1717\n",
      "Epoch 540/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.1147\n",
      "Epoch 541/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.1672\n",
      "Epoch 542/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.1127\n",
      "Epoch 543/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.1552\n",
      "Epoch 544/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0134 - val_loss: 0.1169\n",
      "Epoch 545/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0134 - val_loss: 0.1381\n",
      "Epoch 546/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0311 - val_loss: 0.1429\n",
      "Epoch 547/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.1533\n",
      "Epoch 548/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.1087\n",
      "Epoch 549/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.1656\n",
      "Epoch 550/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.1188\n",
      "Epoch 551/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.1501\n",
      "Epoch 552/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0104 - val_loss: 0.1133\n",
      "Epoch 553/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.1269\n",
      "Epoch 554/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.1497\n",
      "Epoch 555/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.1614\n",
      "Epoch 556/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.1229\n",
      "Epoch 557/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.1630\n",
      "Epoch 558/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.1074\n",
      "Epoch 559/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.1661\n",
      "Epoch 560/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.1256\n",
      "Epoch 561/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0172 - val_loss: 0.1451\n",
      "Epoch 562/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.1202\n",
      "Epoch 563/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.1352\n",
      "Epoch 564/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.1393\n",
      "Epoch 565/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.1683\n",
      "Epoch 566/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.1242\n",
      "Epoch 567/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.1634\n",
      "Epoch 568/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.1010\n",
      "Epoch 569/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.1618\n",
      "Epoch 570/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.1223\n",
      "Epoch 571/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.1452\n",
      "Epoch 572/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.1233\n",
      "Epoch 573/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.1345\n",
      "Epoch 574/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.1357\n",
      "Epoch 575/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.1658\n",
      "Epoch 576/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.1267\n",
      "Epoch 577/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.1659\n",
      "Epoch 578/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.1003\n",
      "Epoch 579/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.1641\n",
      "Epoch 580/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.1222\n",
      "Epoch 581/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.1434\n",
      "Epoch 582/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.1198\n",
      "Epoch 583/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.1284\n",
      "Epoch 584/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.1396\n",
      "Epoch 585/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.1683\n",
      "Epoch 586/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.1276\n",
      "Epoch 587/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.1669\n",
      "Epoch 588/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.1004\n",
      "Epoch 589/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.1643\n",
      "Epoch 590/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.1223\n",
      "Epoch 591/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.1420\n",
      "Epoch 592/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.1177\n",
      "Epoch 593/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.1260\n",
      "Epoch 594/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.1409\n",
      "Epoch 595/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.1698\n",
      "Epoch 596/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.1276\n",
      "Epoch 597/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.1664\n",
      "Epoch 598/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.1005\n",
      "Epoch 599/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.1629\n",
      "Epoch 600/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.1218\n",
      "Epoch 601/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.1419\n",
      "Epoch 602/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.1172\n",
      "Epoch 603/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.1261\n",
      "Epoch 604/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.1400\n",
      "Epoch 605/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.1697\n",
      "Epoch 606/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.1263\n",
      "Epoch 607/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.1656\n",
      "Epoch 608/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.1007\n",
      "Epoch 609/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.1617\n",
      "Epoch 610/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.1215\n",
      "Epoch 611/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.1415\n",
      "Epoch 612/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.1178\n",
      "Epoch 613/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.1277\n",
      "Epoch 614/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.1374\n",
      "Epoch 615/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.1689\n",
      "Epoch 616/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.1240\n",
      "Epoch 617/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.1645\n",
      "Epoch 618/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.1009\n",
      "Epoch 619/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.1600\n",
      "Epoch 620/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.1210\n",
      "Epoch 621/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.1411\n",
      "Epoch 622/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.1198\n",
      "Epoch 623/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.1306\n",
      "Epoch 624/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.1331\n",
      "Epoch 625/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.1667\n",
      "Epoch 626/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0180 - val_loss: 0.1213\n",
      "Epoch 627/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.1638\n",
      "Epoch 628/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.1016\n",
      "Epoch 629/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.1570\n",
      "Epoch 630/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.1200\n",
      "Epoch 631/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0165 - val_loss: 0.1416\n",
      "Epoch 632/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.1227\n",
      "Epoch 633/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.1337\n",
      "Epoch 634/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.1273\n",
      "Epoch 635/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.1639\n",
      "Epoch 636/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.1169\n",
      "Epoch 637/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.1636\n",
      "Epoch 638/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.1042\n",
      "Epoch 639/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.1511\n",
      "Epoch 640/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.1170\n",
      "Epoch 641/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.1386\n",
      "Epoch 642/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.1316\n",
      "Epoch 643/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.1429\n",
      "Epoch 644/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.1099\n",
      "Epoch 645/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.1612\n",
      "Epoch 646/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.1152\n",
      "Epoch 647/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.1653\n",
      "Epoch 648/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.1139\n",
      "Epoch 649/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.1194\n",
      "Epoch 650/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.1277\n",
      "Epoch 651/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0171 - val_loss: 0.1399\n",
      "Epoch 652/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.1323\n",
      "Epoch 653/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0151 - val_loss: 0.1626\n",
      "Epoch 654/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.1047\n",
      "Epoch 655/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.1683\n",
      "Epoch 656/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.1194\n",
      "Epoch 657/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0234 - val_loss: 0.1469\n",
      "Epoch 658/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0105 - val_loss: 0.1125\n",
      "Epoch 659/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.1138\n",
      "Epoch 660/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.1381\n",
      "Epoch 661/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0149 - val_loss: 0.1666\n",
      "Epoch 662/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 0.1312\n",
      "Epoch 663/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0246 - val_loss: 0.1683\n",
      "Epoch 664/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0975\n",
      "Epoch 665/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.1580\n",
      "Epoch 666/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.1166\n",
      "Epoch 667/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0176 - val_loss: 0.1382\n",
      "Epoch 668/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.1128\n",
      "Epoch 669/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.1253\n",
      "Epoch 670/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.1327\n",
      "Epoch 671/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.1673\n",
      "Epoch 672/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0178 - val_loss: 0.1215\n",
      "Epoch 673/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.1621\n",
      "Epoch 674/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0990\n",
      "Epoch 675/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.1552\n",
      "Epoch 676/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.1176\n",
      "Epoch 677/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.1397\n",
      "Epoch 678/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.1185\n",
      "Epoch 679/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.1318\n",
      "Epoch 680/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.1246\n",
      "Epoch 681/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.1628\n",
      "Epoch 682/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.1162\n",
      "Epoch 683/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.1611\n",
      "Epoch 684/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.1011\n",
      "Epoch 685/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.1485\n",
      "Epoch 686/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.1150\n",
      "Epoch 687/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.1385\n",
      "Epoch 688/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.1268\n",
      "Epoch 689/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.1401\n",
      "Epoch 690/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.1099\n",
      "Epoch 691/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.1598\n",
      "Epoch 692/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.1127\n",
      "Epoch 693/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.1626\n",
      "Epoch 694/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.1098\n",
      "Epoch 695/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.1204\n",
      "Epoch 696/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.1206\n",
      "Epoch 697/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.1373\n",
      "Epoch 698/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.1352\n",
      "Epoch 699/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.1608\n",
      "Epoch 700/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.1041\n",
      "Epoch 701/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.1658\n",
      "Epoch 702/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.1144\n",
      "Epoch 703/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.1476\n",
      "Epoch 704/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.1100\n",
      "Epoch 705/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.1098\n",
      "Epoch 706/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.1323\n",
      "Epoch 707/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.1605\n",
      "Epoch 708/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.1311\n",
      "Epoch 709/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.1676\n",
      "Epoch 710/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0972\n",
      "Epoch 711/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.1574\n",
      "Epoch 712/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.1150\n",
      "Epoch 713/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.1352\n",
      "Epoch 714/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.1094\n",
      "Epoch 715/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.1216\n",
      "Epoch 716/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.1299\n",
      "Epoch 717/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.1666\n",
      "Epoch 718/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.1222\n",
      "Epoch 719/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.1614\n",
      "Epoch 720/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0975\n",
      "Epoch 721/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.1522\n",
      "Epoch 722/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.1155\n",
      "Epoch 723/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.1373\n",
      "Epoch 724/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.1149\n",
      "Epoch 725/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.1296\n",
      "Epoch 726/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.1231\n",
      "Epoch 727/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.1622\n",
      "Epoch 728/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.1149\n",
      "Epoch 729/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.1589\n",
      "Epoch 730/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0160 - val_loss: 0.0996\n",
      "Epoch 731/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.1457\n",
      "Epoch 732/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0144 - val_loss: 0.1134\n",
      "Epoch 733/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0131 - val_loss: 0.1360\n",
      "Epoch 734/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.1242\n",
      "Epoch 735/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0170 - val_loss: 0.1394\n",
      "Epoch 736/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.1084\n",
      "Epoch 737/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.1591\n",
      "Epoch 738/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.1110\n",
      "Epoch 739/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.1596\n",
      "Epoch 740/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.1078\n",
      "Epoch 741/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0137 - val_loss: 0.1170\n",
      "Epoch 742/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0186 - val_loss: 0.1184\n",
      "Epoch 743/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0145 - val_loss: 0.1384\n",
      "Epoch 744/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0171 - val_loss: 0.1342\n",
      "Epoch 745/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0112 - val_loss: 0.1609\n",
      "Epoch 746/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.1028\n",
      "Epoch 747/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0258 - val_loss: 0.1629\n",
      "Epoch 748/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.1126\n",
      "Epoch 749/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.1440\n",
      "Epoch 750/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.1078\n",
      "Epoch 751/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.1093\n",
      "Epoch 752/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.1287\n",
      "Epoch 753/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.1582\n",
      "Epoch 754/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.1303\n",
      "Epoch 755/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.1672\n",
      "Epoch 756/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0966\n",
      "Epoch 757/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.1542\n",
      "Epoch 758/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.1134\n",
      "Epoch 759/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.1331\n",
      "Epoch 760/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.1079\n",
      "Epoch 761/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.1214\n",
      "Epoch 762/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.1264\n",
      "Epoch 763/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.1644\n",
      "Epoch 764/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.1203\n",
      "Epoch 765/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.1599\n",
      "Epoch 766/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0971\n",
      "Epoch 767/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.1485\n",
      "Epoch 768/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.1137\n",
      "Epoch 769/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.1356\n",
      "Epoch 770/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.1142\n",
      "Epoch 771/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.1301\n",
      "Epoch 772/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.1186\n",
      "Epoch 773/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.1606\n",
      "Epoch 774/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.1121\n",
      "Epoch 775/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.1578\n",
      "Epoch 776/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0999\n",
      "Epoch 777/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.1379\n",
      "Epoch 778/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.1096\n",
      "Epoch 779/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.1319\n",
      "Epoch 780/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.1270\n",
      "Epoch 781/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.1430\n",
      "Epoch 782/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.1050\n",
      "Epoch 783/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.1593\n",
      "Epoch 784/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.1132\n",
      "Epoch 785/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.1524\n",
      "Epoch 786/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.1051\n",
      "Epoch 787/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.1047\n",
      "Epoch 788/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.1218\n",
      "Epoch 789/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.1495\n",
      "Epoch 790/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.1355\n",
      "Epoch 791/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.1647\n",
      "Epoch 792/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0983\n",
      "Epoch 793/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.1565\n",
      "Epoch 794/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.1116\n",
      "Epoch 795/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.1382\n",
      "Epoch 796/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.1047\n",
      "Epoch 797/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.1122\n",
      "Epoch 798/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.1280\n",
      "Epoch 799/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.1596\n",
      "Epoch 800/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.1246\n",
      "Epoch 801/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.1621\n",
      "Epoch 802/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0959\n",
      "Epoch 803/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.1511\n",
      "Epoch 804/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.1130\n",
      "Epoch 805/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.1320\n",
      "Epoch 806/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.1096\n",
      "Epoch 807/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.1258\n",
      "Epoch 808/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.1206\n",
      "Epoch 809/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.1620\n",
      "Epoch 810/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.1155\n",
      "Epoch 811/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.1571\n",
      "Epoch 812/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0970\n",
      "Epoch 813/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0186 - val_loss: 0.1418\n",
      "Epoch 814/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0137 - val_loss: 0.1104\n",
      "Epoch 815/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.1346\n",
      "Epoch 816/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0180 - val_loss: 0.1179\n",
      "Epoch 817/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0144 - val_loss: 0.1349\n",
      "Epoch 818/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.1093\n",
      "Epoch 819/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.1579\n",
      "Epoch 820/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0142 - val_loss: 0.1087\n",
      "Epoch 821/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0288 - val_loss: 0.1558\n",
      "Epoch 822/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.1032\n",
      "Epoch 823/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0136 - val_loss: 0.1194\n",
      "Epoch 824/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0160 - val_loss: 0.1106\n",
      "Epoch 825/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.1342\n",
      "Epoch 826/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0171 - val_loss: 0.1326\n",
      "Epoch 827/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0085 - val_loss: 0.1533\n",
      "Epoch 828/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.1049\n",
      "Epoch 829/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0195 - val_loss: 0.1599\n",
      "Epoch 830/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0172 - val_loss: 0.1111\n",
      "Epoch 831/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.1448\n",
      "Epoch 832/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.1056\n",
      "Epoch 833/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0121 - val_loss: 0.1049\n",
      "Epoch 834/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.1226\n",
      "Epoch 835/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.1496\n",
      "Epoch 836/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.1293\n",
      "Epoch 837/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.1666\n",
      "Epoch 838/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0946\n",
      "Epoch 839/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.1522\n",
      "Epoch 840/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.1107\n",
      "Epoch 841/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.1301\n",
      "Epoch 842/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.1051\n",
      "Epoch 843/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.1198\n",
      "Epoch 844/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.1221\n",
      "Epoch 845/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.1619\n",
      "Epoch 846/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.1196\n",
      "Epoch 847/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.1586\n",
      "Epoch 848/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0965\n",
      "Epoch 849/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.1443\n",
      "Epoch 850/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.1107\n",
      "Epoch 851/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.1337\n",
      "Epoch 852/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.1111\n",
      "Epoch 853/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.1284\n",
      "Epoch 854/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.1152\n",
      "Epoch 855/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.1581\n",
      "Epoch 856/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.1106\n",
      "Epoch 857/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.1548\n",
      "Epoch 858/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0978\n",
      "Epoch 859/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.1332\n",
      "Epoch 860/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.1069\n",
      "Epoch 861/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.1303\n",
      "Epoch 862/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.1234\n",
      "Epoch 863/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.1404\n",
      "Epoch 864/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.1044\n",
      "Epoch 865/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.1586\n",
      "Epoch 866/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.1107\n",
      "Epoch 867/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.1476\n",
      "Epoch 868/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.1030\n",
      "Epoch 869/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.1038\n",
      "Epoch 870/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.1173\n",
      "Epoch 871/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.1472\n",
      "Epoch 872/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.1358\n",
      "Epoch 873/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.1652\n",
      "Epoch 874/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0976\n",
      "Epoch 875/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.1528\n",
      "Epoch 876/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.1093\n",
      "Epoch 877/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.1351\n",
      "Epoch 878/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.1024\n",
      "Epoch 879/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.1114\n",
      "Epoch 880/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.1223\n",
      "Epoch 881/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.1558\n",
      "Epoch 882/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.1247\n",
      "Epoch 883/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.1609\n",
      "Epoch 884/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0963\n",
      "Epoch 885/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.1477\n",
      "Epoch 886/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.1109\n",
      "Epoch 887/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.1311\n",
      "Epoch 888/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.1062\n",
      "Epoch 889/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.1224\n",
      "Epoch 890/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0146 - val_loss: 0.1195\n",
      "Epoch 891/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.1589\n",
      "Epoch 892/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.1149\n",
      "Epoch 893/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.1553\n",
      "Epoch 894/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0959\n",
      "Epoch 895/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.1401\n",
      "Epoch 896/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.1090\n",
      "Epoch 897/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.1318\n",
      "Epoch 898/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.1144\n",
      "Epoch 899/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.1324\n",
      "Epoch 900/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.1084\n",
      "Epoch 901/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.1566\n",
      "Epoch 902/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.1078\n",
      "Epoch 903/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0270 - val_loss: 0.1528\n",
      "Epoch 904/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0109 - val_loss: 0.1006\n",
      "Epoch 905/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.1177\n",
      "Epoch 906/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0148 - val_loss: 0.1071\n",
      "Epoch 907/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.1337\n",
      "Epoch 908/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.1303\n",
      "Epoch 909/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.1501\n",
      "Epoch 910/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0148 - val_loss: 0.1037\n",
      "Epoch 911/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0175 - val_loss: 0.1566\n",
      "Epoch 912/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0159 - val_loss: 0.1093\n",
      "Epoch 913/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0287 - val_loss: 0.1429\n",
      "Epoch 914/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0072 - val_loss: 0.1042\n",
      "Epoch 915/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0109 - val_loss: 0.1046\n",
      "Epoch 916/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0136 - val_loss: 0.1184\n",
      "Epoch 917/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0095 - val_loss: 0.1463\n",
      "Epoch 918/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.1284\n",
      "Epoch 919/1000\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0141 - val_loss: 0.1646\n",
      "Epoch 920/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0948\n",
      "Epoch 921/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.1495\n",
      "Epoch 922/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0145 - val_loss: 0.1092\n",
      "Epoch 923/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0141 - val_loss: 0.1293\n",
      "Epoch 924/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0088 - val_loss: 0.1032\n",
      "Epoch 925/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0110 - val_loss: 0.1181\n",
      "Epoch 926/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0137 - val_loss: 0.1199\n",
      "Epoch 927/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0097 - val_loss: 0.1586\n",
      "Epoch 928/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.1181\n",
      "Epoch 929/1000\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.1568\n",
      "Epoch 930/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0959\n",
      "Epoch 931/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.1413\n",
      "Epoch 932/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.1089\n",
      "Epoch 933/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.1315\n",
      "Epoch 934/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.1098\n",
      "Epoch 935/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.1282\n",
      "Epoch 936/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.1120\n",
      "Epoch 937/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.1561\n",
      "Epoch 938/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.1088\n",
      "Epoch 939/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.1527\n",
      "Epoch 940/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0976\n",
      "Epoch 941/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.1266\n",
      "Epoch 942/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.1042\n",
      "Epoch 943/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.1297\n",
      "Epoch 944/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.1233\n",
      "Epoch 945/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.1399\n",
      "Epoch 946/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.1051\n",
      "Epoch 947/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.1557\n",
      "Epoch 948/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.1099\n",
      "Epoch 949/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.1457\n",
      "Epoch 950/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.1033\n",
      "Epoch 951/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.1030\n",
      "Epoch 952/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.1154\n",
      "Epoch 953/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.1433\n",
      "Epoch 954/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.1313\n",
      "Epoch 955/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.1648\n",
      "Epoch 956/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0951\n",
      "Epoch 957/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.1499\n",
      "Epoch 958/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.1079\n",
      "Epoch 959/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.1306\n",
      "Epoch 960/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.1017\n",
      "Epoch 961/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.1145\n",
      "Epoch 962/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.1188\n",
      "Epoch 963/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.1560\n",
      "Epoch 964/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.1202\n",
      "Epoch 965/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.1576\n",
      "Epoch 966/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0959\n",
      "Epoch 967/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.1423\n",
      "Epoch 968/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.1087\n",
      "Epoch 969/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.1304\n",
      "Epoch 970/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.1068\n",
      "Epoch 971/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.1253\n",
      "Epoch 972/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.1138\n",
      "Epoch 973/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.1559\n",
      "Epoch 974/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.1099\n",
      "Epoch 975/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.1524\n",
      "Epoch 976/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0961\n",
      "Epoch 977/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.1304\n",
      "Epoch 978/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.1043\n",
      "Epoch 979/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.1299\n",
      "Epoch 980/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.1185\n",
      "Epoch 981/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.1365\n",
      "Epoch 982/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.1028\n",
      "Epoch 983/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.1558\n",
      "Epoch 984/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.1081\n",
      "Epoch 985/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.1440\n",
      "Epoch 986/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.1018\n",
      "Epoch 987/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.1052\n",
      "Epoch 988/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.1126\n",
      "Epoch 989/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.1444\n",
      "Epoch 990/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.1341\n",
      "Epoch 991/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.1638\n",
      "Epoch 992/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0976\n",
      "Epoch 993/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.1482\n",
      "Epoch 994/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.1078\n",
      "Epoch 995/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.1334\n",
      "Epoch 996/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.1013\n",
      "Epoch 997/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.1120\n",
      "Epoch 998/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.1179\n",
      "Epoch 999/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 0.1525\n",
      "Epoch 1000/1000\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.1202\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_inputs['X'],\n",
    "          train_inputs['target'],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(valid_inputs['X'], valid_inputs['target']),\n",
    "          callbacks=[earlystop ,best_val],\n",
    "          verbose=1 , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(np.array(history.history['val_loss']))+1\n",
    "model.load_weights(str(sat_var) +'_' +  var_name + '_{:02d}.h5'.format(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0865\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2621\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1230\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1213\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1203\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1201\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1196\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1188\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1177\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1163\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1147\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1128\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1107\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1084\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1031\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1002\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0969\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0935\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0900\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0865\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0835\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0810\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0800\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0819\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0863\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0961\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0933\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0756\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0661\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0624\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0604\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0603\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0599\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0597\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0639\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0748\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0745\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0699\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0578\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0596\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0470\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0504\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0453\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0524\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0570\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0550\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0415\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0451\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0326\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0301\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0454\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0459\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0325\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0408\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0505\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0265\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0474\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0996\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0211\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0286\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0390\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0214\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0211\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0351\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0680\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0227\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0226\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0870\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0215\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0389\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0282\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0243\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0231\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0524\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0306\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0417\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0984\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0277\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0322\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0236\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0336\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0232\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0121\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0498\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0980\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0266\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0379\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0164\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0243\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f12f792ac50>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(valid_inputs['X'],\n",
    "          valid_inputs['target'],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=100,\n",
    "          callbacks=[earlystop ,best_val],\n",
    "          verbose=1 , shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"10\" halign=\"left\">target</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"10\" halign=\"left\">y</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">OMEGA_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t+7</th>\n",
       "      <th>t+8</th>\n",
       "      <th>t+9</th>\n",
       "      <th>t+10</th>\n",
       "      <th>...</th>\n",
       "      <th>t-9</th>\n",
       "      <th>t-8</th>\n",
       "      <th>t-7</th>\n",
       "      <th>t-6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-14 00:00:00</th>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.93065690156849911752</td>\n",
       "      <td>-1.10080417998033053273</td>\n",
       "      <td>-1.21574312915980842220</td>\n",
       "      <td>-1.27703416807841030334</td>\n",
       "      <td>-1.28623771573667489143</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 01:00:00</th>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.10080417998033053273</td>\n",
       "      <td>-1.21574312915980842220</td>\n",
       "      <td>-1.27703416807841030334</td>\n",
       "      <td>-1.28623771573667489143</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 02:00:00</th>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.21574312915980842220</td>\n",
       "      <td>-1.27703416807841030334</td>\n",
       "      <td>-1.28623771573667489143</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 03:00:00</th>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.27703416807841030334</td>\n",
       "      <td>-1.28623771573667489143</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 04:00:00</th>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.28623771573667489143</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                               target                          \\\n",
       "feature                                   y                           \n",
       "time step                               t+1                     t+2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00 -0.33500514781107137985 -0.04749514996851413040   \n",
       "2017-11-14 01:00:00 -0.04749514996851413040  0.32803331121033646456   \n",
       "2017-11-14 02:00:00  0.32803331121033646456  0.71428778670423287434   \n",
       "2017-11-14 03:00:00  0.71428778670423287434  1.05926195957457713881   \n",
       "2017-11-14 04:00:00  1.05926195957457713881  1.22118133280867247059   \n",
       "\n",
       "tensor                                                             \\\n",
       "feature                                                             \n",
       "time step                              t+3                    t+4   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-14 00:00:00 0.32803331121033646456 0.71428778670423287434   \n",
       "2017-11-14 01:00:00 0.71428778670423287434 1.05926195957457713881   \n",
       "2017-11-14 02:00:00 1.05926195957457713881 1.22118133280867247059   \n",
       "2017-11-14 03:00:00 1.22118133280867247059 1.19273531193175696963   \n",
       "2017-11-14 04:00:00 1.19273531193175696963 1.09942999377016903395   \n",
       "\n",
       "tensor                                                             \\\n",
       "feature                                                             \n",
       "time step                              t+5                    t+6   \n",
       "Epoch_Time_of_Clock                                                 \n",
       "2017-11-14 00:00:00 1.05926195957457713881 1.22118133280867247059   \n",
       "2017-11-14 01:00:00 1.22118133280867247059 1.19273531193175696963   \n",
       "2017-11-14 02:00:00 1.19273531193175696963 1.09942999377016903395   \n",
       "2017-11-14 03:00:00 1.09942999377016903395 0.96332459895237088077   \n",
       "2017-11-14 04:00:00 0.96332459895237088077 0.70140826416528545550   \n",
       "\n",
       "tensor                                                              \\\n",
       "feature                                                              \n",
       "time step                              t+7                     t+8   \n",
       "Epoch_Time_of_Clock                                                  \n",
       "2017-11-14 00:00:00 1.19273531193175696963  1.09942999377016903395   \n",
       "2017-11-14 01:00:00 1.09942999377016903395  0.96332459895237088077   \n",
       "2017-11-14 02:00:00 0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 03:00:00 0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 04:00:00 0.32615303693918301642 -0.04749514996851413040   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+9                    t+10   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00  0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 01:00:00  0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 02:00:00  0.32615303693918301642 -0.04749514996851413040   \n",
       "2017-11-14 03:00:00 -0.04749514996851413040 -0.37568496094050468637   \n",
       "2017-11-14 04:00:00 -0.37568496094050468637 -0.64304871699996779544   \n",
       "\n",
       "tensor                        ...                                 X  \\\n",
       "feature                       ...                         OMEGA_dot   \n",
       "time step                     ...                               t-9   \n",
       "Epoch_Time_of_Clock           ...                                     \n",
       "2017-11-14 00:00:00           ...           -0.93065690156849911752   \n",
       "2017-11-14 01:00:00           ...           -1.10080417998033053273   \n",
       "2017-11-14 02:00:00           ...           -1.21574312915980842220   \n",
       "2017-11-14 03:00:00           ...           -1.27703416807841030334   \n",
       "2017-11-14 04:00:00           ...           -1.28623771573667489143   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-8                     t-7   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00 -1.10080417998033053273 -1.21574312915980842220   \n",
       "2017-11-14 01:00:00 -1.21574312915980842220 -1.27703416807841030334   \n",
       "2017-11-14 02:00:00 -1.27703416807841030334 -1.28623771573667489143   \n",
       "2017-11-14 03:00:00 -1.28623771573667489143 -1.24491419109156065126   \n",
       "2017-11-14 04:00:00 -1.24491419109156065126 -1.15462401312907236850   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-6                     t-5   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00 -1.27703416807841030334 -1.28623771573667489143   \n",
       "2017-11-14 01:00:00 -1.28623771573667489143 -1.24491419109156065126   \n",
       "2017-11-14 02:00:00 -1.24491419109156065126 -1.15462401312907236850   \n",
       "2017-11-14 03:00:00 -1.15462401312907236850 -1.01692760083522149017   \n",
       "2017-11-14 04:00:00 -1.01692760083522149017 -0.83338537319602012943   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-4                     t-3   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00 -1.24491419109156065126 -1.15462401312907236850   \n",
       "2017-11-14 01:00:00 -1.15462401312907236850 -1.01692760083522149017   \n",
       "2017-11-14 02:00:00 -1.01692760083522149017 -0.83338537319602012943   \n",
       "2017-11-14 03:00:00 -0.83338537319602012943 -0.60555774919747740181   \n",
       "2017-11-14 04:00:00 -0.60555774919747740181 -0.33500514781107137985   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-2                     t-1   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00 -1.01692760083522149017 -0.83338537319602012943   \n",
       "2017-11-14 01:00:00 -0.83338537319602012943 -0.60555774919747740181   \n",
       "2017-11-14 02:00:00 -0.60555774919747740181 -0.33500514781107137985   \n",
       "2017-11-14 03:00:00 -0.33500514781107137985 -0.04749514996851413040   \n",
       "2017-11-14 04:00:00 -0.04749514996851413040  0.32803331121033646456   \n",
       "\n",
       "tensor                                       \n",
       "feature                                      \n",
       "time step                                 t  \n",
       "Epoch_Time_of_Clock                          \n",
       "2017-11-14 00:00:00 -0.60555774919747740181  \n",
       "2017-11-14 01:00:00 -0.33500514781107137985  \n",
       "2017-11-14 02:00:00 -0.04749514996851413040  \n",
       "2017-11-14 03:00:00  0.32803331121033646456  \n",
       "2017-11-14 04:00:00  0.71428778670423287434  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look_back_dt = dt.datetime.strptime(test_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1)\n",
    "#test = df.copy()[test_start_dt:end_dt][Paras[var_name]]\n",
    "test[Paras[var_name]] = X_scaler.transform(test)\n",
    "test_inputs = TimeSeriesTensor(test, var_name, HORIZON, tensor_structure,freq =None)\n",
    "test_inputs.dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>tensor</th>\n",
       "      <th colspan=\"10\" halign=\"left\">target</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"10\" halign=\"left\">y</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">OMEGA_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time step</th>\n",
       "      <th>t+1</th>\n",
       "      <th>t+2</th>\n",
       "      <th>t+3</th>\n",
       "      <th>t+4</th>\n",
       "      <th>t+5</th>\n",
       "      <th>t+6</th>\n",
       "      <th>t+7</th>\n",
       "      <th>t+8</th>\n",
       "      <th>t+9</th>\n",
       "      <th>t+10</th>\n",
       "      <th>...</th>\n",
       "      <th>t-9</th>\n",
       "      <th>t-8</th>\n",
       "      <th>t-7</th>\n",
       "      <th>t-6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-14 00:00:00</th>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.93065690156849911752</td>\n",
       "      <td>-1.10080417998033053273</td>\n",
       "      <td>-1.21574312915980842220</td>\n",
       "      <td>-1.27703416807841030334</td>\n",
       "      <td>-1.28623771573667489143</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 01:00:00</th>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.10080417998033053273</td>\n",
       "      <td>-1.21574312915980842220</td>\n",
       "      <td>-1.27703416807841030334</td>\n",
       "      <td>-1.28623771573667489143</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 02:00:00</th>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.21574312915980842220</td>\n",
       "      <td>-1.27703416807841030334</td>\n",
       "      <td>-1.28623771573667489143</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 03:00:00</th>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.27703416807841030334</td>\n",
       "      <td>-1.28623771573667489143</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 04:00:00</th>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.28623771573667489143</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 05:00:00</th>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.24491419109156065126</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 06:00:00</th>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.15462401312907236850</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 07:00:00</th>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.01692760083522149017</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 08:00:00</th>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.83338537319602012943</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 09:00:00</th>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.60555774919747740181</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 10:00:00</th>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.33500514781107137985</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 11:00:00</th>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 12:00:00</th>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.32803331121033646456</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 13:00:00</th>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.71428778670423287434</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 14:00:00</th>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>...</td>\n",
       "      <td>1.05926195957457713881</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 15:00:00</th>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>...</td>\n",
       "      <td>1.22118133280867247059</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 16:00:00</th>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>...</td>\n",
       "      <td>1.19273531193175696963</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 17:00:00</th>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>...</td>\n",
       "      <td>1.09942999377016903395</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 18:00:00</th>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.96332459895237088077</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 19:00:00</th>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>1.18956312249132434289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70140826416528545550</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 20:00:00</th>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>1.18956312249132434289</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.32615303693918301642</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 21:00:00</th>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>1.18956312249132434289</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.32087715502241476173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 22:00:00</th>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>1.18956312249132434289</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.32087715502241476173</td>\n",
       "      <td>1.20348892381350824898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.37568496094050468637</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 23:00:00</th>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>1.18956312249132434289</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.32087715502241476173</td>\n",
       "      <td>1.20348892381350824898</td>\n",
       "      <td>1.03423176485774215294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.64304871699996779544</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 00:00:00</th>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>1.18956312249132434289</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.32087715502241476173</td>\n",
       "      <td>1.20348892381350824898</td>\n",
       "      <td>1.03423176485774215294</td>\n",
       "      <td>0.75885363374001235659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.85547013537058791410</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 01:00:00</th>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>1.18956312249132434289</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.32087715502241476173</td>\n",
       "      <td>1.20348892381350824898</td>\n",
       "      <td>1.03423176485774215294</td>\n",
       "      <td>0.75885363374001235659</td>\n",
       "      <td>0.39538395910800050448</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.01413601029847377788</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 02:00:00</th>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>1.18956312249132434289</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.32087715502241476173</td>\n",
       "      <td>1.20348892381350824898</td>\n",
       "      <td>1.03423176485774215294</td>\n",
       "      <td>0.75885363374001235659</td>\n",
       "      <td>0.39538395910800050448</td>\n",
       "      <td>0.03596579095793609598</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.12023313600068541440</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 03:00:00</th>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>1.18956312249132434289</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.32087715502241476173</td>\n",
       "      <td>1.20348892381350824898</td>\n",
       "      <td>1.03423176485774215294</td>\n",
       "      <td>0.75885363374001235659</td>\n",
       "      <td>0.39538395910800050448</td>\n",
       "      <td>0.03596579095793609598</td>\n",
       "      <td>-0.27111263126045964666</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.17494830669427896552</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 04:00:00</th>\n",
       "      <td>1.18956312249132434289</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.32087715502241476173</td>\n",
       "      <td>1.20348892381350824898</td>\n",
       "      <td>1.03423176485774215294</td>\n",
       "      <td>0.75885363374001235659</td>\n",
       "      <td>0.39538395910800050448</td>\n",
       "      <td>0.03596579095793609598</td>\n",
       "      <td>-0.27111263126045964666</td>\n",
       "      <td>-0.52472937119813389817</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.17946831659631423683</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15 05:00:00</th>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.32087715502241476173</td>\n",
       "      <td>1.20348892381350824898</td>\n",
       "      <td>1.03423176485774215294</td>\n",
       "      <td>0.75885363374001235659</td>\n",
       "      <td>0.39538395910800050448</td>\n",
       "      <td>0.03596579095793609598</td>\n",
       "      <td>-0.27111263126045964666</td>\n",
       "      <td>-0.52472937119813389817</td>\n",
       "      <td>-0.72581320134154281298</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.13497995993837852424</td>\n",
       "      <td>-1.04267003093752497200</td>\n",
       "      <td>-0.90372532381081616126</td>\n",
       "      <td>-0.71933263277530545832</td>\n",
       "      <td>-0.49067875207711098007</td>\n",
       "      <td>-0.21895047591875707593</td>\n",
       "      <td>0.09466540146816218582</td>\n",
       "      <td>0.44898208588112492601</td>\n",
       "      <td>0.83964953951184229108</td>\n",
       "      <td>1.18956312249132434289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 18:00:00</th>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>...</td>\n",
       "      <td>1.16637650457455577246</td>\n",
       "      <td>0.87791323568086254703</td>\n",
       "      <td>0.52151834541413533142</td>\n",
       "      <td>0.17540672701975912329</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.36272895390056852927</td>\n",
       "      <td>-0.55471669787537580820</td>\n",
       "      <td>-0.69644997832958432937</td>\n",
       "      <td>-0.78877799585452845754</td>\n",
       "      <td>-0.83254995105606888206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 19:00:00</th>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.87791323568086254703</td>\n",
       "      <td>0.52151834541413533142</td>\n",
       "      <td>0.17540672701975912329</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.36272895390056852927</td>\n",
       "      <td>-0.55471669787537580820</td>\n",
       "      <td>-0.69644997832958432937</td>\n",
       "      <td>-0.78877799585452845754</td>\n",
       "      <td>-0.83254995105606888206</td>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 20:00:00</th>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52151834541413533142</td>\n",
       "      <td>0.17540672701975912329</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.36272895390056852927</td>\n",
       "      <td>-0.55471669787537580820</td>\n",
       "      <td>-0.69644997832958432937</td>\n",
       "      <td>-0.78877799585452845754</td>\n",
       "      <td>-0.83254995105606888206</td>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 21:00:00</th>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.17540672701975912329</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.36272895390056852927</td>\n",
       "      <td>-0.55471669787537580820</td>\n",
       "      <td>-0.69644997832958432937</td>\n",
       "      <td>-0.78877799585452845754</td>\n",
       "      <td>-0.83254995105606888206</td>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 22:00:00</th>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04749514996851413040</td>\n",
       "      <td>-0.36272895390056852927</td>\n",
       "      <td>-0.55471669787537580820</td>\n",
       "      <td>-0.69644997832958432937</td>\n",
       "      <td>-0.78877799585452845754</td>\n",
       "      <td>-0.83254995105606888206</td>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16 23:00:00</th>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36272895390056852927</td>\n",
       "      <td>-0.55471669787537580820</td>\n",
       "      <td>-0.69644997832958432937</td>\n",
       "      <td>-0.78877799585452845754</td>\n",
       "      <td>-0.83254995105606888206</td>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 00:00:00</th>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.55471669787537580820</td>\n",
       "      <td>-0.69644997832958432937</td>\n",
       "      <td>-0.78877799585452845754</td>\n",
       "      <td>-0.83254995105606888206</td>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 01:00:00</th>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.69644997832958432937</td>\n",
       "      <td>-0.78877799585452845754</td>\n",
       "      <td>-0.83254995105606888206</td>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 02:00:00</th>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.78877799585452845754</td>\n",
       "      <td>-0.83254995105606888206</td>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 03:00:00</th>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.83254995105606888206</td>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 04:00:00</th>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.82861504452554479716</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 05:00:00</th>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.77782247689786765399</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 06:00:00</th>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.68102144873532100000</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 07:00:00</th>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.53906116065829468020</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 08:00:00</th>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.35279081327264949497</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 09:00:00</th>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.12305960718424864553</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 10:00:00</th>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14928325703010486558</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 11:00:00</th>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46338857875001654429</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 12:00:00</th>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.81840715738415448222</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 13:00:00</th>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "      <td>...</td>\n",
       "      <td>1.21035565823367763372</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 14:00:00</th>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "      <td>0.09415733591199586250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.56316566292614544942</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 15:00:00</th>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "      <td>0.09415733591199586250</td>\n",
       "      <td>0.36178389202753025167</td>\n",
       "      <td>...</td>\n",
       "      <td>1.73394503050644011033</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 16:00:00</th>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "      <td>0.09415733591199586250</td>\n",
       "      <td>0.36178389202753025167</td>\n",
       "      <td>0.66956175414617868924</td>\n",
       "      <td>...</td>\n",
       "      <td>1.69767879398128451740</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 17:00:00</th>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "      <td>0.09415733591199586250</td>\n",
       "      <td>0.36178389202753025167</td>\n",
       "      <td>0.66956175414617868924</td>\n",
       "      <td>1.01658733531831857455</td>\n",
       "      <td>...</td>\n",
       "      <td>1.54796165933561646888</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 18:00:00</th>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "      <td>0.09415733591199586250</td>\n",
       "      <td>0.36178389202753025167</td>\n",
       "      <td>0.66956175414617868924</td>\n",
       "      <td>1.01658733531831857455</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.32841208887390038740</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 19:00:00</th>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "      <td>0.09415733591199586250</td>\n",
       "      <td>0.36178389202753025167</td>\n",
       "      <td>0.66956175414617868924</td>\n",
       "      <td>1.01658733531831857455</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.74393341176094396339</td>\n",
       "      <td>...</td>\n",
       "      <td>1.02916982803134482438</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 20:00:00</th>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "      <td>0.09415733591199586250</td>\n",
       "      <td>0.36178389202753025167</td>\n",
       "      <td>0.66956175414617868924</td>\n",
       "      <td>1.01658733531831857455</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.74393341176094396339</td>\n",
       "      <td>1.91467860483599472410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67842741233001946366</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 21:00:00</th>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "      <td>0.09415733591199586250</td>\n",
       "      <td>0.36178389202753025167</td>\n",
       "      <td>0.66956175414617868924</td>\n",
       "      <td>1.01658733531831857455</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.74393341176094396339</td>\n",
       "      <td>1.91467860483599472410</td>\n",
       "      <td>1.88167235117187092719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34334618915546943896</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 22:00:00</th>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "      <td>0.09415733591199586250</td>\n",
       "      <td>0.36178389202753025167</td>\n",
       "      <td>0.66956175414617868924</td>\n",
       "      <td>1.01658733531831857455</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.74393341176094396339</td>\n",
       "      <td>1.91467860483599472410</td>\n",
       "      <td>1.88167235117187092719</td>\n",
       "      <td>1.72401587348606399885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05867728937758159846</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17 23:00:00</th>\n",
       "      <td>0.09415733591199586250</td>\n",
       "      <td>0.36178389202753025167</td>\n",
       "      <td>0.66956175414617868924</td>\n",
       "      <td>1.01658733531831857455</td>\n",
       "      <td>1.40530733235496274602</td>\n",
       "      <td>1.74393341176094396339</td>\n",
       "      <td>1.91467860483599472410</td>\n",
       "      <td>1.88167235117187092719</td>\n",
       "      <td>1.72401587348606399885</td>\n",
       "      <td>1.48744543823312547204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.17499726074043489565</td>\n",
       "      <td>-0.35858104817723879254</td>\n",
       "      <td>-0.49297765988243807955</td>\n",
       "      <td>-0.57909068284922038394</td>\n",
       "      <td>-0.61782370402718911340</td>\n",
       "      <td>-0.61008031039500765225</td>\n",
       "      <td>-0.55676408893133710887</td>\n",
       "      <td>-0.45877862660030960251</td>\n",
       "      <td>-0.31702751036605747448</td>\n",
       "      <td>-0.13241432720724649608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tensor                               target                          \\\n",
       "feature                                   y                           \n",
       "time step                               t+1                     t+2   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00 -0.33500514781107137985 -0.04749514996851413040   \n",
       "2017-11-14 01:00:00 -0.04749514996851413040  0.32803331121033646456   \n",
       "2017-11-14 02:00:00  0.32803331121033646456  0.71428778670423287434   \n",
       "2017-11-14 03:00:00  0.71428778670423287434  1.05926195957457713881   \n",
       "2017-11-14 04:00:00  1.05926195957457713881  1.22118133280867247059   \n",
       "2017-11-14 05:00:00  1.22118133280867247059  1.19273531193175696963   \n",
       "2017-11-14 06:00:00  1.19273531193175696963  1.09942999377016903395   \n",
       "2017-11-14 07:00:00  1.09942999377016903395  0.96332459895237088077   \n",
       "2017-11-14 08:00:00  0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 09:00:00  0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 10:00:00  0.32615303693918301642 -0.04749514996851413040   \n",
       "2017-11-14 11:00:00 -0.04749514996851413040 -0.37568496094050468637   \n",
       "2017-11-14 12:00:00 -0.37568496094050468637 -0.64304871699996779544   \n",
       "2017-11-14 13:00:00 -0.64304871699996779544 -0.85547013537058791410   \n",
       "2017-11-14 14:00:00 -0.85547013537058791410 -1.01413601029847377788   \n",
       "2017-11-14 15:00:00 -1.01413601029847377788 -1.12023313600068541440   \n",
       "2017-11-14 16:00:00 -1.12023313600068541440 -1.17494830669427896552   \n",
       "2017-11-14 17:00:00 -1.17494830669427896552 -1.17946831659631423683   \n",
       "2017-11-14 18:00:00 -1.17946831659631423683 -1.13497995993837852424   \n",
       "2017-11-14 19:00:00 -1.13497995993837852424 -1.04267003093752497200   \n",
       "2017-11-14 20:00:00 -1.04267003093752497200 -0.90372532381081616126   \n",
       "2017-11-14 21:00:00 -0.90372532381081616126 -0.71933263277530545832   \n",
       "2017-11-14 22:00:00 -0.71933263277530545832 -0.49067875207711098007   \n",
       "2017-11-14 23:00:00 -0.49067875207711098007 -0.21895047591875707593   \n",
       "2017-11-15 00:00:00 -0.21895047591875707593  0.09466540146816218582   \n",
       "2017-11-15 01:00:00  0.09466540146816218582  0.44898208588112492601   \n",
       "2017-11-15 02:00:00  0.44898208588112492601  0.83964953951184229108   \n",
       "2017-11-15 03:00:00  0.83964953951184229108  1.18956312249132434289   \n",
       "2017-11-15 04:00:00  1.18956312249132434289  1.40530733235496274602   \n",
       "2017-11-15 05:00:00  1.40530733235496274602  1.32087715502241476173   \n",
       "...                                     ...                     ...   \n",
       "2017-11-16 18:00:00 -0.82861504452554479716 -0.77782247689786765399   \n",
       "2017-11-16 19:00:00 -0.77782247689786765399 -0.68102144873532100000   \n",
       "2017-11-16 20:00:00 -0.68102144873532100000 -0.53906116065829468020   \n",
       "2017-11-16 21:00:00 -0.53906116065829468020 -0.35279081327264949497   \n",
       "2017-11-16 22:00:00 -0.35279081327264949497 -0.12305960718424864553   \n",
       "2017-11-16 23:00:00 -0.12305960718424864553  0.14928325703010486558   \n",
       "2017-11-17 00:00:00  0.14928325703010486558  0.46338857875001654429   \n",
       "2017-11-17 01:00:00  0.46338857875001654429  0.81840715738415448222   \n",
       "2017-11-17 02:00:00  0.81840715738415448222  1.21035565823367763372   \n",
       "2017-11-17 03:00:00  1.21035565823367763372  1.56316566292614544942   \n",
       "2017-11-17 04:00:00  1.56316566292614544942  1.73394503050644011033   \n",
       "2017-11-17 05:00:00  1.73394503050644011033  1.69767879398128451740   \n",
       "2017-11-17 06:00:00  1.69767879398128451740  1.54796165933561646888   \n",
       "2017-11-17 07:00:00  1.54796165933561646888  1.32841208887390038740   \n",
       "2017-11-17 08:00:00  1.32841208887390038740  1.02916982803134482438   \n",
       "2017-11-17 09:00:00  1.02916982803134482438  0.67842741233001946366   \n",
       "2017-11-17 10:00:00  0.67842741233001946366  0.34334618915546943896   \n",
       "2017-11-17 11:00:00  0.34334618915546943896  0.05867728937758159846   \n",
       "2017-11-17 12:00:00  0.05867728937758159846 -0.17499726074043489565   \n",
       "2017-11-17 13:00:00 -0.17499726074043489565 -0.35858104817723879254   \n",
       "2017-11-17 14:00:00 -0.35858104817723879254 -0.49297765988243807955   \n",
       "2017-11-17 15:00:00 -0.49297765988243807955 -0.57909068284922038394   \n",
       "2017-11-17 16:00:00 -0.57909068284922038394 -0.61782370402718911340   \n",
       "2017-11-17 17:00:00 -0.61782370402718911340 -0.61008031039500765225   \n",
       "2017-11-17 18:00:00 -0.61008031039500765225 -0.55676408893133710887   \n",
       "2017-11-17 19:00:00 -0.55676408893133710887 -0.45877862660030960251   \n",
       "2017-11-17 20:00:00 -0.45877862660030960251 -0.31702751036605747448   \n",
       "2017-11-17 21:00:00 -0.31702751036605747448 -0.13241432720724649608   \n",
       "2017-11-17 22:00:00 -0.13241432720724649608  0.09415733591199586250   \n",
       "2017-11-17 23:00:00  0.09415733591199586250  0.36178389202753025167   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+3                     t+4   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00  0.32803331121033646456  0.71428778670423287434   \n",
       "2017-11-14 01:00:00  0.71428778670423287434  1.05926195957457713881   \n",
       "2017-11-14 02:00:00  1.05926195957457713881  1.22118133280867247059   \n",
       "2017-11-14 03:00:00  1.22118133280867247059  1.19273531193175696963   \n",
       "2017-11-14 04:00:00  1.19273531193175696963  1.09942999377016903395   \n",
       "2017-11-14 05:00:00  1.09942999377016903395  0.96332459895237088077   \n",
       "2017-11-14 06:00:00  0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 07:00:00  0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 08:00:00  0.32615303693918301642 -0.04749514996851413040   \n",
       "2017-11-14 09:00:00 -0.04749514996851413040 -0.37568496094050468637   \n",
       "2017-11-14 10:00:00 -0.37568496094050468637 -0.64304871699996779544   \n",
       "2017-11-14 11:00:00 -0.64304871699996779544 -0.85547013537058791410   \n",
       "2017-11-14 12:00:00 -0.85547013537058791410 -1.01413601029847377788   \n",
       "2017-11-14 13:00:00 -1.01413601029847377788 -1.12023313600068541440   \n",
       "2017-11-14 14:00:00 -1.12023313600068541440 -1.17494830669427896552   \n",
       "2017-11-14 15:00:00 -1.17494830669427896552 -1.17946831659631423683   \n",
       "2017-11-14 16:00:00 -1.17946831659631423683 -1.13497995993837852424   \n",
       "2017-11-14 17:00:00 -1.13497995993837852424 -1.04267003093752497200   \n",
       "2017-11-14 18:00:00 -1.04267003093752497200 -0.90372532381081616126   \n",
       "2017-11-14 19:00:00 -0.90372532381081616126 -0.71933263277530545832   \n",
       "2017-11-14 20:00:00 -0.71933263277530545832 -0.49067875207711098007   \n",
       "2017-11-14 21:00:00 -0.49067875207711098007 -0.21895047591875707593   \n",
       "2017-11-14 22:00:00 -0.21895047591875707593  0.09466540146816218582   \n",
       "2017-11-14 23:00:00  0.09466540146816218582  0.44898208588112492601   \n",
       "2017-11-15 00:00:00  0.44898208588112492601  0.83964953951184229108   \n",
       "2017-11-15 01:00:00  0.83964953951184229108  1.18956312249132434289   \n",
       "2017-11-15 02:00:00  1.18956312249132434289  1.40530733235496274602   \n",
       "2017-11-15 03:00:00  1.40530733235496274602  1.32087715502241476173   \n",
       "2017-11-15 04:00:00  1.32087715502241476173  1.20348892381350824898   \n",
       "2017-11-15 05:00:00  1.20348892381350824898  1.03423176485774215294   \n",
       "...                                     ...                     ...   \n",
       "2017-11-16 18:00:00 -0.68102144873532100000 -0.53906116065829468020   \n",
       "2017-11-16 19:00:00 -0.53906116065829468020 -0.35279081327264949497   \n",
       "2017-11-16 20:00:00 -0.35279081327264949497 -0.12305960718424864553   \n",
       "2017-11-16 21:00:00 -0.12305960718424864553  0.14928325703010486558   \n",
       "2017-11-16 22:00:00  0.14928325703010486558  0.46338857875001654429   \n",
       "2017-11-16 23:00:00  0.46338857875001654429  0.81840715738415448222   \n",
       "2017-11-17 00:00:00  0.81840715738415448222  1.21035565823367763372   \n",
       "2017-11-17 01:00:00  1.21035565823367763372  1.56316566292614544942   \n",
       "2017-11-17 02:00:00  1.56316566292614544942  1.73394503050644011033   \n",
       "2017-11-17 03:00:00  1.73394503050644011033  1.69767879398128451740   \n",
       "2017-11-17 04:00:00  1.69767879398128451740  1.54796165933561646888   \n",
       "2017-11-17 05:00:00  1.54796165933561646888  1.32841208887390038740   \n",
       "2017-11-17 06:00:00  1.32841208887390038740  1.02916982803134482438   \n",
       "2017-11-17 07:00:00  1.02916982803134482438  0.67842741233001946366   \n",
       "2017-11-17 08:00:00  0.67842741233001946366  0.34334618915546943896   \n",
       "2017-11-17 09:00:00  0.34334618915546943896  0.05867728937758159846   \n",
       "2017-11-17 10:00:00  0.05867728937758159846 -0.17499726074043489565   \n",
       "2017-11-17 11:00:00 -0.17499726074043489565 -0.35858104817723879254   \n",
       "2017-11-17 12:00:00 -0.35858104817723879254 -0.49297765988243807955   \n",
       "2017-11-17 13:00:00 -0.49297765988243807955 -0.57909068284922038394   \n",
       "2017-11-17 14:00:00 -0.57909068284922038394 -0.61782370402718911340   \n",
       "2017-11-17 15:00:00 -0.61782370402718911340 -0.61008031039500765225   \n",
       "2017-11-17 16:00:00 -0.61008031039500765225 -0.55676408893133710887   \n",
       "2017-11-17 17:00:00 -0.55676408893133710887 -0.45877862660030960251   \n",
       "2017-11-17 18:00:00 -0.45877862660030960251 -0.31702751036605747448   \n",
       "2017-11-17 19:00:00 -0.31702751036605747448 -0.13241432720724649608   \n",
       "2017-11-17 20:00:00 -0.13241432720724649608  0.09415733591199586250   \n",
       "2017-11-17 21:00:00  0.09415733591199586250  0.36178389202753025167   \n",
       "2017-11-17 22:00:00  0.36178389202753025167  0.66956175414617868924   \n",
       "2017-11-17 23:00:00  0.66956175414617868924  1.01658733531831857455   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+5                     t+6   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00  1.05926195957457713881  1.22118133280867247059   \n",
       "2017-11-14 01:00:00  1.22118133280867247059  1.19273531193175696963   \n",
       "2017-11-14 02:00:00  1.19273531193175696963  1.09942999377016903395   \n",
       "2017-11-14 03:00:00  1.09942999377016903395  0.96332459895237088077   \n",
       "2017-11-14 04:00:00  0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 05:00:00  0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 06:00:00  0.32615303693918301642 -0.04749514996851413040   \n",
       "2017-11-14 07:00:00 -0.04749514996851413040 -0.37568496094050468637   \n",
       "2017-11-14 08:00:00 -0.37568496094050468637 -0.64304871699996779544   \n",
       "2017-11-14 09:00:00 -0.64304871699996779544 -0.85547013537058791410   \n",
       "2017-11-14 10:00:00 -0.85547013537058791410 -1.01413601029847377788   \n",
       "2017-11-14 11:00:00 -1.01413601029847377788 -1.12023313600068541440   \n",
       "2017-11-14 12:00:00 -1.12023313600068541440 -1.17494830669427896552   \n",
       "2017-11-14 13:00:00 -1.17494830669427896552 -1.17946831659631423683   \n",
       "2017-11-14 14:00:00 -1.17946831659631423683 -1.13497995993837852424   \n",
       "2017-11-14 15:00:00 -1.13497995993837852424 -1.04267003093752497200   \n",
       "2017-11-14 16:00:00 -1.04267003093752497200 -0.90372532381081616126   \n",
       "2017-11-14 17:00:00 -0.90372532381081616126 -0.71933263277530545832   \n",
       "2017-11-14 18:00:00 -0.71933263277530545832 -0.49067875207711098007   \n",
       "2017-11-14 19:00:00 -0.49067875207711098007 -0.21895047591875707593   \n",
       "2017-11-14 20:00:00 -0.21895047591875707593  0.09466540146816218582   \n",
       "2017-11-14 21:00:00  0.09466540146816218582  0.44898208588112492601   \n",
       "2017-11-14 22:00:00  0.44898208588112492601  0.83964953951184229108   \n",
       "2017-11-14 23:00:00  0.83964953951184229108  1.18956312249132434289   \n",
       "2017-11-15 00:00:00  1.18956312249132434289  1.40530733235496274602   \n",
       "2017-11-15 01:00:00  1.40530733235496274602  1.32087715502241476173   \n",
       "2017-11-15 02:00:00  1.32087715502241476173  1.20348892381350824898   \n",
       "2017-11-15 03:00:00  1.20348892381350824898  1.03423176485774215294   \n",
       "2017-11-15 04:00:00  1.03423176485774215294  0.75885363374001235659   \n",
       "2017-11-15 05:00:00  0.75885363374001235659  0.39538395910800050448   \n",
       "...                                     ...                     ...   \n",
       "2017-11-16 18:00:00 -0.35279081327264949497 -0.12305960718424864553   \n",
       "2017-11-16 19:00:00 -0.12305960718424864553  0.14928325703010486558   \n",
       "2017-11-16 20:00:00  0.14928325703010486558  0.46338857875001654429   \n",
       "2017-11-16 21:00:00  0.46338857875001654429  0.81840715738415448222   \n",
       "2017-11-16 22:00:00  0.81840715738415448222  1.21035565823367763372   \n",
       "2017-11-16 23:00:00  1.21035565823367763372  1.56316566292614544942   \n",
       "2017-11-17 00:00:00  1.56316566292614544942  1.73394503050644011033   \n",
       "2017-11-17 01:00:00  1.73394503050644011033  1.69767879398128451740   \n",
       "2017-11-17 02:00:00  1.69767879398128451740  1.54796165933561646888   \n",
       "2017-11-17 03:00:00  1.54796165933561646888  1.32841208887390038740   \n",
       "2017-11-17 04:00:00  1.32841208887390038740  1.02916982803134482438   \n",
       "2017-11-17 05:00:00  1.02916982803134482438  0.67842741233001946366   \n",
       "2017-11-17 06:00:00  0.67842741233001946366  0.34334618915546943896   \n",
       "2017-11-17 07:00:00  0.34334618915546943896  0.05867728937758159846   \n",
       "2017-11-17 08:00:00  0.05867728937758159846 -0.17499726074043489565   \n",
       "2017-11-17 09:00:00 -0.17499726074043489565 -0.35858104817723879254   \n",
       "2017-11-17 10:00:00 -0.35858104817723879254 -0.49297765988243807955   \n",
       "2017-11-17 11:00:00 -0.49297765988243807955 -0.57909068284922038394   \n",
       "2017-11-17 12:00:00 -0.57909068284922038394 -0.61782370402718911340   \n",
       "2017-11-17 13:00:00 -0.61782370402718911340 -0.61008031039500765225   \n",
       "2017-11-17 14:00:00 -0.61008031039500765225 -0.55676408893133710887   \n",
       "2017-11-17 15:00:00 -0.55676408893133710887 -0.45877862660030960251   \n",
       "2017-11-17 16:00:00 -0.45877862660030960251 -0.31702751036605747448   \n",
       "2017-11-17 17:00:00 -0.31702751036605747448 -0.13241432720724649608   \n",
       "2017-11-17 18:00:00 -0.13241432720724649608  0.09415733591199586250   \n",
       "2017-11-17 19:00:00  0.09415733591199586250  0.36178389202753025167   \n",
       "2017-11-17 20:00:00  0.36178389202753025167  0.66956175414617868924   \n",
       "2017-11-17 21:00:00  0.66956175414617868924  1.01658733531831857455   \n",
       "2017-11-17 22:00:00  1.01658733531831857455  1.40530733235496274602   \n",
       "2017-11-17 23:00:00  1.40530733235496274602  1.74393341176094396339   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+7                     t+8   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00  1.19273531193175696963  1.09942999377016903395   \n",
       "2017-11-14 01:00:00  1.09942999377016903395  0.96332459895237088077   \n",
       "2017-11-14 02:00:00  0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 03:00:00  0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 04:00:00  0.32615303693918301642 -0.04749514996851413040   \n",
       "2017-11-14 05:00:00 -0.04749514996851413040 -0.37568496094050468637   \n",
       "2017-11-14 06:00:00 -0.37568496094050468637 -0.64304871699996779544   \n",
       "2017-11-14 07:00:00 -0.64304871699996779544 -0.85547013537058791410   \n",
       "2017-11-14 08:00:00 -0.85547013537058791410 -1.01413601029847377788   \n",
       "2017-11-14 09:00:00 -1.01413601029847377788 -1.12023313600068541440   \n",
       "2017-11-14 10:00:00 -1.12023313600068541440 -1.17494830669427896552   \n",
       "2017-11-14 11:00:00 -1.17494830669427896552 -1.17946831659631423683   \n",
       "2017-11-14 12:00:00 -1.17946831659631423683 -1.13497995993837852424   \n",
       "2017-11-14 13:00:00 -1.13497995993837852424 -1.04267003093752497200   \n",
       "2017-11-14 14:00:00 -1.04267003093752497200 -0.90372532381081616126   \n",
       "2017-11-14 15:00:00 -0.90372532381081616126 -0.71933263277530545832   \n",
       "2017-11-14 16:00:00 -0.71933263277530545832 -0.49067875207711098007   \n",
       "2017-11-14 17:00:00 -0.49067875207711098007 -0.21895047591875707593   \n",
       "2017-11-14 18:00:00 -0.21895047591875707593  0.09466540146816218582   \n",
       "2017-11-14 19:00:00  0.09466540146816218582  0.44898208588112492601   \n",
       "2017-11-14 20:00:00  0.44898208588112492601  0.83964953951184229108   \n",
       "2017-11-14 21:00:00  0.83964953951184229108  1.18956312249132434289   \n",
       "2017-11-14 22:00:00  1.18956312249132434289  1.40530733235496274602   \n",
       "2017-11-14 23:00:00  1.40530733235496274602  1.32087715502241476173   \n",
       "2017-11-15 00:00:00  1.32087715502241476173  1.20348892381350824898   \n",
       "2017-11-15 01:00:00  1.20348892381350824898  1.03423176485774215294   \n",
       "2017-11-15 02:00:00  1.03423176485774215294  0.75885363374001235659   \n",
       "2017-11-15 03:00:00  0.75885363374001235659  0.39538395910800050448   \n",
       "2017-11-15 04:00:00  0.39538395910800050448  0.03596579095793609598   \n",
       "2017-11-15 05:00:00  0.03596579095793609598 -0.27111263126045964666   \n",
       "...                                     ...                     ...   \n",
       "2017-11-16 18:00:00  0.14928325703010486558  0.46338857875001654429   \n",
       "2017-11-16 19:00:00  0.46338857875001654429  0.81840715738415448222   \n",
       "2017-11-16 20:00:00  0.81840715738415448222  1.21035565823367763372   \n",
       "2017-11-16 21:00:00  1.21035565823367763372  1.56316566292614544942   \n",
       "2017-11-16 22:00:00  1.56316566292614544942  1.73394503050644011033   \n",
       "2017-11-16 23:00:00  1.73394503050644011033  1.69767879398128451740   \n",
       "2017-11-17 00:00:00  1.69767879398128451740  1.54796165933561646888   \n",
       "2017-11-17 01:00:00  1.54796165933561646888  1.32841208887390038740   \n",
       "2017-11-17 02:00:00  1.32841208887390038740  1.02916982803134482438   \n",
       "2017-11-17 03:00:00  1.02916982803134482438  0.67842741233001946366   \n",
       "2017-11-17 04:00:00  0.67842741233001946366  0.34334618915546943896   \n",
       "2017-11-17 05:00:00  0.34334618915546943896  0.05867728937758159846   \n",
       "2017-11-17 06:00:00  0.05867728937758159846 -0.17499726074043489565   \n",
       "2017-11-17 07:00:00 -0.17499726074043489565 -0.35858104817723879254   \n",
       "2017-11-17 08:00:00 -0.35858104817723879254 -0.49297765988243807955   \n",
       "2017-11-17 09:00:00 -0.49297765988243807955 -0.57909068284922038394   \n",
       "2017-11-17 10:00:00 -0.57909068284922038394 -0.61782370402718911340   \n",
       "2017-11-17 11:00:00 -0.61782370402718911340 -0.61008031039500765225   \n",
       "2017-11-17 12:00:00 -0.61008031039500765225 -0.55676408893133710887   \n",
       "2017-11-17 13:00:00 -0.55676408893133710887 -0.45877862660030960251   \n",
       "2017-11-17 14:00:00 -0.45877862660030960251 -0.31702751036605747448   \n",
       "2017-11-17 15:00:00 -0.31702751036605747448 -0.13241432720724649608   \n",
       "2017-11-17 16:00:00 -0.13241432720724649608  0.09415733591199586250   \n",
       "2017-11-17 17:00:00  0.09415733591199586250  0.36178389202753025167   \n",
       "2017-11-17 18:00:00  0.36178389202753025167  0.66956175414617868924   \n",
       "2017-11-17 19:00:00  0.66956175414617868924  1.01658733531831857455   \n",
       "2017-11-17 20:00:00  1.01658733531831857455  1.40530733235496274602   \n",
       "2017-11-17 21:00:00  1.40530733235496274602  1.74393341176094396339   \n",
       "2017-11-17 22:00:00  1.74393341176094396339  1.91467860483599472410   \n",
       "2017-11-17 23:00:00  1.91467860483599472410  1.88167235117187092719   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t+9                    t+10   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00  0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 01:00:00  0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 02:00:00  0.32615303693918301642 -0.04749514996851413040   \n",
       "2017-11-14 03:00:00 -0.04749514996851413040 -0.37568496094050468637   \n",
       "2017-11-14 04:00:00 -0.37568496094050468637 -0.64304871699996779544   \n",
       "2017-11-14 05:00:00 -0.64304871699996779544 -0.85547013537058791410   \n",
       "2017-11-14 06:00:00 -0.85547013537058791410 -1.01413601029847377788   \n",
       "2017-11-14 07:00:00 -1.01413601029847377788 -1.12023313600068541440   \n",
       "2017-11-14 08:00:00 -1.12023313600068541440 -1.17494830669427896552   \n",
       "2017-11-14 09:00:00 -1.17494830669427896552 -1.17946831659631423683   \n",
       "2017-11-14 10:00:00 -1.17946831659631423683 -1.13497995993837852424   \n",
       "2017-11-14 11:00:00 -1.13497995993837852424 -1.04267003093752497200   \n",
       "2017-11-14 12:00:00 -1.04267003093752497200 -0.90372532381081616126   \n",
       "2017-11-14 13:00:00 -0.90372532381081616126 -0.71933263277530545832   \n",
       "2017-11-14 14:00:00 -0.71933263277530545832 -0.49067875207711098007   \n",
       "2017-11-14 15:00:00 -0.49067875207711098007 -0.21895047591875707593   \n",
       "2017-11-14 16:00:00 -0.21895047591875707593  0.09466540146816218582   \n",
       "2017-11-14 17:00:00  0.09466540146816218582  0.44898208588112492601   \n",
       "2017-11-14 18:00:00  0.44898208588112492601  0.83964953951184229108   \n",
       "2017-11-14 19:00:00  0.83964953951184229108  1.18956312249132434289   \n",
       "2017-11-14 20:00:00  1.18956312249132434289  1.40530733235496274602   \n",
       "2017-11-14 21:00:00  1.40530733235496274602  1.32087715502241476173   \n",
       "2017-11-14 22:00:00  1.32087715502241476173  1.20348892381350824898   \n",
       "2017-11-14 23:00:00  1.20348892381350824898  1.03423176485774215294   \n",
       "2017-11-15 00:00:00  1.03423176485774215294  0.75885363374001235659   \n",
       "2017-11-15 01:00:00  0.75885363374001235659  0.39538395910800050448   \n",
       "2017-11-15 02:00:00  0.39538395910800050448  0.03596579095793609598   \n",
       "2017-11-15 03:00:00  0.03596579095793609598 -0.27111263126045964666   \n",
       "2017-11-15 04:00:00 -0.27111263126045964666 -0.52472937119813389817   \n",
       "2017-11-15 05:00:00 -0.52472937119813389817 -0.72581320134154281298   \n",
       "...                                     ...                     ...   \n",
       "2017-11-16 18:00:00  0.81840715738415448222  1.21035565823367763372   \n",
       "2017-11-16 19:00:00  1.21035565823367763372  1.56316566292614544942   \n",
       "2017-11-16 20:00:00  1.56316566292614544942  1.73394503050644011033   \n",
       "2017-11-16 21:00:00  1.73394503050644011033  1.69767879398128451740   \n",
       "2017-11-16 22:00:00  1.69767879398128451740  1.54796165933561646888   \n",
       "2017-11-16 23:00:00  1.54796165933561646888  1.32841208887390038740   \n",
       "2017-11-17 00:00:00  1.32841208887390038740  1.02916982803134482438   \n",
       "2017-11-17 01:00:00  1.02916982803134482438  0.67842741233001946366   \n",
       "2017-11-17 02:00:00  0.67842741233001946366  0.34334618915546943896   \n",
       "2017-11-17 03:00:00  0.34334618915546943896  0.05867728937758159846   \n",
       "2017-11-17 04:00:00  0.05867728937758159846 -0.17499726074043489565   \n",
       "2017-11-17 05:00:00 -0.17499726074043489565 -0.35858104817723879254   \n",
       "2017-11-17 06:00:00 -0.35858104817723879254 -0.49297765988243807955   \n",
       "2017-11-17 07:00:00 -0.49297765988243807955 -0.57909068284922038394   \n",
       "2017-11-17 08:00:00 -0.57909068284922038394 -0.61782370402718911340   \n",
       "2017-11-17 09:00:00 -0.61782370402718911340 -0.61008031039500765225   \n",
       "2017-11-17 10:00:00 -0.61008031039500765225 -0.55676408893133710887   \n",
       "2017-11-17 11:00:00 -0.55676408893133710887 -0.45877862660030960251   \n",
       "2017-11-17 12:00:00 -0.45877862660030960251 -0.31702751036605747448   \n",
       "2017-11-17 13:00:00 -0.31702751036605747448 -0.13241432720724649608   \n",
       "2017-11-17 14:00:00 -0.13241432720724649608  0.09415733591199586250   \n",
       "2017-11-17 15:00:00  0.09415733591199586250  0.36178389202753025167   \n",
       "2017-11-17 16:00:00  0.36178389202753025167  0.66956175414617868924   \n",
       "2017-11-17 17:00:00  0.66956175414617868924  1.01658733531831857455   \n",
       "2017-11-17 18:00:00  1.01658733531831857455  1.40530733235496274602   \n",
       "2017-11-17 19:00:00  1.40530733235496274602  1.74393341176094396339   \n",
       "2017-11-17 20:00:00  1.74393341176094396339  1.91467860483599472410   \n",
       "2017-11-17 21:00:00  1.91467860483599472410  1.88167235117187092719   \n",
       "2017-11-17 22:00:00  1.88167235117187092719  1.72401587348606399885   \n",
       "2017-11-17 23:00:00  1.72401587348606399885  1.48744543823312547204   \n",
       "\n",
       "tensor                        ...                                 X  \\\n",
       "feature                       ...                         OMEGA_dot   \n",
       "time step                     ...                               t-9   \n",
       "Epoch_Time_of_Clock           ...                                     \n",
       "2017-11-14 00:00:00           ...           -0.93065690156849911752   \n",
       "2017-11-14 01:00:00           ...           -1.10080417998033053273   \n",
       "2017-11-14 02:00:00           ...           -1.21574312915980842220   \n",
       "2017-11-14 03:00:00           ...           -1.27703416807841030334   \n",
       "2017-11-14 04:00:00           ...           -1.28623771573667489143   \n",
       "2017-11-14 05:00:00           ...           -1.24491419109156065126   \n",
       "2017-11-14 06:00:00           ...           -1.15462401312907236850   \n",
       "2017-11-14 07:00:00           ...           -1.01692760083522149017   \n",
       "2017-11-14 08:00:00           ...           -0.83338537319602012943   \n",
       "2017-11-14 09:00:00           ...           -0.60555774919747740181   \n",
       "2017-11-14 10:00:00           ...           -0.33500514781107137985   \n",
       "2017-11-14 11:00:00           ...           -0.04749514996851413040   \n",
       "2017-11-14 12:00:00           ...            0.32803331121033646456   \n",
       "2017-11-14 13:00:00           ...            0.71428778670423287434   \n",
       "2017-11-14 14:00:00           ...            1.05926195957457713881   \n",
       "2017-11-14 15:00:00           ...            1.22118133280867247059   \n",
       "2017-11-14 16:00:00           ...            1.19273531193175696963   \n",
       "2017-11-14 17:00:00           ...            1.09942999377016903395   \n",
       "2017-11-14 18:00:00           ...            0.96332459895237088077   \n",
       "2017-11-14 19:00:00           ...            0.70140826416528545550   \n",
       "2017-11-14 20:00:00           ...            0.32615303693918301642   \n",
       "2017-11-14 21:00:00           ...           -0.04749514996851413040   \n",
       "2017-11-14 22:00:00           ...           -0.37568496094050468637   \n",
       "2017-11-14 23:00:00           ...           -0.64304871699996779544   \n",
       "2017-11-15 00:00:00           ...           -0.85547013537058791410   \n",
       "2017-11-15 01:00:00           ...           -1.01413601029847377788   \n",
       "2017-11-15 02:00:00           ...           -1.12023313600068541440   \n",
       "2017-11-15 03:00:00           ...           -1.17494830669427896552   \n",
       "2017-11-15 04:00:00           ...           -1.17946831659631423683   \n",
       "2017-11-15 05:00:00           ...           -1.13497995993837852424   \n",
       "...                           ...                               ...   \n",
       "2017-11-16 18:00:00           ...            1.16637650457455577246   \n",
       "2017-11-16 19:00:00           ...            0.87791323568086254703   \n",
       "2017-11-16 20:00:00           ...            0.52151834541413533142   \n",
       "2017-11-16 21:00:00           ...            0.17540672701975912329   \n",
       "2017-11-16 22:00:00           ...           -0.04749514996851413040   \n",
       "2017-11-16 23:00:00           ...           -0.36272895390056852927   \n",
       "2017-11-17 00:00:00           ...           -0.55471669787537580820   \n",
       "2017-11-17 01:00:00           ...           -0.69644997832958432937   \n",
       "2017-11-17 02:00:00           ...           -0.78877799585452845754   \n",
       "2017-11-17 03:00:00           ...           -0.83254995105606888206   \n",
       "2017-11-17 04:00:00           ...           -0.82861504452554479716   \n",
       "2017-11-17 05:00:00           ...           -0.77782247689786765399   \n",
       "2017-11-17 06:00:00           ...           -0.68102144873532100000   \n",
       "2017-11-17 07:00:00           ...           -0.53906116065829468020   \n",
       "2017-11-17 08:00:00           ...           -0.35279081327264949497   \n",
       "2017-11-17 09:00:00           ...           -0.12305960718424864553   \n",
       "2017-11-17 10:00:00           ...            0.14928325703010486558   \n",
       "2017-11-17 11:00:00           ...            0.46338857875001654429   \n",
       "2017-11-17 12:00:00           ...            0.81840715738415448222   \n",
       "2017-11-17 13:00:00           ...            1.21035565823367763372   \n",
       "2017-11-17 14:00:00           ...            1.56316566292614544942   \n",
       "2017-11-17 15:00:00           ...            1.73394503050644011033   \n",
       "2017-11-17 16:00:00           ...            1.69767879398128451740   \n",
       "2017-11-17 17:00:00           ...            1.54796165933561646888   \n",
       "2017-11-17 18:00:00           ...            1.32841208887390038740   \n",
       "2017-11-17 19:00:00           ...            1.02916982803134482438   \n",
       "2017-11-17 20:00:00           ...            0.67842741233001946366   \n",
       "2017-11-17 21:00:00           ...            0.34334618915546943896   \n",
       "2017-11-17 22:00:00           ...            0.05867728937758159846   \n",
       "2017-11-17 23:00:00           ...           -0.17499726074043489565   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-8                     t-7   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00 -1.10080417998033053273 -1.21574312915980842220   \n",
       "2017-11-14 01:00:00 -1.21574312915980842220 -1.27703416807841030334   \n",
       "2017-11-14 02:00:00 -1.27703416807841030334 -1.28623771573667489143   \n",
       "2017-11-14 03:00:00 -1.28623771573667489143 -1.24491419109156065126   \n",
       "2017-11-14 04:00:00 -1.24491419109156065126 -1.15462401312907236850   \n",
       "2017-11-14 05:00:00 -1.15462401312907236850 -1.01692760083522149017   \n",
       "2017-11-14 06:00:00 -1.01692760083522149017 -0.83338537319602012943   \n",
       "2017-11-14 07:00:00 -0.83338537319602012943 -0.60555774919747740181   \n",
       "2017-11-14 08:00:00 -0.60555774919747740181 -0.33500514781107137985   \n",
       "2017-11-14 09:00:00 -0.33500514781107137985 -0.04749514996851413040   \n",
       "2017-11-14 10:00:00 -0.04749514996851413040  0.32803331121033646456   \n",
       "2017-11-14 11:00:00  0.32803331121033646456  0.71428778670423287434   \n",
       "2017-11-14 12:00:00  0.71428778670423287434  1.05926195957457713881   \n",
       "2017-11-14 13:00:00  1.05926195957457713881  1.22118133280867247059   \n",
       "2017-11-14 14:00:00  1.22118133280867247059  1.19273531193175696963   \n",
       "2017-11-14 15:00:00  1.19273531193175696963  1.09942999377016903395   \n",
       "2017-11-14 16:00:00  1.09942999377016903395  0.96332459895237088077   \n",
       "2017-11-14 17:00:00  0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 18:00:00  0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 19:00:00  0.32615303693918301642 -0.04749514996851413040   \n",
       "2017-11-14 20:00:00 -0.04749514996851413040 -0.37568496094050468637   \n",
       "2017-11-14 21:00:00 -0.37568496094050468637 -0.64304871699996779544   \n",
       "2017-11-14 22:00:00 -0.64304871699996779544 -0.85547013537058791410   \n",
       "2017-11-14 23:00:00 -0.85547013537058791410 -1.01413601029847377788   \n",
       "2017-11-15 00:00:00 -1.01413601029847377788 -1.12023313600068541440   \n",
       "2017-11-15 01:00:00 -1.12023313600068541440 -1.17494830669427896552   \n",
       "2017-11-15 02:00:00 -1.17494830669427896552 -1.17946831659631423683   \n",
       "2017-11-15 03:00:00 -1.17946831659631423683 -1.13497995993837852424   \n",
       "2017-11-15 04:00:00 -1.13497995993837852424 -1.04267003093752497200   \n",
       "2017-11-15 05:00:00 -1.04267003093752497200 -0.90372532381081616126   \n",
       "...                                     ...                     ...   \n",
       "2017-11-16 18:00:00  0.87791323568086254703  0.52151834541413533142   \n",
       "2017-11-16 19:00:00  0.52151834541413533142  0.17540672701975912329   \n",
       "2017-11-16 20:00:00  0.17540672701975912329 -0.04749514996851413040   \n",
       "2017-11-16 21:00:00 -0.04749514996851413040 -0.36272895390056852927   \n",
       "2017-11-16 22:00:00 -0.36272895390056852927 -0.55471669787537580820   \n",
       "2017-11-16 23:00:00 -0.55471669787537580820 -0.69644997832958432937   \n",
       "2017-11-17 00:00:00 -0.69644997832958432937 -0.78877799585452845754   \n",
       "2017-11-17 01:00:00 -0.78877799585452845754 -0.83254995105606888206   \n",
       "2017-11-17 02:00:00 -0.83254995105606888206 -0.82861504452554479716   \n",
       "2017-11-17 03:00:00 -0.82861504452554479716 -0.77782247689786765399   \n",
       "2017-11-17 04:00:00 -0.77782247689786765399 -0.68102144873532100000   \n",
       "2017-11-17 05:00:00 -0.68102144873532100000 -0.53906116065829468020   \n",
       "2017-11-17 06:00:00 -0.53906116065829468020 -0.35279081327264949497   \n",
       "2017-11-17 07:00:00 -0.35279081327264949497 -0.12305960718424864553   \n",
       "2017-11-17 08:00:00 -0.12305960718424864553  0.14928325703010486558   \n",
       "2017-11-17 09:00:00  0.14928325703010486558  0.46338857875001654429   \n",
       "2017-11-17 10:00:00  0.46338857875001654429  0.81840715738415448222   \n",
       "2017-11-17 11:00:00  0.81840715738415448222  1.21035565823367763372   \n",
       "2017-11-17 12:00:00  1.21035565823367763372  1.56316566292614544942   \n",
       "2017-11-17 13:00:00  1.56316566292614544942  1.73394503050644011033   \n",
       "2017-11-17 14:00:00  1.73394503050644011033  1.69767879398128451740   \n",
       "2017-11-17 15:00:00  1.69767879398128451740  1.54796165933561646888   \n",
       "2017-11-17 16:00:00  1.54796165933561646888  1.32841208887390038740   \n",
       "2017-11-17 17:00:00  1.32841208887390038740  1.02916982803134482438   \n",
       "2017-11-17 18:00:00  1.02916982803134482438  0.67842741233001946366   \n",
       "2017-11-17 19:00:00  0.67842741233001946366  0.34334618915546943896   \n",
       "2017-11-17 20:00:00  0.34334618915546943896  0.05867728937758159846   \n",
       "2017-11-17 21:00:00  0.05867728937758159846 -0.17499726074043489565   \n",
       "2017-11-17 22:00:00 -0.17499726074043489565 -0.35858104817723879254   \n",
       "2017-11-17 23:00:00 -0.35858104817723879254 -0.49297765988243807955   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-6                     t-5   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00 -1.27703416807841030334 -1.28623771573667489143   \n",
       "2017-11-14 01:00:00 -1.28623771573667489143 -1.24491419109156065126   \n",
       "2017-11-14 02:00:00 -1.24491419109156065126 -1.15462401312907236850   \n",
       "2017-11-14 03:00:00 -1.15462401312907236850 -1.01692760083522149017   \n",
       "2017-11-14 04:00:00 -1.01692760083522149017 -0.83338537319602012943   \n",
       "2017-11-14 05:00:00 -0.83338537319602012943 -0.60555774919747740181   \n",
       "2017-11-14 06:00:00 -0.60555774919747740181 -0.33500514781107137985   \n",
       "2017-11-14 07:00:00 -0.33500514781107137985 -0.04749514996851413040   \n",
       "2017-11-14 08:00:00 -0.04749514996851413040  0.32803331121033646456   \n",
       "2017-11-14 09:00:00  0.32803331121033646456  0.71428778670423287434   \n",
       "2017-11-14 10:00:00  0.71428778670423287434  1.05926195957457713881   \n",
       "2017-11-14 11:00:00  1.05926195957457713881  1.22118133280867247059   \n",
       "2017-11-14 12:00:00  1.22118133280867247059  1.19273531193175696963   \n",
       "2017-11-14 13:00:00  1.19273531193175696963  1.09942999377016903395   \n",
       "2017-11-14 14:00:00  1.09942999377016903395  0.96332459895237088077   \n",
       "2017-11-14 15:00:00  0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 16:00:00  0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 17:00:00  0.32615303693918301642 -0.04749514996851413040   \n",
       "2017-11-14 18:00:00 -0.04749514996851413040 -0.37568496094050468637   \n",
       "2017-11-14 19:00:00 -0.37568496094050468637 -0.64304871699996779544   \n",
       "2017-11-14 20:00:00 -0.64304871699996779544 -0.85547013537058791410   \n",
       "2017-11-14 21:00:00 -0.85547013537058791410 -1.01413601029847377788   \n",
       "2017-11-14 22:00:00 -1.01413601029847377788 -1.12023313600068541440   \n",
       "2017-11-14 23:00:00 -1.12023313600068541440 -1.17494830669427896552   \n",
       "2017-11-15 00:00:00 -1.17494830669427896552 -1.17946831659631423683   \n",
       "2017-11-15 01:00:00 -1.17946831659631423683 -1.13497995993837852424   \n",
       "2017-11-15 02:00:00 -1.13497995993837852424 -1.04267003093752497200   \n",
       "2017-11-15 03:00:00 -1.04267003093752497200 -0.90372532381081616126   \n",
       "2017-11-15 04:00:00 -0.90372532381081616126 -0.71933263277530545832   \n",
       "2017-11-15 05:00:00 -0.71933263277530545832 -0.49067875207711098007   \n",
       "...                                     ...                     ...   \n",
       "2017-11-16 18:00:00  0.17540672701975912329 -0.04749514996851413040   \n",
       "2017-11-16 19:00:00 -0.04749514996851413040 -0.36272895390056852927   \n",
       "2017-11-16 20:00:00 -0.36272895390056852927 -0.55471669787537580820   \n",
       "2017-11-16 21:00:00 -0.55471669787537580820 -0.69644997832958432937   \n",
       "2017-11-16 22:00:00 -0.69644997832958432937 -0.78877799585452845754   \n",
       "2017-11-16 23:00:00 -0.78877799585452845754 -0.83254995105606888206   \n",
       "2017-11-17 00:00:00 -0.83254995105606888206 -0.82861504452554479716   \n",
       "2017-11-17 01:00:00 -0.82861504452554479716 -0.77782247689786765399   \n",
       "2017-11-17 02:00:00 -0.77782247689786765399 -0.68102144873532100000   \n",
       "2017-11-17 03:00:00 -0.68102144873532100000 -0.53906116065829468020   \n",
       "2017-11-17 04:00:00 -0.53906116065829468020 -0.35279081327264949497   \n",
       "2017-11-17 05:00:00 -0.35279081327264949497 -0.12305960718424864553   \n",
       "2017-11-17 06:00:00 -0.12305960718424864553  0.14928325703010486558   \n",
       "2017-11-17 07:00:00  0.14928325703010486558  0.46338857875001654429   \n",
       "2017-11-17 08:00:00  0.46338857875001654429  0.81840715738415448222   \n",
       "2017-11-17 09:00:00  0.81840715738415448222  1.21035565823367763372   \n",
       "2017-11-17 10:00:00  1.21035565823367763372  1.56316566292614544942   \n",
       "2017-11-17 11:00:00  1.56316566292614544942  1.73394503050644011033   \n",
       "2017-11-17 12:00:00  1.73394503050644011033  1.69767879398128451740   \n",
       "2017-11-17 13:00:00  1.69767879398128451740  1.54796165933561646888   \n",
       "2017-11-17 14:00:00  1.54796165933561646888  1.32841208887390038740   \n",
       "2017-11-17 15:00:00  1.32841208887390038740  1.02916982803134482438   \n",
       "2017-11-17 16:00:00  1.02916982803134482438  0.67842741233001946366   \n",
       "2017-11-17 17:00:00  0.67842741233001946366  0.34334618915546943896   \n",
       "2017-11-17 18:00:00  0.34334618915546943896  0.05867728937758159846   \n",
       "2017-11-17 19:00:00  0.05867728937758159846 -0.17499726074043489565   \n",
       "2017-11-17 20:00:00 -0.17499726074043489565 -0.35858104817723879254   \n",
       "2017-11-17 21:00:00 -0.35858104817723879254 -0.49297765988243807955   \n",
       "2017-11-17 22:00:00 -0.49297765988243807955 -0.57909068284922038394   \n",
       "2017-11-17 23:00:00 -0.57909068284922038394 -0.61782370402718911340   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-4                     t-3   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00 -1.24491419109156065126 -1.15462401312907236850   \n",
       "2017-11-14 01:00:00 -1.15462401312907236850 -1.01692760083522149017   \n",
       "2017-11-14 02:00:00 -1.01692760083522149017 -0.83338537319602012943   \n",
       "2017-11-14 03:00:00 -0.83338537319602012943 -0.60555774919747740181   \n",
       "2017-11-14 04:00:00 -0.60555774919747740181 -0.33500514781107137985   \n",
       "2017-11-14 05:00:00 -0.33500514781107137985 -0.04749514996851413040   \n",
       "2017-11-14 06:00:00 -0.04749514996851413040  0.32803331121033646456   \n",
       "2017-11-14 07:00:00  0.32803331121033646456  0.71428778670423287434   \n",
       "2017-11-14 08:00:00  0.71428778670423287434  1.05926195957457713881   \n",
       "2017-11-14 09:00:00  1.05926195957457713881  1.22118133280867247059   \n",
       "2017-11-14 10:00:00  1.22118133280867247059  1.19273531193175696963   \n",
       "2017-11-14 11:00:00  1.19273531193175696963  1.09942999377016903395   \n",
       "2017-11-14 12:00:00  1.09942999377016903395  0.96332459895237088077   \n",
       "2017-11-14 13:00:00  0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 14:00:00  0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 15:00:00  0.32615303693918301642 -0.04749514996851413040   \n",
       "2017-11-14 16:00:00 -0.04749514996851413040 -0.37568496094050468637   \n",
       "2017-11-14 17:00:00 -0.37568496094050468637 -0.64304871699996779544   \n",
       "2017-11-14 18:00:00 -0.64304871699996779544 -0.85547013537058791410   \n",
       "2017-11-14 19:00:00 -0.85547013537058791410 -1.01413601029847377788   \n",
       "2017-11-14 20:00:00 -1.01413601029847377788 -1.12023313600068541440   \n",
       "2017-11-14 21:00:00 -1.12023313600068541440 -1.17494830669427896552   \n",
       "2017-11-14 22:00:00 -1.17494830669427896552 -1.17946831659631423683   \n",
       "2017-11-14 23:00:00 -1.17946831659631423683 -1.13497995993837852424   \n",
       "2017-11-15 00:00:00 -1.13497995993837852424 -1.04267003093752497200   \n",
       "2017-11-15 01:00:00 -1.04267003093752497200 -0.90372532381081616126   \n",
       "2017-11-15 02:00:00 -0.90372532381081616126 -0.71933263277530545832   \n",
       "2017-11-15 03:00:00 -0.71933263277530545832 -0.49067875207711098007   \n",
       "2017-11-15 04:00:00 -0.49067875207711098007 -0.21895047591875707593   \n",
       "2017-11-15 05:00:00 -0.21895047591875707593  0.09466540146816218582   \n",
       "...                                     ...                     ...   \n",
       "2017-11-16 18:00:00 -0.36272895390056852927 -0.55471669787537580820   \n",
       "2017-11-16 19:00:00 -0.55471669787537580820 -0.69644997832958432937   \n",
       "2017-11-16 20:00:00 -0.69644997832958432937 -0.78877799585452845754   \n",
       "2017-11-16 21:00:00 -0.78877799585452845754 -0.83254995105606888206   \n",
       "2017-11-16 22:00:00 -0.83254995105606888206 -0.82861504452554479716   \n",
       "2017-11-16 23:00:00 -0.82861504452554479716 -0.77782247689786765399   \n",
       "2017-11-17 00:00:00 -0.77782247689786765399 -0.68102144873532100000   \n",
       "2017-11-17 01:00:00 -0.68102144873532100000 -0.53906116065829468020   \n",
       "2017-11-17 02:00:00 -0.53906116065829468020 -0.35279081327264949497   \n",
       "2017-11-17 03:00:00 -0.35279081327264949497 -0.12305960718424864553   \n",
       "2017-11-17 04:00:00 -0.12305960718424864553  0.14928325703010486558   \n",
       "2017-11-17 05:00:00  0.14928325703010486558  0.46338857875001654429   \n",
       "2017-11-17 06:00:00  0.46338857875001654429  0.81840715738415448222   \n",
       "2017-11-17 07:00:00  0.81840715738415448222  1.21035565823367763372   \n",
       "2017-11-17 08:00:00  1.21035565823367763372  1.56316566292614544942   \n",
       "2017-11-17 09:00:00  1.56316566292614544942  1.73394503050644011033   \n",
       "2017-11-17 10:00:00  1.73394503050644011033  1.69767879398128451740   \n",
       "2017-11-17 11:00:00  1.69767879398128451740  1.54796165933561646888   \n",
       "2017-11-17 12:00:00  1.54796165933561646888  1.32841208887390038740   \n",
       "2017-11-17 13:00:00  1.32841208887390038740  1.02916982803134482438   \n",
       "2017-11-17 14:00:00  1.02916982803134482438  0.67842741233001946366   \n",
       "2017-11-17 15:00:00  0.67842741233001946366  0.34334618915546943896   \n",
       "2017-11-17 16:00:00  0.34334618915546943896  0.05867728937758159846   \n",
       "2017-11-17 17:00:00  0.05867728937758159846 -0.17499726074043489565   \n",
       "2017-11-17 18:00:00 -0.17499726074043489565 -0.35858104817723879254   \n",
       "2017-11-17 19:00:00 -0.35858104817723879254 -0.49297765988243807955   \n",
       "2017-11-17 20:00:00 -0.49297765988243807955 -0.57909068284922038394   \n",
       "2017-11-17 21:00:00 -0.57909068284922038394 -0.61782370402718911340   \n",
       "2017-11-17 22:00:00 -0.61782370402718911340 -0.61008031039500765225   \n",
       "2017-11-17 23:00:00 -0.61008031039500765225 -0.55676408893133710887   \n",
       "\n",
       "tensor                                                               \\\n",
       "feature                                                               \n",
       "time step                               t-2                     t-1   \n",
       "Epoch_Time_of_Clock                                                   \n",
       "2017-11-14 00:00:00 -1.01692760083522149017 -0.83338537319602012943   \n",
       "2017-11-14 01:00:00 -0.83338537319602012943 -0.60555774919747740181   \n",
       "2017-11-14 02:00:00 -0.60555774919747740181 -0.33500514781107137985   \n",
       "2017-11-14 03:00:00 -0.33500514781107137985 -0.04749514996851413040   \n",
       "2017-11-14 04:00:00 -0.04749514996851413040  0.32803331121033646456   \n",
       "2017-11-14 05:00:00  0.32803331121033646456  0.71428778670423287434   \n",
       "2017-11-14 06:00:00  0.71428778670423287434  1.05926195957457713881   \n",
       "2017-11-14 07:00:00  1.05926195957457713881  1.22118133280867247059   \n",
       "2017-11-14 08:00:00  1.22118133280867247059  1.19273531193175696963   \n",
       "2017-11-14 09:00:00  1.19273531193175696963  1.09942999377016903395   \n",
       "2017-11-14 10:00:00  1.09942999377016903395  0.96332459895237088077   \n",
       "2017-11-14 11:00:00  0.96332459895237088077  0.70140826416528545550   \n",
       "2017-11-14 12:00:00  0.70140826416528545550  0.32615303693918301642   \n",
       "2017-11-14 13:00:00  0.32615303693918301642 -0.04749514996851413040   \n",
       "2017-11-14 14:00:00 -0.04749514996851413040 -0.37568496094050468637   \n",
       "2017-11-14 15:00:00 -0.37568496094050468637 -0.64304871699996779544   \n",
       "2017-11-14 16:00:00 -0.64304871699996779544 -0.85547013537058791410   \n",
       "2017-11-14 17:00:00 -0.85547013537058791410 -1.01413601029847377788   \n",
       "2017-11-14 18:00:00 -1.01413601029847377788 -1.12023313600068541440   \n",
       "2017-11-14 19:00:00 -1.12023313600068541440 -1.17494830669427896552   \n",
       "2017-11-14 20:00:00 -1.17494830669427896552 -1.17946831659631423683   \n",
       "2017-11-14 21:00:00 -1.17946831659631423683 -1.13497995993837852424   \n",
       "2017-11-14 22:00:00 -1.13497995993837852424 -1.04267003093752497200   \n",
       "2017-11-14 23:00:00 -1.04267003093752497200 -0.90372532381081616126   \n",
       "2017-11-15 00:00:00 -0.90372532381081616126 -0.71933263277530545832   \n",
       "2017-11-15 01:00:00 -0.71933263277530545832 -0.49067875207711098007   \n",
       "2017-11-15 02:00:00 -0.49067875207711098007 -0.21895047591875707593   \n",
       "2017-11-15 03:00:00 -0.21895047591875707593  0.09466540146816218582   \n",
       "2017-11-15 04:00:00  0.09466540146816218582  0.44898208588112492601   \n",
       "2017-11-15 05:00:00  0.44898208588112492601  0.83964953951184229108   \n",
       "...                                     ...                     ...   \n",
       "2017-11-16 18:00:00 -0.69644997832958432937 -0.78877799585452845754   \n",
       "2017-11-16 19:00:00 -0.78877799585452845754 -0.83254995105606888206   \n",
       "2017-11-16 20:00:00 -0.83254995105606888206 -0.82861504452554479716   \n",
       "2017-11-16 21:00:00 -0.82861504452554479716 -0.77782247689786765399   \n",
       "2017-11-16 22:00:00 -0.77782247689786765399 -0.68102144873532100000   \n",
       "2017-11-16 23:00:00 -0.68102144873532100000 -0.53906116065829468020   \n",
       "2017-11-17 00:00:00 -0.53906116065829468020 -0.35279081327264949497   \n",
       "2017-11-17 01:00:00 -0.35279081327264949497 -0.12305960718424864553   \n",
       "2017-11-17 02:00:00 -0.12305960718424864553  0.14928325703010486558   \n",
       "2017-11-17 03:00:00  0.14928325703010486558  0.46338857875001654429   \n",
       "2017-11-17 04:00:00  0.46338857875001654429  0.81840715738415448222   \n",
       "2017-11-17 05:00:00  0.81840715738415448222  1.21035565823367763372   \n",
       "2017-11-17 06:00:00  1.21035565823367763372  1.56316566292614544942   \n",
       "2017-11-17 07:00:00  1.56316566292614544942  1.73394503050644011033   \n",
       "2017-11-17 08:00:00  1.73394503050644011033  1.69767879398128451740   \n",
       "2017-11-17 09:00:00  1.69767879398128451740  1.54796165933561646888   \n",
       "2017-11-17 10:00:00  1.54796165933561646888  1.32841208887390038740   \n",
       "2017-11-17 11:00:00  1.32841208887390038740  1.02916982803134482438   \n",
       "2017-11-17 12:00:00  1.02916982803134482438  0.67842741233001946366   \n",
       "2017-11-17 13:00:00  0.67842741233001946366  0.34334618915546943896   \n",
       "2017-11-17 14:00:00  0.34334618915546943896  0.05867728937758159846   \n",
       "2017-11-17 15:00:00  0.05867728937758159846 -0.17499726074043489565   \n",
       "2017-11-17 16:00:00 -0.17499726074043489565 -0.35858104817723879254   \n",
       "2017-11-17 17:00:00 -0.35858104817723879254 -0.49297765988243807955   \n",
       "2017-11-17 18:00:00 -0.49297765988243807955 -0.57909068284922038394   \n",
       "2017-11-17 19:00:00 -0.57909068284922038394 -0.61782370402718911340   \n",
       "2017-11-17 20:00:00 -0.61782370402718911340 -0.61008031039500765225   \n",
       "2017-11-17 21:00:00 -0.61008031039500765225 -0.55676408893133710887   \n",
       "2017-11-17 22:00:00 -0.55676408893133710887 -0.45877862660030960251   \n",
       "2017-11-17 23:00:00 -0.45877862660030960251 -0.31702751036605747448   \n",
       "\n",
       "tensor                                       \n",
       "feature                                      \n",
       "time step                                 t  \n",
       "Epoch_Time_of_Clock                          \n",
       "2017-11-14 00:00:00 -0.60555774919747740181  \n",
       "2017-11-14 01:00:00 -0.33500514781107137985  \n",
       "2017-11-14 02:00:00 -0.04749514996851413040  \n",
       "2017-11-14 03:00:00  0.32803331121033646456  \n",
       "2017-11-14 04:00:00  0.71428778670423287434  \n",
       "2017-11-14 05:00:00  1.05926195957457713881  \n",
       "2017-11-14 06:00:00  1.22118133280867247059  \n",
       "2017-11-14 07:00:00  1.19273531193175696963  \n",
       "2017-11-14 08:00:00  1.09942999377016903395  \n",
       "2017-11-14 09:00:00  0.96332459895237088077  \n",
       "2017-11-14 10:00:00  0.70140826416528545550  \n",
       "2017-11-14 11:00:00  0.32615303693918301642  \n",
       "2017-11-14 12:00:00 -0.04749514996851413040  \n",
       "2017-11-14 13:00:00 -0.37568496094050468637  \n",
       "2017-11-14 14:00:00 -0.64304871699996779544  \n",
       "2017-11-14 15:00:00 -0.85547013537058791410  \n",
       "2017-11-14 16:00:00 -1.01413601029847377788  \n",
       "2017-11-14 17:00:00 -1.12023313600068541440  \n",
       "2017-11-14 18:00:00 -1.17494830669427896552  \n",
       "2017-11-14 19:00:00 -1.17946831659631423683  \n",
       "2017-11-14 20:00:00 -1.13497995993837852424  \n",
       "2017-11-14 21:00:00 -1.04267003093752497200  \n",
       "2017-11-14 22:00:00 -0.90372532381081616126  \n",
       "2017-11-14 23:00:00 -0.71933263277530545832  \n",
       "2017-11-15 00:00:00 -0.49067875207711098007  \n",
       "2017-11-15 01:00:00 -0.21895047591875707593  \n",
       "2017-11-15 02:00:00  0.09466540146816218582  \n",
       "2017-11-15 03:00:00  0.44898208588112492601  \n",
       "2017-11-15 04:00:00  0.83964953951184229108  \n",
       "2017-11-15 05:00:00  1.18956312249132434289  \n",
       "...                                     ...  \n",
       "2017-11-16 18:00:00 -0.83254995105606888206  \n",
       "2017-11-16 19:00:00 -0.82861504452554479716  \n",
       "2017-11-16 20:00:00 -0.77782247689786765399  \n",
       "2017-11-16 21:00:00 -0.68102144873532100000  \n",
       "2017-11-16 22:00:00 -0.53906116065829468020  \n",
       "2017-11-16 23:00:00 -0.35279081327264949497  \n",
       "2017-11-17 00:00:00 -0.12305960718424864553  \n",
       "2017-11-17 01:00:00  0.14928325703010486558  \n",
       "2017-11-17 02:00:00  0.46338857875001654429  \n",
       "2017-11-17 03:00:00  0.81840715738415448222  \n",
       "2017-11-17 04:00:00  1.21035565823367763372  \n",
       "2017-11-17 05:00:00  1.56316566292614544942  \n",
       "2017-11-17 06:00:00  1.73394503050644011033  \n",
       "2017-11-17 07:00:00  1.69767879398128451740  \n",
       "2017-11-17 08:00:00  1.54796165933561646888  \n",
       "2017-11-17 09:00:00  1.32841208887390038740  \n",
       "2017-11-17 10:00:00  1.02916982803134482438  \n",
       "2017-11-17 11:00:00  0.67842741233001946366  \n",
       "2017-11-17 12:00:00  0.34334618915546943896  \n",
       "2017-11-17 13:00:00  0.05867728937758159846  \n",
       "2017-11-17 14:00:00 -0.17499726074043489565  \n",
       "2017-11-17 15:00:00 -0.35858104817723879254  \n",
       "2017-11-17 16:00:00 -0.49297765988243807955  \n",
       "2017-11-17 17:00:00 -0.57909068284922038394  \n",
       "2017-11-17 18:00:00 -0.61782370402718911340  \n",
       "2017-11-17 19:00:00 -0.61008031039500765225  \n",
       "2017-11-17 20:00:00 -0.55676408893133710887  \n",
       "2017-11-17 21:00:00 -0.45877862660030960251  \n",
       "2017-11-17 22:00:00 -0.31702751036605747448  \n",
       "2017-11-17 23:00:00 -0.13241432720724649608  \n",
       "\n",
       "[96 rows x 48 columns]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 48)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs.dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8900159 , -0.30699697,  0.0718559 , ...,  0.96953815,\n",
       "         0.9494998 ,  0.9306598 ],\n",
       "       [-0.59082997, -0.02457769,  0.34801617, ...,  0.6653469 ,\n",
       "         0.63376516,  0.60358256],\n",
       "       [-0.28638306,  0.20970134,  0.52282906, ...,  0.2231604 ,\n",
       "         0.1893303 ,  0.15928854],\n",
       "       ...,\n",
       "       [-0.16521913, -0.05440329,  0.16540731, ..., -0.9801845 ,\n",
       "        -0.8864463 , -0.728399  ],\n",
       "       [ 0.05422975,  0.27898765,  0.595263  , ..., -0.82080674,\n",
       "        -0.6878603 , -0.5011163 ],\n",
       "       [ 0.28025535,  0.62125456,  1.026844  , ..., -0.6488487 ,\n",
       "        -0.47628677, -0.26239297]], dtype=float32)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp    h              prediction                  actual\n",
      "0 2017-11-14 00:00:00  t+1 -0.00000000957992793956 -0.00000000919790026610\n",
      "1 2017-11-14 01:00:00  t+1 -0.00000000937399083846 -0.00000000900000000000\n",
      "2 2017-11-14 02:00:00  t+1 -0.00000000916443247829 -0.00000000874151444140\n",
      "3 2017-11-14 03:00:00  t+1 -0.00000000896360320994 -0.00000000847564590098\n",
      "4 2017-11-14 04:00:00  t+1 -0.00000000879226256149 -0.00000000823819161723\n",
      "               timestamp     h              prediction                  actual\n",
      "2299 2017-11-17 19:00:00  t+24 -0.00000000965855239553 -0.00000000923309127418\n",
      "2300 2017-11-17 20:00:00  t+24 -0.00000000960720341822 -0.00000000919509634570\n",
      "2301 2017-11-17 21:00:00  t+24 -0.00000000946868300201 -0.00000000912768453682\n",
      "2302 2017-11-17 22:00:00  t+24 -0.00000000931223867755 -0.00000000900000000000\n",
      "2303 2017-11-17 23:00:00  t+24 -0.00000000914791950082 -0.00000000890752722203\n",
      "(2304, 4)\n"
     ]
    }
   ],
   "source": [
    "eval_df = create_evaluation_df(predictions, test_inputs, HORIZON, y_scalar)\n",
    "print(eval_df.head())\n",
    "print(eval_df.tail())\n",
    "print(eval_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h\n",
       "t+1    -0.02852753370615188078\n",
       "t+10   -0.02742525664654022963\n",
       "t+11   -0.03163749332700745848\n",
       "t+12   -0.03619501156316016349\n",
       "t+13   -0.04095404414083867600\n",
       "t+14   -0.04580070491347552203\n",
       "t+15   -0.05135903415852629411\n",
       "t+16   -0.05701608398570434666\n",
       "t+17   -0.06300830973545822056\n",
       "t+18   -0.06877630737166091912\n",
       "t+19   -0.07412036885807517239\n",
       "t+2    -0.02567906035819169860\n",
       "t+20   -0.07903654954987580916\n",
       "t+21   -0.08287281421544202831\n",
       "t+22   -0.08481329403656585886\n",
       "t+23   -0.08596366223023872710\n",
       "t+24   -0.08481750997114706481\n",
       "t+3    -0.02372710050985196717\n",
       "t+4    -0.02086262912490459420\n",
       "t+5    -0.01844046604845235235\n",
       "t+6    -0.01743768462792127938\n",
       "t+7    -0.01794164320858453524\n",
       "t+8    -0.02008503396798941645\n",
       "t+9    -0.02375487721713289910\n",
       "Name: APE, dtype: float64"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual']\n",
    "eval_df.groupby('h')['APE'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.100594421762091e-10"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(eval_df['prediction'], eval_df['actual'])\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "a = mean_absolute_error(eval_df['prediction'], eval_df['actual'])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot actuals vs predictions at each horizon for first week of the test period. As is to be expected, predictions for one step ahead (*t+1*) are more accurate than those for 2 or 3 steps ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12f791b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df = eval_df[(eval_df.h=='t+1')][['timestamp', 'actual']]\n",
    "for t in range(1, HORIZON+1):\n",
    "    plot_df['t+'+str(t)] = eval_df[ (eval_df.h=='t+'+str(t))]['prediction'].values\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.plot(plot_df['timestamp'], plot_df['actual'], color='red', linewidth=4.0)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(plot_df['timestamp'], plot_df['t+1'], color='blue', linewidth=4.0, alpha=0.75)\n",
    "#ax.plot(plot_df['timestamp'], plot_df['t+2'], color='blue', linewidth=3.0, alpha=0.5)\n",
    "#ax.plot(plot_df['timestamp'], plot_df['t+3'], color='blue', linewidth=2.0, alpha=0.25)\n",
    "plt.xlabel('timestamp', fontsize=12)\n",
    "plt.ylabel('load', fontsize=12)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take input here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = df.iloc[456:480  , :]\n",
    "Actual = df.iloc[480:504,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [Paras[var_name]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key , value in enumerate(columns):\n",
    "    new_df[value] = a[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.dropna( how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OMEGA_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-21 00:00:00</th>\n",
       "      <td>-0.00000000851895451729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 01:00:00</th>\n",
       "      <td>-0.00000000836287991553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 02:00:00</th>\n",
       "      <td>-0.00000000818642722636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 03:00:00</th>\n",
       "      <td>-0.00000000800000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 04:00:00</th>\n",
       "      <td>-0.00000000777776760905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 05:00:00</th>\n",
       "      <td>-0.00000000758390547225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 06:00:00</th>\n",
       "      <td>-0.00000000747674242305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 07:00:00</th>\n",
       "      <td>-0.00000000748801923454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 08:00:00</th>\n",
       "      <td>-0.00000000761091060503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 09:00:00</th>\n",
       "      <td>-0.00000000781165426851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 10:00:00</th>\n",
       "      <td>-0.00000000800000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 11:00:00</th>\n",
       "      <td>-0.00000000823743187372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 12:00:00</th>\n",
       "      <td>-0.00000000841629087807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 13:00:00</th>\n",
       "      <td>-0.00000000856423961712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 14:00:00</th>\n",
       "      <td>-0.00000000868211486023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 15:00:00</th>\n",
       "      <td>-0.00000000877098795925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 16:00:00</th>\n",
       "      <td>-0.00000000883193026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 17:00:00</th>\n",
       "      <td>-0.00000000886601313235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 18:00:00</th>\n",
       "      <td>-0.00000000887430791012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 19:00:00</th>\n",
       "      <td>-0.00000000885788595117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 20:00:00</th>\n",
       "      <td>-0.00000000881781860732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 21:00:00</th>\n",
       "      <td>-0.00000000875517723043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 22:00:00</th>\n",
       "      <td>-0.00000000867103317233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21 23:00:00</th>\n",
       "      <td>-0.00000000856645778487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  OMEGA_dot\n",
       "Epoch_Time_of_Clock                        \n",
       "2017-11-21 00:00:00 -0.00000000851895451729\n",
       "2017-11-21 01:00:00 -0.00000000836287991553\n",
       "2017-11-21 02:00:00 -0.00000000818642722636\n",
       "2017-11-21 03:00:00 -0.00000000800000000000\n",
       "2017-11-21 04:00:00 -0.00000000777776760905\n",
       "2017-11-21 05:00:00 -0.00000000758390547225\n",
       "2017-11-21 06:00:00 -0.00000000747674242305\n",
       "2017-11-21 07:00:00 -0.00000000748801923454\n",
       "2017-11-21 08:00:00 -0.00000000761091060503\n",
       "2017-11-21 09:00:00 -0.00000000781165426851\n",
       "2017-11-21 10:00:00 -0.00000000800000000000\n",
       "2017-11-21 11:00:00 -0.00000000823743187372\n",
       "2017-11-21 12:00:00 -0.00000000841629087807\n",
       "2017-11-21 13:00:00 -0.00000000856423961712\n",
       "2017-11-21 14:00:00 -0.00000000868211486023\n",
       "2017-11-21 15:00:00 -0.00000000877098795925\n",
       "2017-11-21 16:00:00 -0.00000000883193026600\n",
       "2017-11-21 17:00:00 -0.00000000886601313235\n",
       "2017-11-21 18:00:00 -0.00000000887430791012\n",
       "2017-11-21 19:00:00 -0.00000000885788595117\n",
       "2017-11-21 20:00:00 -0.00000000881781860732\n",
       "2017-11-21 21:00:00 -0.00000000875517723043\n",
       "2017-11-21 22:00:00 -0.00000000867103317233\n",
       "2017-11-21 23:00:00 -0.00000000856645778487"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 11, 22)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating index for output\n",
    "import datetime\n",
    "date = new_df.index.date[0]\n",
    "date + datetime.timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = new_df.index + datetime.timedelta(days =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2017-11-22 00:00:00', '2017-11-22 01:00:00',\n",
       "               '2017-11-22 02:00:00', '2017-11-22 03:00:00',\n",
       "               '2017-11-22 04:00:00', '2017-11-22 05:00:00',\n",
       "               '2017-11-22 06:00:00', '2017-11-22 07:00:00',\n",
       "               '2017-11-22 08:00:00', '2017-11-22 09:00:00',\n",
       "               '2017-11-22 10:00:00', '2017-11-22 11:00:00',\n",
       "               '2017-11-22 12:00:00', '2017-11-22 13:00:00',\n",
       "               '2017-11-22 14:00:00', '2017-11-22 15:00:00',\n",
       "               '2017-11-22 16:00:00', '2017-11-22 17:00:00',\n",
       "               '2017-11-22 18:00:00', '2017-11-22 19:00:00',\n",
       "               '2017-11-22 20:00:00', '2017-11-22 21:00:00',\n",
       "               '2017-11-22 22:00:00', '2017-11-22 23:00:00'],\n",
       "              dtype='datetime64[ns]', name='Epoch_Time_of_Clock', freq='H')"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.index= date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OMEGA_dot'], dtype='object')"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  OMEGA_dot\n",
      "Epoch_Time_of_Clock                        \n",
      "2017-11-22 00:00:00 -0.00000000851895451729\n",
      "2017-11-22 01:00:00 -0.00000000836287991553\n",
      "2017-11-22 02:00:00 -0.00000000818642722636\n",
      "2017-11-22 03:00:00 -0.00000000800000000000\n",
      "2017-11-22 04:00:00 -0.00000000777776760905\n",
      "2017-11-22 05:00:00 -0.00000000758390547225\n",
      "2017-11-22 06:00:00 -0.00000000747674242305\n",
      "2017-11-22 07:00:00 -0.00000000748801923454\n",
      "2017-11-22 08:00:00 -0.00000000761091060503\n",
      "2017-11-22 09:00:00 -0.00000000781165426851\n",
      "2017-11-22 10:00:00 -0.00000000800000000000\n",
      "2017-11-22 11:00:00 -0.00000000823743187372\n",
      "2017-11-22 12:00:00 -0.00000000841629087807\n",
      "2017-11-22 13:00:00 -0.00000000856423961712\n",
      "2017-11-22 14:00:00 -0.00000000868211486023\n",
      "2017-11-22 15:00:00 -0.00000000877098795925\n",
      "2017-11-22 16:00:00 -0.00000000883193026600\n",
      "2017-11-22 17:00:00 -0.00000000886601313235\n",
      "2017-11-22 18:00:00 -0.00000000887430791012\n",
      "2017-11-22 19:00:00 -0.00000000885788595117\n",
      "2017-11-22 20:00:00 -0.00000000881781860732\n",
      "2017-11-22 21:00:00 -0.00000000875517723043\n",
      "2017-11-22 22:00:00 -0.00000000867103317233\n",
      "2017-11-22 23:00:00 -0.00000000856645778487\n",
      "Index(['OMEGA_dot'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(new_df)\n",
    "print(new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OMEGA_dot\n",
      "Epoch_Time_of_Clock                       \n",
      "2017-11-22 00:00:00 0.65136892142306768161\n",
      "2017-11-22 01:00:00 0.87811449028764543279\n",
      "2017-11-22 02:00:00 1.13446539512647248671\n",
      "2017-11-22 03:00:00 1.40530733235496274602\n",
      "2017-11-22 04:00:00 1.72816710157980502061\n"
     ]
    }
   ],
   "source": [
    "freq = None\n",
    "idx_tuples = []\n",
    "drop_incomplete  = True\n",
    "new_df[Paras[var_name]] = X_scaler.transform(new_df)\n",
    "new_new_df = new_df.copy()\n",
    "tensor_structure={'X':(range(-T+1, 1), Paras[var_name])}\n",
    "for name, structure in tensor_structure.items():\n",
    "        rng = structure[0]\n",
    "        dataset_cols = structure[1]\n",
    "        for col in dataset_cols:\n",
    "        # do not shift non-sequential 'static' features\n",
    "            if rng is None:\n",
    "                new_df['context_'+col] = new_df[col]\n",
    "                idx_tuples.append((name, col, 'static'))\n",
    "            else:\n",
    "                for t in rng:\n",
    "                    sign = '+' if t > 0 else ''\n",
    "                    shift = str(t) if t != 0 else ''\n",
    "                    period = 't'+sign+shift\n",
    "                    shifted_col = name+'_'+col+'_'+ period\n",
    "                    new_new_df[shifted_col] = new_new_df[col].shift(t*-1, freq=freq)\n",
    "                    idx_tuples.append((name, col, period))\n",
    "        new_new_df = new_new_df.drop(new_df.columns, axis=1)\n",
    "        idx = pd.MultiIndex.from_tuples(idx_tuples, names=['tensor', 'feature', 'time step'])\n",
    "        print(new_df.head())\n",
    "        new_new_df.columns = idx\n",
    "        if drop_incomplete:\n",
    "            new_new_df = new_new_df.dropna(how='any')\n",
    "            \n",
    "inputs = {}           \n",
    "for name, structure in tensor_structure.items():\n",
    "    rng = structure[0]\n",
    "    cols = structure[1]\n",
    "    tensor = new_new_df[name][cols].as_matrix()\n",
    "    if rng is None:\n",
    "        tensor = tensor.reshape(tensor.shape[0], len(cols))\n",
    "    else:\n",
    "        tensor = tensor.reshape(tensor.shape[0], len(cols), len(rng))\n",
    "        tensor = np.transpose(tensor, axes=[0, 2, 1])\n",
    "    inputs[name] = tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor                                   X                         \\\n",
      "feature                          OMEGA_dot                          \n",
      "time step                             t-23                   t-22   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-22 23:00:00 0.65136892142306768161 0.87811449028764543279   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                             t-21                   t-20   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-22 23:00:00 1.13446539512647248671 1.40530733235496274602   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                             t-19                   t-18   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-22 23:00:00 1.72816710157980502061 2.00981049515137977224   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                             t-17                   t-16   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-22 23:00:00 2.16549723904249180606 2.14911425931712551929   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                             t-15                   t-14   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-22 23:00:00 1.97057737121311982165 1.67893647859866712579   \n",
      "\n",
      "tensor                       ...                                   \\\n",
      "feature                      ...                                    \n",
      "time step                    ...                              t-9   \n",
      "Epoch_Time_of_Clock          ...                                    \n",
      "2017-11-22 23:00:00          ...           0.41432917018308745849   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                              t-8                    t-7   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-22 23:00:00 0.28521411131505070014 0.19667697679013171341   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                              t-6                    t-5   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-22 23:00:00 0.14716130395215190618 0.13511063021757485636   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                              t-4                    t-3   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-22 23:00:00 0.15896849294474846293 0.21717842955013388906   \n",
      "\n",
      "tensor                                                             \\\n",
      "feature                                                             \n",
      "time step                              t-2                    t-1   \n",
      "Epoch_Time_of_Clock                                                 \n",
      "2017-11-22 23:00:00 0.30818397739209107966 0.43042867387254268863   \n",
      "\n",
      "tensor                                      \n",
      "feature                                     \n",
      "time step                                t  \n",
      "Epoch_Time_of_Clock                         \n",
      "2017-11-22 23:00:00 0.58235605636436793553  \n",
      "\n",
      "[1 rows x 24 columns]\n",
      "[[[0.6513689214230677 ]\n",
      "  [0.8781144902876454 ]\n",
      "  [1.1344653951264725 ]\n",
      "  [1.4053073323549627 ]\n",
      "  [1.728167101579805  ]\n",
      "  [2.0098104951513798 ]\n",
      "  [2.165497239042492  ]\n",
      "  [2.1491142593171255 ]\n",
      "  [1.9705773712131198 ]\n",
      "  [1.6789364785986671 ]\n",
      "  [1.4053073323549627 ]\n",
      "  [1.0603657168318323 ]\n",
      "  [0.8005189113262468 ]\n",
      "  [0.5855786159777785 ]\n",
      "  [0.41432917018308746]\n",
      "  [0.2852141113150507 ]\n",
      "  [0.1966769767901317 ]\n",
      "  [0.1471613039521519 ]\n",
      "  [0.13511063021757486]\n",
      "  [0.15896849294474846]\n",
      "  [0.2171784295501339 ]\n",
      "  [0.3081839773920911 ]\n",
      "  [0.4304286738725427 ]\n",
      "  [0.5823560563643679 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(new_new_df)\n",
    "print(inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(inputs['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2705225  ,  0.44467944 ,  0.5463613  ,  0.6035408  ,\n",
       "         0.6292668  ,  0.6290099  ,  0.6051212  ,  0.55929863 ,\n",
       "         0.4939327  ,  0.4127072  ,  0.320663   ,  0.22383897 ,\n",
       "         0.12863086 ,  0.04120782 , -0.032651946, -0.08731775 ,\n",
       "        -0.117116734, -0.116717145, -0.082098916, -0.011463596,\n",
       "         0.09475133 ,  0.23514079 ,  0.40940058 ,  0.61902    ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictions[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2705225  ,  0.44467944 ,  0.5463613  ,  0.6035408  ,\n",
       "        0.6292668  ,  0.6290099  ,  0.6051212  ,  0.55929863 ,\n",
       "        0.4939327  ,  0.4127072  ,  0.320663   ,  0.22383897 ,\n",
       "        0.12863086 ,  0.04120782 , -0.032651946, -0.08731775 ,\n",
       "       -0.117116734, -0.116717145, -0.082098916, -0.011463596,\n",
       "        0.09475133 ,  0.23514079 ,  0.40940058 ,  0.61902    ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OMEGA_dot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.27052250504493713379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44467943906784057617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.54636132717132568359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.60354077816009521484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.62926679849624633789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.62900990247726440430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.60512119531631469727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.55929863452911376953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.49393269419670104980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.41270720958709716797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.32066300511360168457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.22383897006511688232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.12863086163997650146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.04120782017707824707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.03265194594860076904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.08731774985790252686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.11711673438549041748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.11671714484691619873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.08209891617298126221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.01146359555423259735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.09475132822990417480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.23514078557491302490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.40940058231353759766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.61901998519897460938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 OMEGA_dot\n",
       "0   0.27052250504493713379\n",
       "1   0.44467943906784057617\n",
       "2   0.54636132717132568359\n",
       "3   0.60354077816009521484\n",
       "4   0.62926679849624633789\n",
       "5   0.62900990247726440430\n",
       "6   0.60512119531631469727\n",
       "7   0.55929863452911376953\n",
       "8   0.49393269419670104980\n",
       "9   0.41270720958709716797\n",
       "10  0.32066300511360168457\n",
       "11  0.22383897006511688232\n",
       "12  0.12863086163997650146\n",
       "13  0.04120782017707824707\n",
       "14 -0.03265194594860076904\n",
       "15 -0.08731774985790252686\n",
       "16 -0.11711673438549041748\n",
       "17 -0.11671714484691619873\n",
       "18 -0.08209891617298126221\n",
       "19 -0.01146359555423259735\n",
       "20  0.09475132822990417480\n",
       "21  0.23514078557491302490\n",
       "22  0.40940058231353759766\n",
       "23  0.61901998519897460938"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(results , columns = [var_name])\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OMEGA_dot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-22 00:00:00</th>\n",
       "      <td>0.27052250504493713379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 01:00:00</th>\n",
       "      <td>0.44467943906784057617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 02:00:00</th>\n",
       "      <td>0.54636132717132568359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 03:00:00</th>\n",
       "      <td>0.60354077816009521484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 04:00:00</th>\n",
       "      <td>0.62926679849624633789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 05:00:00</th>\n",
       "      <td>0.62900990247726440430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 06:00:00</th>\n",
       "      <td>0.60512119531631469727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 07:00:00</th>\n",
       "      <td>0.55929863452911376953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 08:00:00</th>\n",
       "      <td>0.49393269419670104980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 09:00:00</th>\n",
       "      <td>0.41270720958709716797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 10:00:00</th>\n",
       "      <td>0.32066300511360168457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 11:00:00</th>\n",
       "      <td>0.22383897006511688232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 12:00:00</th>\n",
       "      <td>0.12863086163997650146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 13:00:00</th>\n",
       "      <td>0.04120782017707824707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 14:00:00</th>\n",
       "      <td>-0.03265194594860076904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 15:00:00</th>\n",
       "      <td>-0.08731774985790252686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 16:00:00</th>\n",
       "      <td>-0.11711673438549041748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 17:00:00</th>\n",
       "      <td>-0.11671714484691619873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 18:00:00</th>\n",
       "      <td>-0.08209891617298126221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 19:00:00</th>\n",
       "      <td>-0.01146359555423259735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 20:00:00</th>\n",
       "      <td>0.09475132822990417480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 21:00:00</th>\n",
       "      <td>0.23514078557491302490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 22:00:00</th>\n",
       "      <td>0.40940058231353759766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 23:00:00</th>\n",
       "      <td>0.61901998519897460938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  OMEGA_dot\n",
       "Epoch_Time_of_Clock                        \n",
       "2017-11-22 00:00:00  0.27052250504493713379\n",
       "2017-11-22 01:00:00  0.44467943906784057617\n",
       "2017-11-22 02:00:00  0.54636132717132568359\n",
       "2017-11-22 03:00:00  0.60354077816009521484\n",
       "2017-11-22 04:00:00  0.62926679849624633789\n",
       "2017-11-22 05:00:00  0.62900990247726440430\n",
       "2017-11-22 06:00:00  0.60512119531631469727\n",
       "2017-11-22 07:00:00  0.55929863452911376953\n",
       "2017-11-22 08:00:00  0.49393269419670104980\n",
       "2017-11-22 09:00:00  0.41270720958709716797\n",
       "2017-11-22 10:00:00  0.32066300511360168457\n",
       "2017-11-22 11:00:00  0.22383897006511688232\n",
       "2017-11-22 12:00:00  0.12863086163997650146\n",
       "2017-11-22 13:00:00  0.04120782017707824707\n",
       "2017-11-22 14:00:00 -0.03265194594860076904\n",
       "2017-11-22 15:00:00 -0.08731774985790252686\n",
       "2017-11-22 16:00:00 -0.11711673438549041748\n",
       "2017-11-22 17:00:00 -0.11671714484691619873\n",
       "2017-11-22 18:00:00 -0.08209891617298126221\n",
       "2017-11-22 19:00:00 -0.01146359555423259735\n",
       "2017-11-22 20:00:00  0.09475132822990417480\n",
       "2017-11-22 21:00:00  0.23514078557491302490\n",
       "2017-11-22 22:00:00  0.40940058231353759766\n",
       "2017-11-22 23:00:00  0.61901998519897460938"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.index = date\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[var_name] = y_scalar.inverse_transform(res_df[[var_name]])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final generated output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OMEGA_dot</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch_Time_of_Clock</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-22 00:00:00</th>\n",
       "      <td>-0.00000000878110029134</td>\n",
       "      <td>-0.00000000844252241989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 01:00:00</th>\n",
       "      <td>-0.00000000866122373822</td>\n",
       "      <td>-0.00000000830029842923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 02:00:00</th>\n",
       "      <td>-0.00000000859123350239</td>\n",
       "      <td>-0.00000000814085716473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 03:00:00</th>\n",
       "      <td>-0.00000000855187565207</td>\n",
       "      <td>-0.00000000800000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 04:00:00</th>\n",
       "      <td>-0.00000000853416803892</td>\n",
       "      <td>-0.00000000777580986875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 05:00:00</th>\n",
       "      <td>-0.00000000853434478643</td>\n",
       "      <td>-0.00000000760238772032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 06:00:00</th>\n",
       "      <td>-0.00000000855078763351</td>\n",
       "      <td>-0.00000000750163638396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 07:00:00</th>\n",
       "      <td>-0.00000000858232862555</td>\n",
       "      <td>-0.00000000751032424326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 08:00:00</th>\n",
       "      <td>-0.00000000862732196794</td>\n",
       "      <td>-0.00000000764204786484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 09:00:00</th>\n",
       "      <td>-0.00000000868323102310</td>\n",
       "      <td>-0.00000000786105674394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 10:00:00</th>\n",
       "      <td>-0.00000000874658745431</td>\n",
       "      <td>-0.00000000808508018493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 11:00:00</th>\n",
       "      <td>-0.00000000881323369839</td>\n",
       "      <td>-0.00000000827369392876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 12:00:00</th>\n",
       "      <td>-0.00000000887876794309</td>\n",
       "      <td>-0.00000000843033591716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 13:00:00</th>\n",
       "      <td>-0.00000000893894380738</td>\n",
       "      <td>-0.00000000855828647856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 14:00:00</th>\n",
       "      <td>-0.00000000898978314012</td>\n",
       "      <td>-0.00000000865875736553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 15:00:00</th>\n",
       "      <td>-0.00000000902741081887</td>\n",
       "      <td>-0.00000000873287039255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 16:00:00</th>\n",
       "      <td>-0.00000000904792241130</td>\n",
       "      <td>-0.00000000878174737412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 17:00:00</th>\n",
       "      <td>-0.00000000904764707599</td>\n",
       "      <td>-0.00000000880651012472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 18:00:00</th>\n",
       "      <td>-0.00000000902381902534</td>\n",
       "      <td>-0.00000000880828045883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 19:00:00</th>\n",
       "      <td>-0.00000000897519836229</td>\n",
       "      <td>-0.00000000878818019095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 20:00:00</th>\n",
       "      <td>-0.00000000890208795568</td>\n",
       "      <td>-0.00000000874733113555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 21:00:00</th>\n",
       "      <td>-0.00000000880545503179</td>\n",
       "      <td>-0.00000000868685510714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 22:00:00</th>\n",
       "      <td>-0.00000000868550742439</td>\n",
       "      <td>-0.00000000860787392019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22 23:00:00</th>\n",
       "      <td>-0.00000000854122106375</td>\n",
       "      <td>-0.00000000851150938919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  OMEGA_dot                  actual\n",
       "Epoch_Time_of_Clock                                                \n",
       "2017-11-22 00:00:00 -0.00000000878110029134 -0.00000000844252241989\n",
       "2017-11-22 01:00:00 -0.00000000866122373822 -0.00000000830029842923\n",
       "2017-11-22 02:00:00 -0.00000000859123350239 -0.00000000814085716473\n",
       "2017-11-22 03:00:00 -0.00000000855187565207 -0.00000000800000000000\n",
       "2017-11-22 04:00:00 -0.00000000853416803892 -0.00000000777580986875\n",
       "2017-11-22 05:00:00 -0.00000000853434478643 -0.00000000760238772032\n",
       "2017-11-22 06:00:00 -0.00000000855078763351 -0.00000000750163638396\n",
       "2017-11-22 07:00:00 -0.00000000858232862555 -0.00000000751032424326\n",
       "2017-11-22 08:00:00 -0.00000000862732196794 -0.00000000764204786484\n",
       "2017-11-22 09:00:00 -0.00000000868323102310 -0.00000000786105674394\n",
       "2017-11-22 10:00:00 -0.00000000874658745431 -0.00000000808508018493\n",
       "2017-11-22 11:00:00 -0.00000000881323369839 -0.00000000827369392876\n",
       "2017-11-22 12:00:00 -0.00000000887876794309 -0.00000000843033591716\n",
       "2017-11-22 13:00:00 -0.00000000893894380738 -0.00000000855828647856\n",
       "2017-11-22 14:00:00 -0.00000000898978314012 -0.00000000865875736553\n",
       "2017-11-22 15:00:00 -0.00000000902741081887 -0.00000000873287039255\n",
       "2017-11-22 16:00:00 -0.00000000904792241130 -0.00000000878174737412\n",
       "2017-11-22 17:00:00 -0.00000000904764707599 -0.00000000880651012472\n",
       "2017-11-22 18:00:00 -0.00000000902381902534 -0.00000000880828045883\n",
       "2017-11-22 19:00:00 -0.00000000897519836229 -0.00000000878818019095\n",
       "2017-11-22 20:00:00 -0.00000000890208795568 -0.00000000874733113555\n",
       "2017-11-22 21:00:00 -0.00000000880545503179 -0.00000000868685510714\n",
       "2017-11-22 22:00:00 -0.00000000868550742439 -0.00000000860787392019\n",
       "2017-11-22 23:00:00 -0.00000000854122106375 -0.00000000851150938919"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final generated ouput\n",
    "res_df['actual'] = Actual\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('SA2_OMEGA_dot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.694561527107474e-10"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "a = mean_absolute_error(res_df['OMEGA_dot'], res_df['actual'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
